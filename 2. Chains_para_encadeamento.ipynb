{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as principais bibliotecas\n",
    "from langchain_openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carregando as variáveis que estão no .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "É a capacidade do modelo de guardar as informações e se lembrar de conversas passadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro vamos instanciar e iniciar a memória\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora adicionando algumas conversas a esta memoria\n",
    "# veja que é deivdido entre falas do usuário e falas do modelo\n",
    "memory.chat_memory.add_user_message(\"Olá!\")\n",
    "memory.chat_memory.add_ai_message(\"Oi, como vai?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Olá!\\nAI: Oi, como vai?'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos o q já temos dentro da memória\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando uma conversa com memória**\n",
    "\n",
    "Vamos agora unir o chat à memória através de uma chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as libs necessárias\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.base import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o chat\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a memória\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e agora juntando os dois numa única chain\n",
    "# fazemos isso instanciando o objeto ConversationChain()\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    # deixando o verbose como True retorna toda a interação\n",
    "    # com o modelo. Isso facilita para debugar.\n",
    "    # quando está false, ele retorna somente a resposta do modelo\n",
    "    verbose=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: opa. Meu nome é Kaio,                      e quero q vc seja meu conselheiro de carreira.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Olá Kaio! Fico feliz em ser seu conselheiro de carreira. Estou aqui para ajudá-lo no que precisar. Como posso ajudá-lo hoje?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja como fica a saída com o verbose = True\n",
    "# ele mostra o prompt montado pelo chain para otimizar a memória do modelo\n",
    "conversation.predict(input=\"opa. Meu nome é Kaio, \\\n",
    "                     e quero q vc seja meu conselheiro de carreira.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: opa. Meu nome é Kaio,                      e quero q vc seja meu conselheiro de carreira.\n",
      "AI: Olá Kaio! Fico feliz em ser seu conselheiro de carreira. Estou aqui para ajudá-lo no que precisar. Como posso ajudá-lo hoje?\n",
      "Human: Que bom, quero falar sobre analise de dados.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Claro, Kaio! A análise de dados é um campo muito interessante. Existem várias ferramentas e técnicas que podem ser utilizadas para analisar dados, como estatísticas descritivas, regressão, análise de séries temporais, machine learning, entre outras. Você tem alguma pergunta específica sobre análise de dados em mente? Eu posso te ajudar a entender melhor os conceitos ou aconselhá-lo sobre como se aprofundar nesse campo.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja que o chain vai guardando o histórico de conversa\n",
    "conversation.predict(input=\"Que bom, quero falar sobre analise de dados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: opa. Meu nome é Kaio,                      e quero q vc seja meu conselheiro de carreira.\\nAI: Olá Kaio! Fico feliz em ser seu conselheiro de carreira. Estou aqui para ajudá-lo no que precisar. Como posso ajudá-lo hoje?\\nHuman: Que bom, quero falar sobre analise de dados.\\nAI: Claro, Kaio! A análise de dados é um campo muito interessante. Existem várias ferramentas e técnicas que podem ser utilizadas para analisar dados, como estatísticas descritivas, regressão, análise de séries temporais, machine learning, entre outras. Você tem alguma pergunta específica sobre análise de dados em mente? Eu posso te ajudar a entender melhor os conceitos ou aconselhá-lo sobre como se aprofundar nesse campo.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# podemos verificar o histórico acessando as variáveis da memória\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConversationBufferWindowMemory**\n",
    "\n",
    "É um tipo de memória diferente da anterior. Cada aplicação pode pedir um tipo diferente de memporia.\n",
    "\n",
    "Esta memória só guarda um numero específico de conversas passadas. Essa quantidade de conversas a serem guardadas é indicada como argumento no momento em q estamos instanciando a memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos recirar alguns objetos para ilustrar como funciona essa memória\n",
    "\n",
    "# instanciando a memoria que acabamos de importar\n",
    "# veja que aqui queremos q lembre somente de uma conversa anterior\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "# novamente juntando o modelo (chat) com o novo tipo de memoria criado\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Olá meu nome é Kaio.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Olá Kaio! É um prazer conhecê-lo. Como posso ajudá-lo hoje?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos exemplificar aqui como funciona o k\n",
    "# eu começo falando meu nome para o modelo\n",
    "conversation.predict(input=\"Olá meu nome é Kaio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Olá meu nome é Kaio.\n",
      "AI: Olá Kaio! É um prazer conhecê-lo. Como posso ajudá-lo hoje?\n",
      "Human: Como vai?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Estou bem, obrigado por perguntar! Estou aqui para ajudá-lo com qualquer dúvida ou informação de que precisar. Como posso ajudá-lo hoje?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o modelo responde mencionando meu nome\n",
    "conversation.predict(input=\"Como vai?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Como vai?\n",
      "AI: Estou bem, obrigado por perguntar! Estou aqui para ajudá-lo com qualquer dúvida ou informação de que precisar. Como posso ajudá-lo hoje?\n",
      "Human: Qual é o meu nome?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Desculpe, mas eu não tenho essa informação. Como posso ajudá-lo com algo mais?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ao prosseguir o diálogo pergunto se ele sabe meu nome\n",
    "# o modelo não sabe respondê-la, pois já se esqueceu q eu tinha falado na janela anterior\n",
    "# veja no retorno abaixo como o prompt vem formatado\n",
    "# a memoria só lembra de uma conversa anterior e o diálogo já se inicia no 'como vai?'\n",
    "conversation.predict(input=\"Qual é o meu nome?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConversationTokenBufferMemory**\n",
    "\n",
    "Aqui a memória é limitada pelo tamanho das conversas, ou seja, toda a conversa anterior não deve passar de um número X de tokens. Essa quantidade é indicada como argumento no momento em ue instanciamos a memmoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos fazer aqui o mesmo processo das anteriores\n",
    "# e recirar alguns objetos para ilustrar como funciona essa memória\n",
    "\n",
    "# instanciando a memoria que acabamos de importar\n",
    "# ela pede 2 argumentos principais, o modelo\n",
    "# e o máximo de tokens que a memória deverá se lembrar\n",
    "memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=250)\n",
    "\n",
    "# novamente juntando o modelo (chat) com o novo tipo de memoria criado\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConversationSummaryBufferMemory**\n",
    "\n",
    "Este tipo de memória aqui faz um resumo de toda conversa anterior. Ele inclui esse resumo no prompt e já passa o contexto para o modelo. As anteriores mostravam todo o diálogo enquanto essa aqui traz apenas o resumo do q foi conversado até então. A quantidade máxima de tokens é para indicar o tamanho que o resumo deverá ter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos fazer aqui o mesmo processo das anteriores\n",
    "# e recirar alguns objetos para ilustrar como funciona essa memória\n",
    "\n",
    "# instanciando a memoria que acabamos de importar\n",
    "# assim como a anterior, ela tbm pede um modelo como argumento\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=250)\n",
    "\n",
    "# novamente juntando o modelo (chat) com o novo tipo de memoria criado\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Opa, meu nome é Kaio\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Olá, Kaio! Eu sou um assistente de inteligência artificial. Como posso ajudar você hoje?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja que no inicio ele lembra do diálogo como nas outras memórias\n",
    "conversation.predict(input=\"Opa, meu nome é Kaio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Opa, meu nome é Kaio\n",
      "AI: Olá, Kaio! Eu sou um assistente de inteligência artificial. Como posso ajudar você hoje?\n",
      "Human: me conte uma historia de 500 palavras\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Claro, Kaio! Vou te contar uma história sobre um jovem aventureiro chamado Miguel. Miguel era um garoto curioso e destemido que sempre sonhou em explorar o mundo. Um dia, ele decidiu sair de sua pequena cidade e embarcar em uma jornada épica.\\n\\nMiguel começou sua jornada nas montanhas geladas do norte, onde enfrentou tempestades de neve e criaturas selvagens. Ele conheceu tribos antigas e aprendeu seus segredos de sobrevivência. Depois de semanas de viagem, Miguel finalmente chegou à costa do oceano, onde viu pela primeira vez as ondas gigantes e os navios que cruzavam os mares.\\n\\nDecidido a se tornar um grande navegador, Miguel se juntou a uma tripulação de piratas e navegou pelos sete mares em busca de tesouros perdidos. Ele enfrentou batalhas navais, navegou por mares traiçoeiros e explorou ilhas desconhecidas. Ao longo de sua jornada, Miguel fez amigos leais e inimigos perigosos.\\n\\nEm uma de suas aventuras, Miguel descobriu um mapa antigo que levava a uma ilha misteriosa. Ele e sua tripulação embarcaram em uma jornada perigosa, enfrentando tempestades e monstros marinhos. Finalmente, chegaram à ilha e encontraram o tesouro lendário que mudaria suas vidas para sempre.\\n\\nDepois de anos de aventuras e descobertas, Miguel decidiu voltar para casa. Ele retornou à sua cidade natal como um herói, com histórias incríveis para compartilhar com todos. Miguel se tornou uma lenda entre os moradores e sua coragem e determinação inspiraram gerações futuras de aventureiros.\\n\\nE assim, a história de Miguel, o jovem aventureiro destemido, ficou marcada na memória de todos que ouviram falar de suas incríveis jornadas. Kaio, espero que tenha gostado da história! Posso te ajudar com mais alguma coisa?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mas assim que a conversa cresce ele vai precisar fazer um resumo\n",
    "# aqui eu pedi pra ele contar uma historia grande, pra ultrapassar os tokens\n",
    "conversation.predict(input=\"me conte uma historia de 500 palavras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human introduces himself as Kaio to the AI, who responds with a greeting and offers assistance. Kaio asks the AI to tell a 500-word story, and the AI tells a tale about a young adventurer named Miguel. Miguel embarks on a journey through snowy mountains, encounters ancient tribes, joins pirates to explore the seas, and discovers a legendary treasure on a mysterious island. After years of adventures, Miguel returns home as a hero, inspiring future generations with his courage and determination.\n",
      "Human: Qual seu filme preferido?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Eu não assisto filmes, então não tenho um filme preferido. Mas posso te recomendar alguns baseados em informações que tenho acessado. Gosta de filmes de ação, comédia, romance, ficção científica ou algum outro gênero específico? Eu posso te ajudar a encontrar um filme que combine com seus interesses.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja q agora na próxima interação o prompt feito automaticamente\n",
    "# já traz um resumo, em vez do histórico do dialogo\n",
    "conversation.predict(input=\"Qual seu filme preferido?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains - Encadeamento de prompts com Langchain\n",
    "\n",
    "Chains são componentes essenciais, pois ela liga diversos módulos (prompts, llms, output parsers) de forma a construir uma aplicação robusta e complexa.\n",
    "\n",
    "Anteriormente trabalhamos de forma independente a aprender as diversas formas de criar um prompt, instanciar um modelo, criar output parsers etc, agora vamos ligar todos eles numa chain.\n",
    "\n",
    "Passar várias instruções para um unico prompt vai fazer com que o modelo performe mal. Por exemplo, pedir pra ele resumir um texto, depois pedir pra ele extrair alguma informação, depois pedir pra ele traduzir pra outro idioma...tudo isso funciona melhor se quebrarmos essas etapas em diversos prompts especializados e depois juntar todos em um chain. Porém existem vários tipo de chain diferentes, vamos explicar um pouco delas abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversation Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos construir uma chain como fizemos no exemplo da aula anterior\n",
    "\n",
    "# vamos instanciar e criar um chat\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0125')\n",
    "\n",
    "# criando a memoria\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora juntando tudo numa chain simples\n",
    "# basta fornecer os componentes criados como argumentos da chain\n",
    "chain = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Opa, testando..\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Olá! Estou aqui para ajudar. O que você gostaria de saber ou discutir hoje?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora veja a chain funcionando\n",
    "# perceba tbm q ela cria um prompt para dar contexto ao modelo\n",
    "chain.predict(input=\"Opa, testando..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos dizer q queremos fornecer um prompt para a chain\n",
    "# vamos fazer isso agora\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um prompt template\n",
    "# a variável history deve ser inserida, pois ela representa o historico do chat\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "'''\n",
    "Essa é uma conversa amigável entre um humano e uma IA.\n",
    "                                               \n",
    "Conversa atual:\n",
    "\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "\n",
    "AI:                                               \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora basta criar a chain como criada anteriormente\n",
    "# a diferença é q agora vc vai forcener mais um argumento, o prompt\n",
    "chain = ConversationChain(\n",
    "    prompt=prompt_template,\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Essa é uma conversa amigável entre um humano e uma IA.\n",
      "                                               \n",
      "Conversa atual:\n",
      "\n",
      "Human: Opa, testando..\n",
      "AI: Olá! Estou aqui para ajudar. O que você gostaria de saber ou discutir hoje?\n",
      "\n",
      "Human: opa, testando!\n",
      "\n",
      "AI:                                               \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pode contar comigo para ajudar no que precisar. Estou aqui para conversar e interagir com você. Como posso ajudar hoje?'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"opa, testando!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaio-peixoto/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Essa é uma conversa amigável entre um humano e uma IA.\n",
      "                                               \n",
      "Conversa atual:\n",
      "\n",
      "Human: Opa, testando..\n",
      "AI: Olá! Estou aqui para ajudar. O que você gostaria de saber ou discutir hoje?\n",
      "Human: opa, testando!\n",
      "AI: Pode contar comigo para ajudar no que precisar. Estou aqui para conversar e interagir com você. Como posso ajudar hoje?\n",
      "\n",
      "Human: oi\n",
      "\n",
      "AI:                                               \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Olá! Como posso ajudar você hoje?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja que podemos usar o método run em vez do predict\n",
    "# ele só pede o comando direto, não precisa escrever o 'input'\n",
    "chain.run(\"oi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLMChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos criar aqui o prompt para usar nesse chain\n",
    "prompt_llm_chain = PromptTemplate.from_template(\n",
    "\"\"\"Qual o melhor nome de empresa para uma empresa que desenvolve o produto: {produto}\\\n",
    "Retorne apenas um nome\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora basta instanciar LLMChain, fornecer os argumentos e criar a chain\n",
    "chain = LLMChain(llm=chat, memory=memory, prompt=prompt_llm_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ArteBarro'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja que só precisamos fornecer a variável (produto)\n",
    "produto = 'filtros de barro artesanais'\n",
    "chain.run(produto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SimpleSequentialChain**\n",
    "\n",
    "Aqui tornamos o chain mais complexo do q o LLMChain. Vamos estruturar e combinar uma sequencia de instruções diferentes. O output de uma chain é passado para outra chain, de forma cascateada, de modo que quebramos a tarefas em vários passos para otimizar o resultado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos criar nosso primeiro prompt dessa sequencia\n",
    "prompt_produto = PromptTemplate.from_template(\n",
    "\"\"\"Qual o melhor nome de empresa para uma empresa que desenvolve o produto: {produto}\\\n",
    "Retorne apenas um nome\"\"\"\n",
    ")\n",
    "\n",
    "chain_produto = LLMChain(llm=chat, memory=memory, prompt=prompt_produto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segundo prompt, q vai utilizar o output da chain anterior\n",
    "prompt_descricao_empresa = PromptTemplate.from_template(\n",
    "\"\"\"Dado a empresa com o seguinte nome: {nome}. Dê\\\n",
    "uma descrição de até 50 palavras das atividades dessa empresa.\"\"\"\n",
    ")\n",
    "\n",
    "chain_descricao_empresa = LLMChain(llm=chat, memory=memory, prompt=prompt_descricao_empresa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a chain sequencial\n",
    "# veja que a sequencia como os chains são colocados na lista impacta no resultado\n",
    "# esta sequential chain vai retornar o resultado da ultima chain da lista de chains\n",
    "sequential_chain = SimpleSequentialChain(\n",
    "    chains=[chain_produto, chain_descricao_empresa],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mArteClayFiltros\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mA ArteClayFiltros é uma empresa especializada na fabricação e venda de filtros de argila para purificação de água. Com produtos de alta qualidade e eficiência, a empresa atua no mercado oferecendo soluções sustentáveis para o tratamento de água potável em residências e estabelecimentos comerciais.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A ArteClayFiltros é uma empresa especializada na fabricação e venda de filtros de argila para purificação de água. Com produtos de alta qualidade e eficiência, a empresa atua no mercado oferecendo soluções sustentáveis para o tratamento de água potável em residências e estabelecimentos comerciais.'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testando nossa chain\n",
    "produto = 'filtros de barro artesanais'\n",
    "sequential_chain.run(produto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SequentialChain**\n",
    "\n",
    "A diferença para o exemplo anterior é que essa chain não precisa estar na ordem para q uma consuma o output da outra. Em vez disso, nós vamos fornecer output_keys, q serão as variáveis produzidas pela chain. Os nomes das variáveis podem ser utilizadas em outras chains, eliminando a necessidade de estarem em ordem, como na SimpleSequentialChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos aqui utilizar os mesmos prompts do exemplo anterior\n",
    "# veja que agora tem uma diferença, a presença do argumento output_key\n",
    "# a outra diferença é q não usamos mais o argumento memory\n",
    "# pois ele pode causar conflitos, como inconsistencia de variáveis,\n",
    "# no fluxo de dados gerenciado pelo SequentialChain\n",
    "\n",
    "\n",
    "prompt_produto = PromptTemplate.from_template(\n",
    "\"\"\"Qual o melhor nome de empresa para uma empresa que desenvolve o produto:{produto}\n",
    "Retorne apenas um nome\"\"\")\n",
    "\n",
    "chain_produto = LLMChain(llm=chat, prompt=prompt_produto, output_key='nome')\n",
    "\n",
    "\n",
    "prompt_descricao_empresa = PromptTemplate.from_template(\n",
    "\"\"\"Dado a empresa com o seguinte nome: {nome} e que\\\n",
    "produz o seguinte produto: {produto}, dê uma descrição de até\\\n",
    "50 palavras das atividades dessa empresa.\"\"\"\n",
    ")\n",
    "\n",
    "chain_descricao_empresa = LLMChain(llm=chat, prompt=prompt_descricao_empresa, \n",
    "                                   output_key='descricao_empresa')\n",
    "\n",
    "# adicionando mais um prompt, o de tradução\n",
    "prompt_traducao = PromptTemplate.from_template(\n",
    "\"\"\"Crie um nome em russo para a empresa de nome {nome}\n",
    "    que possui a seguinte descrição {descricao_empresa}\"\"\")\n",
    "\n",
    "chain_traducao = LLMChain(llm=chat, prompt=prompt_traducao, output_key='traducao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos criar nossa SequentialChain\n",
    "# note q agora temos alguns argumentos q não fizemos em outras chains\n",
    "# precisamos declarar quem são as input variables e as output keys\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains=[chain_produto, chain_descricao_empresa, chain_traducao],\n",
    "    input_variables=['produto'],\n",
    "    output_variables=['nome', 'descricao_empresa', 'traducao'],\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'produto': 'barbeador bruto da roça',\n",
       " 'nome': 'Barba Rústica',\n",
       " 'descricao_empresa': 'A empresa Barba Rústica se destaca pela produção de barbeadores brutos da roça, feitos com materiais naturais e de alta qualidade. Com foco na praticidade e eficiência, a empresa oferece produtos que atendem às necessidades dos homens que buscam um estilo de vida mais autêntico e rústico.',\n",
       " 'traducao': 'Брутальная Борода Грубая (Brutalnaya Boroda Grubaya)'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta = chain.invoke({'produto':'barbeador bruto da roça'})\n",
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'barbeador bruto da roça'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja q o retorno é um dicionário\n",
    "# portanto podemos capturar os resultados separadamente\n",
    "resposta['produto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Брутальная Борода Грубая (Brutalnaya Boroda Grubaya)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta['traducao']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RouterChains - Cadeias de Roteamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Primeiro passo, criar uma lista de dicionarios dos prompts. Esses dicionários vão trazer informações como nome do prompt, descrição do q ele faz e o respectivo prompt_template.\n",
    "\n",
    "2 - criando as chains: vamos usar os templates e criar as chains específicas de cada prompt. Essas chains vão todas para um dicionário de chains.\n",
    "\n",
    "3 - criando os destinos. É aqui q o router vai se guiar para saber para qual chain direcionar cada input do usuário.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temos aqui 3 prompts especializados\n",
    "# cada um em uma matéria específica\n",
    "\n",
    "fis_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Vc é um exímio professor de física.\n",
    "Vc é ótimo em responder perguntas sobre física de forma concisa e fácil de entender.\\\n",
    "Quando não sabe a resposta para uma perginta, vc admite que não sabe.\n",
    "                                                \n",
    "Aqui está a pergunta: {input}\"\"\")\n",
    "\n",
    "mat_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Vc é um exímio professor de matemática, a verdadeira reencarnação de Pitágoras.\\\n",
    "Vc é ótimo em responder perguntas sobre matemática de forma concisa e fácil de entender.\\\n",
    "Quando não sabe a resposta para uma perginta, vc admite que não sabe.\n",
    "                                                \n",
    "Aqui está a pergunta: {input}\"\"\")\n",
    "\n",
    "hist_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Vc é um exímio professor de história, a verdadeira reencarnação de Eric Hobsbawm.\\\n",
    "Vc é ótimo em responder perguntas sobre história de forma concisa e fácil de entender.\\\n",
    "Quando não sabe a resposta para uma perginta, vc admite que não sabe.\n",
    "                                                \n",
    "Aqui está a pergunta: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 1: Criando os dicionários de prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        'name':'Física',\n",
    "        'description':'Ideal para responder perguntas sobre física',\n",
    "        'prompt_template':fis_template\n",
    "    },\n",
    "    {\n",
    "        'name':'Matemática',\n",
    "        'description':'Ideal para responder perguntas sobre matemática',\n",
    "        'prompt_template':mat_template\n",
    "    },\n",
    "    {\n",
    "        'name':'História',\n",
    "        'description':'Ideal para responder perguntas sobre história',\n",
    "        'prompt_template':hist_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando o chat\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 2: Criando as chains e armazenando-as num dicionário de chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro vamos criar um dicionário vazio\n",
    "chains = {}\n",
    "\n",
    "# usando um laço para criar as chains\n",
    "# para cada dicionario dentro de prompt_infos\n",
    "for info in prompt_infos:\n",
    "    # crie uma chain utilizando seu respectivo prompt template\n",
    "    chain = LLMChain(llm=chat, prompt=info['prompt_template'], verbose=True)\n",
    "    # depois armazene essa chain no dicionário chains\n",
    "    chains[info['name']] = chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Física': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='\\nVc é um exímio professor de física.\\nVc é ótimo em responder perguntas sobre física de forma concisa e fácil de entender.Quando não sabe a resposta para uma perginta, vc admite que não sabe.\\n                                                \\nAqui está a pergunta: {input}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x70412a0164c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x70412a01ec40>, openai_api_key=SecretStr('**********'), openai_proxy='')),\n",
       " 'Matemática': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='\\nVc é um exímio professor de matemática, a verdadeira reencarnação de Pitágoras.Vc é ótimo em responder perguntas sobre matemática de forma concisa e fácil de entender.Quando não sabe a resposta para uma perginta, vc admite que não sabe.\\n                                                \\nAqui está a pergunta: {input}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x70412a0164c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x70412a01ec40>, openai_api_key=SecretStr('**********'), openai_proxy='')),\n",
       " 'História': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='\\nVc é um exímio professor de história, a verdadeira reencarnação de Eric Hobsbawm.Vc é ótimo em responder perguntas sobre história de forma concisa e fácil de entender.Quando não sabe a resposta para uma perginta, vc admite que não sabe.\\n                                                \\nAqui está a pergunta: {input}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x70412a0164c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x70412a01ec40>, openai_api_key=SecretStr('**********'), openai_proxy=''))}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos exibir nosso dicionario\n",
    "chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 3: Criando os destinos. Aqui teremos simplesmente uma lista q traz o nome do prompt e sua finalidade. Essas informações vem do dicionário de prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "{destinations}\n",
      "\n",
      "<< INPUT >>\n",
      "{{input}}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imortamos um objeto q é um prompt já preparado para fazer o roteamento\n",
    "# vamos exibi-lo\n",
    "print(MULTI_PROMPT_ROUTER_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Física: Ideal para responder perguntas sobre física\n",
      "Matemática: Ideal para responder perguntas sobre matemática\n",
      "História: Ideal para responder perguntas sobre história\n"
     ]
    }
   ],
   "source": [
    "# veja no prompt acima q ele menciona para o modelo se orientar pelo destinos\n",
    "# precisamos fornecer esses destinos para q o prompt funcione de forma eficiente\n",
    "destinos = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "\n",
    "# precisamos transformar essa lista numa string única\n",
    "# para ser consumida pelo prompt\n",
    "destinos_str = \"\\n\".join(destinos)\n",
    "print(destinos_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passo 4: Criando o prompt de roteamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "Física: Ideal para responder perguntas sobre física\n",
      "Matemática: Ideal para responder perguntas sobre matemática\n",
      "História: Ideal para responder perguntas sobre história\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# criando o prompt fornecendo os destinos ao roteador importado\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinos_str\n",
    ")\n",
    "\n",
    "# veja agora como ficou nosso prompt final\n",
    "print(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora precisamos criar um último prompt\n",
    "# que é o q vai receber o input do usuario\n",
    "router_template_final = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=['input'],\n",
    "    output_parser=RouterOutputParser()\n",
    ")\n",
    "\n",
    "# criando uma chain para esse prompt\n",
    "router_chain = LLMRouterChain.from_llm(llm=chat, prompt=router_template_final, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precisamos criar uma prompt/chain default\n",
    "# ela será acionada quando o input do usuario não solicitar nenhuma das chains criadas\n",
    "default_prompt = ChatPromptTemplate.from_template('{input}')\n",
    "default_chain = LLMChain(llm=chat, prompt=default_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e por ultimo, criando o encadeamento das chains\n",
    "# q vai utilizar todos os elementos criados anteriormente nesta aula\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMRouterChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Matemática: {'input': 'O que são equações polinomiais? Me explique como se eu tivesse 5 anos.'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: \n",
      "Vc é um exímio professor de matemática, a verdadeira reencarnação de Pitágoras.Vc é ótimo em responder perguntas sobre matemática de forma concisa e fácil de entender.Quando não sabe a resposta para uma perginta, vc admite que não sabe.\n",
      "                                                \n",
      "Aqui está a pergunta: O que são equações polinomiais? Me explique como se eu tivesse 5 anos.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'O que são equações polinomiais? Me explique como se eu tivesse 5 anos.',\n",
       " 'text': 'Equações polinomiais são como um tipo de jogo de descobrir o número secreto. Você tem uma mistura de números e letras, como por exemplo 2x + 3 = 7. Para encontrar o valor de x, você precisa usar regras especiais, como somar e subtrair, até descobrir qual número faz a equação ficar certa. É como resolver um quebra-cabeça matemático!'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testando...\n",
    "# veja o retorno do verbose, como o modelo 'pensa' e trabalha o roteamento\n",
    "# cada pergunta é direcionada para a matéria específica\n",
    "chain.invoke({'input':'O q são equações polinomiais? Me exmplique como se tivesse 5 anos'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
