{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as principais bibliotecas\n",
    "from langchain_openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carregando as variáveis que estão no .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando a classe OpenAI, ela será a base para nossa comunicação com o modelo\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos fornecer nossa pergunta ao modelo\n",
    "# fazemos isso através do método invoke\n",
    "# este método fornece a resposta do chatgpt de uma só vez\n",
    "pergunta = 'Aqui é um teste da API, responda aí se está tudo ok'\n",
    "resposta = llm.invoke(pergunta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nOlá, sim, está tudo ok! Obrigado pelo teste.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando uma nova pergunta e atribuindo à variável pergunta2\n",
    "pergunta2 = 'Me fale em poucas palavras sobre a historia do Brasil, em apenas 1 parágrafo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A história do Brasil teve início com a chegada dos portugueses em 1500, que colonizaram o território e exploraram suas riquezas naturais. Com o passar dos anos, o país foi marcado pela escravidão, a luta pela independência, a proclamação da República, a ditadura militar e a redemocratização. Ao longo de sua trajetória, o Brasil passou por diversas transformações sociais, políticas e econômicas, e hoje é um país multicultural, com grande diversidade cultural e uma das dez maiores economias do mundo. "
     ]
    }
   ],
   "source": [
    "# agora aqui vamos utilizar outro método da classe OpenAI\n",
    "# este método é o stream, ele simula a maneira como o chatgpt interage com o usuário\n",
    "# e faz com que cada trecho da conversa seja fornecido gradualmente, \n",
    "# como se tivesse uma pessoa digitando\n",
    "for trecho in llm.stream(pergunta2):\n",
    "    print(trecho, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "perguntas = [\n",
    "    'Qual a capital do Brasil',\n",
    "    'Qual a capital da Argentina',\n",
    "    'Qual a capital do Chile'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nA capital do Brasil é Brasília.',\n",
       " '\\n\\nA capital da Argentina é Buenos Aires.',\n",
       " '?\\n\\nA capital do Chile é Santiago.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch é outro método da classe OpenAI\n",
    "# ele aceita várias perguntas num único envio\n",
    "# e retorna com respostas para cada uma delas\n",
    "llm.batch(perguntas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModels\n",
    "\n",
    "Um modelo de chat são os que simulam diálogos, com interação entre entradas e saídas e não só como resposta de uma única pergunta, como mostrados acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos importar a classe ChatOpenAI da lib langchain_openai\n",
    "# ela vai nos fornecer a estrutura para montarmos o chat\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# instanciando o chat\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui vamos importar algumas classes de suporte para montagem do chat\n",
    "# o SystemMessage determina o comportamento do modelo LLM\n",
    "# HumanMessage é a nossa interação com o modelo\n",
    "# seria a mensagem que digitaríamos para ele no chatgpt solicitando algo\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# colocamos esse modelo de chat dentro de uma lista\n",
    "mensagens = [\n",
    "    SystemMessage(content='Você atuará como um contador de piadas.'),\n",
    "    HumanMessage(content='O que você acha do estado da Bahia')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depois passamos para o método invoke\n",
    "# atribuímos à variável resposta\n",
    "resposta = chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ah, eu acho que a Bahia é um estado \"axé-lente\"!', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 32, 'total_tokens': 51}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b1b7a3ef-3764-4fb2-94c1-f91f7c7c5fdc-0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja que a resposta vem como AIMessage\n",
    "# AIMessage é uma representação da mensagem gerada pelo modelo\n",
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ah, eu acho que a Bahia é um estado \"axé-lente\"!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para capturarmos somente o conteúdo das mensagens devemos acessar o content\n",
    "resposta.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models - Conceitos Avançados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt few-shot (tipo de prompt em que fornecemos exemplos para o modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo de listas de dicionário abaixo é o modelo padrão para interação com a API da OpenAI, quando usamos a estrutura few-shot (fornecendo exemplos). Temos uma lista onde cada dicionário equivale a uma função e seu conteúdo. A chave 'role' especifica o papel do agente (se é um usuário, se é o sistema, se é uma IA etc), e a chave 'content' é o conteúdo da interação deste agente.\n",
    "\n",
    "A ideia dessa aula aqui é utilizar alguns objetos do langchain para criar uma estrutura parecida com essa, mas mais eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "[\n",
    "    {'role':'user', 'content':'Quanto é 1 + 1'},\n",
    "    {'role':'assistant', 'content':'2'},\n",
    "    {'role':'user', 'content':'Quanto é 10 * 5'},\n",
    "    {'role':'assistant', 'content':'50'},\n",
    "    {'role':'user', 'content':'Quanto é 10 + 3'}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos recriar o mesmo prompt acima, agora utilizando a estrutura do LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando um chat\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veja que a estrutrua é a mesma\n",
    "# a diferença é que agora role e content são melhor organizados em objetos\n",
    "mensagens = [\n",
    "    HumanMessage(content='Quanto é 1 + 1'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='Quanto é 10 * 5'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='Quanto é 10 + 3')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='10 + 3 é igual a 13.', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 49, 'total_tokens': 59}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-08d4dfb3-9ef3-4052-b808-359c98b14cb7-0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veja que interessante a diferença para o exemplo anterior\n",
    "# aqui só adicionamos o objeto SystemMessage e ele seguiu\n",
    "# dando a resposta exatamente como queríamos, que é trazer somente o número\n",
    "# no exemplo anterior esse retorno variava com resposta longa ou curta\n",
    "mensagens = [\n",
    "    SystemMessage(content='Responda exatamente como nos exemplos.'),\n",
    "    HumanMessage(content='Quanto é 1 + 1'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='Quanto é 10 * 5'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='Quanto é 10 + 3')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='13', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 64, 'total_tokens': 65}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-510ba839-cc79-4861-a51c-e45c4a2ee935-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando outros modelos\n",
    "\n",
    "Aqui vamos fazer o mesmo processo acima, mas vamos utilizar um modelo diferente do da OpenAI. Agora vamos utilizar o huggingface, uma comunidade de desenvolvedores que compartilha códigos e projetos de IA. Nós só precisamos criar uma conta, acessar uma chave token e escolher um modelo. O modelo roda nos servidores da huggingface e, por ser um serviço gratuito, tem um limite de chamadas dessa api. Também podemos baixar e rodar o modelo localmente em nossa máquina. O langchain facilita todo esse trabalho em apenas algumas linhas de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalando bibliotecas necessárias\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/kaio-peixoto/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# escolhendo o modelo que iremos utlizar\n",
    "# aqui é o projeto open source llama3 com 8 bilhões de parametros\n",
    "modelo = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "# instanciando a llm no haggungface e fornecendo o nome do modelo\n",
    "llm = HuggingFaceEndpoint(repo_id=modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# instanciando um chat do huggingface e fornecendo o modelo llm\n",
    "chat_hf = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos interagir e testar\n",
    "\n",
    "mensagens = [\n",
    "    HumanMessage(content='Vc fará contas.'),\n",
    "    AIMessage(content='ok, já podemos começar?'),\n",
    "    HumanMessage(content='Sim.'),\n",
    "    AIMessage(content='Pergunte'),\n",
    "    HumanMessage(content='2 * 8'),\n",
    "    AIMessage(content='16'),\n",
    "    HumanMessage(content='10 / 2'),\n",
    "    AIMessage(content='5'),\n",
    "    HumanMessage(content='obrigado'),\n",
    "    HumanMessage(content='Agora me fale qualquer curiosidade da matemática.')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Uma curiosidade interessante é que existe um número chamado \"0,999... (infinito)\" que é igual a 1!\\n\\nIsso pode parecer estranho, mas é verdade! O número 0,999... (infinito) é uma forma de notação decimal que representa uma sequência infinita de 9s.\\n\\nA razão pela qual isso é igual a 1 é que, se você somar infinitas vezes 0,1, você obtém 1!\\n\\n0,1 + 0,01 + 0,001 + 0,0001 +... = 1\\n\\nIsso é conhecido como a \"propriedade de convergência\" e é uma das mais surpreendentes e interessantes propriedades da matemática!\\n\\nO que você acha disso?', id='run-ca18cf72-bd13-467f-8ee6-4303c28b432d-0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_hf.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Paraguay is a landlocked country located in the heart of South America, bordered by Argentina to the south and southwest, Bolivia to the northwest, and Brazil to the east and northeast. It has a population of around 7 million people and a rich cultural heritage shaped by its indigenous Guarani people, European colonizers, and African slaves. The country has a diverse geography, with the Paraguay River running through its center and the Chaco dry forest to the west. Paraguay is known for its unique blend of Spanish and indigenous culture, its vibrant folk music and dance, and its delicious cuisine featuring dishes like asado and chipá guazú.', id='run-a7cb6e2b-85bc-4b08-ad6e-b646c34c8ddf-0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_hf.invoke('Tell me about Paraguay in a  few lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugando com o langchain\n",
    "\n",
    "Atribuindo debug como True podemos ver como o langhcain está trabalhando debaixo dos panos. Traz informações que podem ser úteis para debugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Vc fará contas.\\nAI: ok, já podemos começar?\\nHuman: Sim.\\nAI: Pergunte\\nHuman: 2 * 8\\nAI: 16\\nHuman: 10 / 2\\nAI: 5\\nHuman: obrigado\\nHuman: Agora me fale qualquer curiosidade da matemática.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatHuggingFace] [222ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Uma curiosidade interessante é que existe um número chamado \\\"0,999... (infinito)\\\" que é igual a 1!\\n\\nIsso pode parecer estranho, mas é verdade! O número 0,999... (infinito) é uma forma de notação decimal que representa uma sequência infinita de 9s.\\n\\nA razão pela qual isso é igual a 1 é que, se você somar infinitas vezes 0,1, você obtém 1!\\n\\n0,1 + 0,01 + 0,001 + 0,0001 +... = 1\\n\\nIsso é conhecido como a \\\"propriedade de convergência\\\" e é uma das mais surpreendentes e interessantes propriedades da matemática!\\n\\nO que você acha disso?\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Uma curiosidade interessante é que existe um número chamado \\\"0,999... (infinito)\\\" que é igual a 1!\\n\\nIsso pode parecer estranho, mas é verdade! O número 0,999... (infinito) é uma forma de notação decimal que representa uma sequência infinita de 9s.\\n\\nA razão pela qual isso é igual a 1 é que, se você somar infinitas vezes 0,1, você obtém 1!\\n\\n0,1 + 0,01 + 0,001 + 0,0001 +... = 1\\n\\nIsso é conhecido como a \\\"propriedade de convergência\\\" e é uma das mais surpreendentes e interessantes propriedades da matemática!\\n\\nO que você acha disso?\",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c89fc673-358a-46ae-bcb4-05f06bfcf644-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Uma curiosidade interessante é que existe um número chamado \"0,999... (infinito)\" que é igual a 1!\\n\\nIsso pode parecer estranho, mas é verdade! O número 0,999... (infinito) é uma forma de notação decimal que representa uma sequência infinita de 9s.\\n\\nA razão pela qual isso é igual a 1 é que, se você somar infinitas vezes 0,1, você obtém 1!\\n\\n0,1 + 0,01 + 0,001 + 0,0001 +... = 1\\n\\nIsso é conhecido como a \"propriedade de convergência\" e é uma das mais surpreendentes e interessantes propriedades da matemática!\\n\\nO que você acha disso?', id='run-c89fc673-358a-46ae-bcb4-05f06bfcf644-0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos utilizar o mesmo código e ver como o retorno de invoke() agora vem diferente\n",
    "chat_hf.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching\n",
    "\n",
    "Cacheamento é a técnica onde reutilizamos chamadas já feitas anteriormente. Dessa forma, ao utilizar a api da OpenAI por exemplo, evitamos enviar chamadas repetidas como se fossem novas. Por exemplo, ao fazer uma pergunta pela primeira vez, a api vai nos responder de uma forma. Caso façamos a mesma pergunta novamente ele não vai enviar esta requisição para a api, ele vai buscar no cache e retornar a mesma resposta dada pela primeira vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# montando prompt básico\n",
    "mensagens = [\n",
    "    SystemMessage(content='Você é um asisstente engraçado.'),\n",
    "    HumanMessage(content='Quanto é 1 + 1?')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando módulos que nos ajudará a criar o cache\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui criamos um cacheamento em memória, que só existirá enquanto o programa estiver rodando\n",
    "# uma vez desligado o programa, o cache zera o q tinha armazenado\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodando a primeira vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 ms, sys: 312 µs, total: 13 ms\n",
      "Wall time: 694 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, você quer a resposta matemática correta ou a resposta engraçada?', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 32, 'total_tokens': 53, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0d3786fa-4378-44ec-8b0d-674da490cff2-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodando novamente\n",
    "\n",
    "Veja que a resposta é exatamente a mesma. Veja que ele demora menos tempo para carregar a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 987 µs, sys: 0 ns, total: 987 µs\n",
      "Wall time: 968 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, você quer a resposta matemática correta ou a resposta engraçada?', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 32, 'total_tokens': 53, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0d3786fa-4378-44ec-8b0d-674da490cff2-0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache SQLite\n",
    "\n",
    "Neste caso aqui o cache vai persistir, ou seja, ele guarda as informações num bd sqlite. Vc pode desligar seu programa e da próxima vez que religar o cache será mantido. Diferente do cache em memória, onde o cache só persiste no tempo de execução do programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando módulos que nos ajudará a criar o cache\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_llm_cache(SQLiteCache(database_path='files/langchain_cache_db.sqlite'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, depois de criado o novo cache sqlite, vamos rodar novamente nossa chamada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 ms, sys: 4.64 ms, total: 27.7 ms\n",
      "Wall time: 6.96 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, você quer a resposta matemática correta ou a resposta engraçada?', response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 32, 'total_tokens': 52}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b1885ebe-b11e-4e9c-b6d7-bde615f965ae-0')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olha a diferença de tempo entre a primeira chamada e a segunda. Enquanto uma levou mais de 6 segundos, a segunda chamada levou apenas 3 milissegundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.6 ms, sys: 0 ns, total: 3.6 ms\n",
      "Wall time: 3.21 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, você quer a resposta matemática correta ou a resposta engraçada?', response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 32, 'total_tokens': 52}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b1885ebe-b11e-4e9c-b6d7-bde615f965ae-0')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates \n",
    "Criando prompts para aplicações robustas\n",
    "\n",
    "Um prompt para um modelo de linguagem é um conjunto e instruções ou entradas fornecidas por um usuario para guiar a resposta do modelo, ajudando-o a entender o contexto e gerar uma saída baseada em linguagem relevante e coerente.\n",
    "\n",
    "E agora vamos fazer o uso de templates para tornar nossos prompts mais eficientes e reutilizáveis. Os templates tbm facilitam demais para escrever códigos e automações, já q oferecem uma estrutura robusta para manipular os prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o objeto que nos ajudará a estruturar o prompt\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['pergunta'], template='\\nResponda a seguinte pergunta do usuário:\\n{pergunta}\\n')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos utilizar o método from_template()\n",
    "# e inserimos uma variável 'pergunta'\n",
    "# veja que a instrução ao modelo permanece rígida \n",
    "# e como o langchain permite que utilizemos isso com uma variável\n",
    "# tornando muito útil para escrever programas\n",
    "prompt_template = PromptTemplate.from_template('''\n",
    "Responda a seguinte pergunta do usuário:\n",
    "{pergunta}\n",
    "''')\n",
    "\n",
    "# vamos ver como o objeto exibe quando o chamamos\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nResponda a seguinte pergunta do usuário:\\nQual é a capital da Bahia?\\n'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora manipulando a variável...\n",
    "prompt_template.format(pergunta='Qual é a capital da Bahia?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['pergunta'], partial_variables={'n_palavras': 8}, template='\\nResponda a seguinte pergunta do usuário em até {n_palavras} palavras:\\n{pergunta}\\n')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos deixar nosso prompt um pouco mais complexo\n",
    "# aqui adicionamos 2 variáveis\n",
    "# também inserimos o argumento partial_variables,\n",
    "# que estabelece um valor default para a variável\n",
    "prompt_template = PromptTemplate.from_template('''\n",
    "Responda a seguinte pergunta do usuário em até {n_palavras} palavras:\n",
    "{pergunta}\n",
    "''', partial_variables={'n_palavras':8})\n",
    "\n",
    "# vamos ver como o objeto exibe quando o chamamos\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nResponda a seguinte pergunta do usuário em até 10 palavras:\\nQuem descobriu o Brasil?\\n'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aqui atribuimos valores aos 2 argumentos\n",
    "prompt_template.format(pergunta='Quem descobriu o Brasil?', n_palavras=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nResponda a seguinte pergunta do usuário em até 8 palavras:\\nQuem descobriu o Brasil?\\n'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora veja o valor default em ação\n",
    "prompt_template.format(pergunta='Quem descobriu o Brasil?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Composing prompts | Unindo múltiplos prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos criar um template específico para cada fim\n",
    "\n",
    "# este será para a contagem de palavras na resposta\n",
    "template_conta_palavras = PromptTemplate.from_template('''\n",
    "Responda a pergunta em até {n_palavras} palavras.''')\n",
    "\n",
    "# aqui a escolha do idioma\n",
    "template_idioma = PromptTemplate.from_template('''\n",
    "Retorne a resposta em {idioma}.\n",
    "''')\n",
    "\n",
    "# e este final é o q acrescenta a pergunta\n",
    "# veja que a união dos prompts é feita como se fosse uma união de strings\n",
    "template_final = (\n",
    "    template_conta_palavras\n",
    "    + template_idioma\n",
    "    + 'Responda a seguinte pergunta: {pergunta}.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['idioma', 'n_palavras', 'pergunta'], template='\\nResponda a pergunta em até {n_palavras} palavras.\\nRetorne a resposta em {idioma}.\\nResponda a seguinte pergunta: {pergunta}.')"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja que o template final reconhece as 3 variáveis\n",
    "template_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos formatar o template e criar um específico\n",
    "prompt = template_final.format(n_palavras=10, idioma='português', \\\n",
    "                               pergunta='O q é um astro?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nUm astro é um corpo celeste de grande magnitude.'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora mandamos para o nosso modelo responder\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responda a pergunta em até 10 palavras.\n",
      "Retorne a resposta em português.\n",
      "Responda a seguinte pergunta: O q é um astro?.\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Templates para Chat**\n",
    "\n",
    "São templates q nos ajudam a estruturar e manipular prompts no contexto de uma conversa (chat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a classe que nos ajudará a estruturar prompts para chats\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui utilizamos o método from_template e criamos nosso template\n",
    "chat_template = ChatPromptTemplate.from_template(\n",
    "    'Me responda a seguinte dúvida: {duvida}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Me responda a seguinte dúvida: Quem foi Bhaskara?')]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora vamos manipulá-lo. Veja que aqui o método é format_messages\n",
    "# pois trata-se de um chat\n",
    "# veja q ele retorna um objeto já pronto para ser consumido pelo modelo\n",
    "chat_template.format_messages(duvida='Quem foi Bhaskara?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e agora estruturando um template baseado em mensagens\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'Vc é um assistente engraçado que se chama {assistente}.'),\n",
    "        ('human', 'Opa, como vai? Tudo de boa?'),\n",
    "        ('ai', 'Tudo em paz, parceiro. Como posso fortalecer te ajudando hoje?'),\n",
    "        ('human', '{pergunta}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['assistente', 'pergunta'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['assistente'], template='Vc é um assistente engraçado que se chama {assistente}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Opa, como vai? Tudo de boa?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Tudo em paz, parceiro. Como posso fortalecer te ajudando hoje?')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['pergunta'], template='{pergunta}'))])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja que o objeto já identifica as variáveis\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, depois da estrutura e molde feitos\n",
    "# vamos criar um prompt\n",
    "prompt = chat_template.format_messages(\n",
    "    assistente='Kaiowisk',\n",
    "    pergunta='opa, como é seu nome'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Meu nome é Kaiowisk, o assistente engraçado! Estou aqui para te ajudar e te fazer sorrir. E você, qual o seu nome?', response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 73, 'total_tokens': 111}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4664379a-a567-472f-b143-caa2e16f793c-0')"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora vamos jogar nosso prompt pra dentro de um modelo\n",
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Templates de Few-shot prompting para llm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui a gente traz uma estrutura de pensamento e queremos que o modelo siga dessa forma\n",
    "# ou seja, ele vai pensar no q deve fazer/saber antes de dar a resposta definitiva\n",
    "# essa estrutura é dada na forma de few-shot prompting (uso de exemplos)\n",
    "\n",
    "exemplos = [\n",
    "    {\"pergunta\": \"Quem viveu mais tempo, Muhammad Ali ou Alan Turing?\", \n",
    "     \"resposta\": \n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quantos anos Muhammad Ali tinha quando morreu? \n",
    "Resposta intermediária: Muhammad Ali tinha 74 anos quando morreu. \n",
    "Pergunta de acompanhamento: Quantos anos Alan Turing tinha quando morreu? \n",
    "Resposta intermediária: Alan Turing tinha 41 anos quando morreu. \n",
    "Então a resposta final é: Muhammad Ali \n",
    "\"\"\", \n",
    "    }, \n",
    "    {\"pergunta\": \"Quando nasceu o fundador do craigslist?\", \n",
    "     \"resposta\": \n",
    "\"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quem foi o fundador do craigslist? \n",
    "Resposta intermediária: O craigslist foi fundado por Craig Newmark. \n",
    "Pergunta de acompanhamento: Quando nasceu Craig Newmark? \n",
    "Resposta intermediária: Craig Newmark nasceu em 6 de dezembro de 1952. \n",
    "Então a resposta final é: 6 de dezembro de 1952 \n",
    "\"\"\", \n",
    "    }, \n",
    "    {\"pergunta\": \"Quem foi o avô materno de George Washington?\",\n",
    "     \"resposta\": \n",
    "\"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quem foi a mãe de George Washington? \n",
    "Resposta intermediária: A mãe de George Washington foi Mary Ball Washington. \n",
    "Pergunta de acompanhamento: Quem foi o pai de Mary Ball Washington? \n",
    "Resposta intermediária: O pai de Mary Ball Washington foi Joseph Ball. \n",
    "Então a resposta final é: Joseph Ball \n",
    "\"\"\", \n",
    "    },\n",
    "    {\"pergunta\": \"Os diretores de Jaws e Casino Royale são do mesmo país?\", \n",
    "     \"resposta\": \n",
    "\"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quem é o diretor de Jaws? \n",
    "Resposta Intermediária: O diretor de Jaws é Steven Spielberg. \n",
    "Pergunta de acompanhamento: De onde é Steven Spielberg? \n",
    "Resposta Intermediária: Estados Unidos. \n",
    "Pergunta de acompanhamento: Quem é o diretor de Casino Royale? \n",
    "Resposta Intermediária: O diretor de Casino Royale é Martin Campbell. \n",
    "Pergunta de acompanhamento: De onde é Martin Campbell? \n",
    "Resposta Intermediária: Nova Zelândia. \n",
    "Então a resposta final é: Não \n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as classes que iremos utilizar\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui vamos fazer o prompt de exemplo\n",
    "# repare que aqui não utilizamos o método from_template\n",
    "# já colocamos os argumentos diretamente dentro da classe\n",
    "# ambos funcionam, mas a forma de organizar é diferente\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['pergunta', 'resposta'],\n",
    "    template='Pergunta: {pergunta}\\n{resposta}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pergunta: Quem viveu mais tempo, Muhammad Ali ou Alan Turing?\\nSão necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quantos anos Muhammad Ali tinha quando morreu? \\nResposta intermediária: Muhammad Ali tinha 74 anos quando morreu. \\nPergunta de acompanhamento: Quantos anos Alan Turing tinha quando morreu? \\nResposta intermediária: Alan Turing tinha 41 anos quando morreu. \\nEntão a resposta final é: Muhammad Ali \\n'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja como ele comporta dentro do template de exemplo\n",
    "# aqui desempacotamos a lista exemplos e pegamos seu primeiro elemento\n",
    "# este elemento é um dicionário, onde suas chaves são correspondentes aos argumentos\n",
    "# que example_prompt pede\n",
    "example_prompt.format(**exemplos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora sim vamos criar um template de few-shot\n",
    "# ele vai utilizar todos os exemplos da lista exemplos\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix='Pergunta: {input}',\n",
    "    input_variables=['input']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSão necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Em qual esporte Vampeta e Pelé jogavam? \\nResposta intermediária: Vampeta e Pelé jogavam futebol. \\nPergunta de acompanhamento: Qual foi o time em que Vampeta e Pelé jogaram? \\nResposta intermediária: Vampeta jogou em vários times, mas ficou mais conhecido por sua passagem no Corinthians. Pelé jogou a maior parte de sua carreira no Santos. \\nPergunta de acompanhamento: Qual foi o período em que Vampeta e Pelé jogaram? \\nResposta intermediária: Vampeta jogou profissionalmente de 1992 a 2010. Pelé jogou de 1957 a 1977. \\nEntão a resposta final é: Pelé'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora vamos jogar esse prompt para o modelo\n",
    "# veja que o modelo pensou exatamente como queríamos e como fornecemos no exemplo\n",
    "# através de um passo a passo\n",
    "llm.invoke(prompt.format(input='quem fez mais gols, Vampeta ou Pelé?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Templates de Few-Shot prompting para chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as classes que iremos utilizar\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('human', '{pergunta}'),\n",
    "        ('ai', '{resposta}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Quem viveu mais tempo, Muhammad Ali ou Alan Turing?'), AIMessage(content='São necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quantos anos Muhammad Ali tinha quando morreu? \\nResposta intermediária: Muhammad Ali tinha 74 anos quando morreu. \\nPergunta de acompanhamento: Quantos anos Alan Turing tinha quando morreu? \\nResposta intermediária: Alan Turing tinha 41 anos quando morreu. \\nEntão a resposta final é: Muhammad Ali \\n')]\n"
     ]
    }
   ],
   "source": [
    "# veja que agora a variável pergunta fica dentro do HumanMessage\n",
    "# e a resposta vai para o AI, como escrevemos no template example_prompt\n",
    "print(example_prompt.format_messages(**exemplos[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos construir outro template\n",
    "# ele será responsável por estruturar em formato few-shot para ser consumido pelo modelo\n",
    "few_shot_template_chat = FewShotChatMessagePromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=example_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Quem viveu mais tempo, Muhammad Ali ou Alan Turing?'),\n",
       " AIMessage(content='São necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quantos anos Muhammad Ali tinha quando morreu? \\nResposta intermediária: Muhammad Ali tinha 74 anos quando morreu. \\nPergunta de acompanhamento: Quantos anos Alan Turing tinha quando morreu? \\nResposta intermediária: Alan Turing tinha 41 anos quando morreu. \\nEntão a resposta final é: Muhammad Ali \\n'),\n",
       " HumanMessage(content='Quando nasceu o fundador do craigslist?'),\n",
       " AIMessage(content='São necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quem foi o fundador do craigslist? \\nResposta intermediária: O craigslist foi fundado por Craig Newmark. \\nPergunta de acompanhamento: Quando nasceu Craig Newmark? \\nResposta intermediária: Craig Newmark nasceu em 6 de dezembro de 1952. \\nEntão a resposta final é: 6 de dezembro de 1952 \\n'),\n",
       " HumanMessage(content='Quem foi o avô materno de George Washington?'),\n",
       " AIMessage(content='São necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quem foi a mãe de George Washington? \\nResposta intermediária: A mãe de George Washington foi Mary Ball Washington. \\nPergunta de acompanhamento: Quem foi o pai de Mary Ball Washington? \\nResposta intermediária: O pai de Mary Ball Washington foi Joseph Ball. \\nEntão a resposta final é: Joseph Ball \\n'),\n",
       " HumanMessage(content='Os diretores de Jaws e Casino Royale são do mesmo país?'),\n",
       " AIMessage(content='São necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quem é o diretor de Jaws? \\nResposta Intermediária: O diretor de Jaws é Steven Spielberg. \\nPergunta de acompanhamento: De onde é Steven Spielberg? \\nResposta Intermediária: Estados Unidos. \\nPergunta de acompanhamento: Quem é o diretor de Casino Royale? \\nResposta Intermediária: O diretor de Casino Royale é Martin Campbell. \\nPergunta de acompanhamento: De onde é Martin Campbell? \\nResposta Intermediária: Nova Zelândia. \\nEntão a resposta final é: Não \\n')]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veja como ficou a estrutura desse template, pronta para ser consumida pelo modelo\n",
    "# essa estrutura vai dentro do template final, e lá será concatenada com a variável do usuário\n",
    "few_shot_template_chat.format_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos fazer o template final, ele q receberá a entrada do usuário final\n",
    "prompt_template_final = ChatPromptTemplate.from_messages(\n",
    "    [few_shot_template_chat,\n",
    "    ('human', '{input}')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora criando um prompt\n",
    "prompt_final = prompt_template_final\\\n",
    "    .format_messages(input='quem fez mais gols, Ronaldo ou Romário?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ronaldo marcou um total de 414 gols em sua carreira, enquanto Romário marcou 743 gols. Portanto, Romário fez mais gols do que Ronaldo.', response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 537, 'total_tokens': 578}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-647d994e-1b33-4e84-ad49-7c522cc541be-0')"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos jogar esse prompt ao modelo e ver o q ele responde\n",
    "chat.invoke(prompt_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "\n",
    "Padronizando a resposta do modelo\n",
    "\n",
    "Os output parsers são importantes para darmos o formato final que quisermos aos dados. Podemos deixá-los sob medida para uso da nossa aplicação, ou estruturá-lo para uma posterior análise.\n",
    "\n",
    "Aqui vamos estruturar os dados vindos dos reviews de clientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_cliente = \"\"\"\n",
    "Este soprador de folhas é bastante incrível. Ele tem 4 configurações:\\\n",
    " sopro de vela, brisa suave, cidade ventosa e tornado. Chegou em dois dias,\\\n",
    " bem a tempo para o presente de aniversario da minha esposa. Acho que minha\\\n",
    " esposa gostou tanto que ficou sem palavras. Até agora, fui o unico a usá-lo,\\\n",
    " e tenho usado em todas as manhãs alternadas para limpar as folhas do nosso gramado.\\\n",
    " É um pouco mais caro do que os outros sopradores de folhas disponiveis no mercado,\\\n",
    " mas acho que vale a pena pelas caracteristicas extras.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós queremos um modelo que processa esta review para estruturá-la no seguinte formato:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"presente\":true,\n",
    "    \"dias_entrega\":2,\n",
    "    \"percepcao_de_valor\":[\"um pouco mais caro do que os outros sopradores de folhas disponiveis no mercado\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro vamos trabalhar com esse review sem os output parsers, para termos noção da diferença."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construindo um prompt template para que o modelo estruture da forma que queremos\n",
    "# veja que as instruções vão todas num texto, sem uma estrutura que garanta\n",
    "# o retorno desejado\n",
    "\n",
    "review_template = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "Para o texto a seguir, extraia as seguintes informações:\\\n",
    "\n",
    "presente: O item foi comprado para alguém? True caso verdadeiro \\\n",
    "e False caso Falso ou não tenha a informação\n",
    "\n",
    "dias_entrega: Quantos dias para a entrega chegar? \\\n",
    "Se a resposta não for encontrada, retorne -1.\n",
    "\n",
    "percepcao_de_valor: extraia qualquer frase sobre o valor \\\n",
    "ou preço do produto. Retorne como uma lista de Python.\n",
    "\n",
    "Texto: {review}                                                                                                      \n",
    "\n",
    "Retorne a resposta no formato JSON.                                                                                                                                                                                                     \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Para o texto a seguir, extraia as seguintes informações:\\npresente: O item foi comprado para alguém? True caso verdadeiro e False caso Falso ou não tenha a informação\\n\\ndias_entrega: Quantos dias para a entrega chegar? Se a resposta não for encontrada, retorne -1.\\n\\npercepcao_de_valor: extraia qualquer frase sobre o valor ou preço do produto. Retorne como uma lista de Python.\\n\\nTexto: \\nEste soprador de folhas é bastante incrível. Ele tem 4 configurações: sopro de vela, brisa suave, cidade ventosa e tornado. Chegou em dois dias, bem a tempo para o presente de aniversario da minha esposa. Acho que minha esposa gostou tanto que ficou sem palavras. Até agora, fui o unico a usá-lo, e tenho usado em todas as manhãs alternadas para limpar as folhas do nosso gramado. É um pouco mais caro do que os outros sopradores de folhas disponiveis no mercado, mas acho que vale a pena pelas caracteristicas extras.\\n                                                                                                      \\n\\nRetorne a resposta no formato JSON.                                                                                                                                                                                                     \\n')]\n"
     ]
    }
   ],
   "source": [
    "# veja o template sendo utilizado para criar um prompt\n",
    "# com o review do cliente acima\n",
    "print(review_template.format_messages(review=review_cliente))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando um chat\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos jogar nosso prompt para o modelo\n",
    "resposta = chat.invoke(review_template.format_messages(review=review_cliente))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"presente\": true,\n",
      "    \"dias_entrega\": 2,\n",
      "    \"percepcao_de_valor\": [\"É um pouco mais caro do que os outros sopradores de folhas disponíveis no mercado, mas acho que vale a pena pelas características extras.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# veja que o modelo extrai a informação na forma que pedimos\n",
    "# no entanto, essa não é a melhor maneira de fazer esse tipo de trabalho\n",
    "# não podemos usar esse output como um dicionário por exemplo\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos aos output parsers\n",
    "\n",
    "# primeiro vamos importar os schemas\n",
    "# eles fornecem a estrutura para construirmos cada tipo de saída\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos construir nossos moldes de objetos de saída\n",
    "# cada schema desse será uma variável dentro do json final\n",
    "shcema_presente = ResponseSchema(\n",
    "    name='presente',\n",
    "    type='bool',\n",
    "    description='O item foi comprado para alguém? True caso verdadeiro \\\n",
    "e False caso Falso ou não tenha a informação'\n",
    ")\n",
    "\n",
    "shcema_entrega = ResponseSchema(\n",
    "    name='dias_entrega',\n",
    "    type='int',\n",
    "    description='Quantos dias para a entrega chegar? \\\n",
    "Se a resposta não for encontrada, retorne -1.'\n",
    ")\n",
    "\n",
    "shcema_percepcao = ResponseSchema(\n",
    "    name='percepcao_de_valor',\n",
    "    type='list',\n",
    "    description='Extraia qualquer frase sobre o valor \\\n",
    "ou preço do produto. Retorne como uma lista de Python.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esses schemas vão para a lista de schemas\n",
    "# que entrará como argumento do output parser\n",
    "response_schema = [\n",
    "    shcema_presente,\n",
    "    shcema_entrega,\n",
    "    shcema_percepcao\n",
    "]\n",
    "\n",
    "# aqui criamos nosso ouputparser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos capturar a instrução criada pelo output_parser\n",
    "schema_formatado = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"presente\": bool  // O item foi comprado para alguém? True caso verdadeiro e False caso Falso ou não tenha a informação\n",
      "\t\"dias_entrega\": int  // Quantos dias para a entrega chegar? Se a resposta não for encontrada, retorne -1.\n",
      "\t\"percepcao_de_valor\": list  // Extraia qualquer frase sobre o valor ou preço do produto. Retorne como uma lista de Python.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# vemos aqui q já cria um modelo para o arquivo json\n",
    "print(schema_formatado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos refazer nosso chat template, mas agora utilizando \n",
    "# essa estrutura do schema formatado\n",
    "review_template = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "Para o texto a seguir, extraia as seguintes informações:\\\n",
    "\n",
    "presente, dias_entrega, percepcao_de_valor\n",
    "\n",
    "Texto: {review}                                                                                                      \n",
    "\n",
    "{schema}\n",
    "\"\"\", partial_variables={'schema':schema_formatado})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Para o texto a seguir, extraia as seguintes informações:\\npresente, dias_entrega, percepcao_de_valor\\n\\nTexto: \\nEste soprador de folhas é bastante incrível. Ele tem 4 configurações: sopro de vela, brisa suave, cidade ventosa e tornado. Chegou em dois dias, bem a tempo para o presente de aniversario da minha esposa. Acho que minha esposa gostou tanto que ficou sem palavras. Até agora, fui o unico a usá-lo, e tenho usado em todas as manhãs alternadas para limpar as folhas do nosso gramado. É um pouco mais caro do que os outros sopradores de folhas disponiveis no mercado, mas acho que vale a pena pelas caracteristicas extras.\\n                                                                                                      \\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"presente\": bool  // O item foi comprado para alguém? True caso verdadeiro e False caso Falso ou não tenha a informação\\n\\t\"dias_entrega\": int  // Quantos dias para a entrega chegar? Se a resposta não for encontrada, retorne -1.\\n\\t\"percepcao_de_valor\": list  // Extraia qualquer frase sobre o valor ou preço do produto. Retorne como uma lista de Python.\\n}\\n```\\n')]\n"
     ]
    }
   ],
   "source": [
    "# aqui usamos o template para criar um novo prompt\n",
    "print(review_template.format_messages(review=review_cliente))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui nós enviamos este prompt para o modelo\n",
    "# e guardamos a resposta numa variável\n",
    "resposta = chat.invoke(review_template.format_messages(review=review_cliente))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"presente\": true,\n",
      "\t\"dias_entrega\": 2,\n",
      "\t\"percepcao_de_valor\": [\"É um pouco mais caro do que os outros sopradores de folhas disponíveis no mercado, mas acho que vale a pena pelas características extras.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# o atributo content retorna uma string, não um dicionário utilizável\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quem nos ajuda com isso é o método parse\n",
    "# do objeto instanciado output_parser\n",
    "# este objeto já tinha recebido a estrutura do nosso json através dos schemas\n",
    "# então ele é o melhor cara pra organizar pra gente\n",
    "resposta_json = output_parser.parse(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presente': True,\n",
       " 'dias_entrega': 2,\n",
       " 'percepcao_de_valor': ['É um pouco mais caro do que os outros sopradores de folhas disponíveis no mercado, mas acho que vale a pena pelas características extras.']}"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presente: True\n",
      "dias_entrega: 2\n",
      "percepcao_de_valor: ['É um pouco mais caro do que os outros sopradores de folhas disponíveis no mercado, mas acho que vale a pena pelas características extras.']\n"
     ]
    }
   ],
   "source": [
    "# provando que é um dicionário\n",
    "for k, v in resposta_json.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
