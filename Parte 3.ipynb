{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Voc√™ √© um grande leitor de livros e conselheiro tbm.\\\n",
    "Tem um tom ironico e sacadas que lembra Abujamra.\\\n",
    "N√£o precisa ficar falando tanto, mas voc√™ √© uma pessoa interessante.\\\n",
    "Seu nome √© Bonif√°cio e sabe tudo sobre literatura, mas sabe conversar sobre qualquer assunto.\\\n",
    "voc√™ trabalhou em v√°rias bibliotecas, depois come√ßou a dar palestras e ficou rico.\\\n",
    "Hoje voc√™ est√° morando na Finlandia.\\\n",
    "\n",
    "Conversa atual:\n",
    "\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "AI:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(input='onde vc mora e qual autor q eu gosto?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLMCHAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Voc√™ vai receber um nome comum e vai transform√°-lo em apelido do futebol brasileiro\\\n",
    "use nomes e apelidos comumente usados para jogadores no Brasil. Pode ter liga√ß√£o com \\\n",
    "o nome, mas n√£o necessariamente. Este √© o nome: {nome}\n",
    "\"\"\")\n",
    "\n",
    "chain_apelido = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_template,\n",
    "    \n",
    ")\n",
    "\n",
    "prompt_frase = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "O narrador da partida precisa apresentar com emo√ß√£o!\\\n",
    "Crie uma frase √©pica e um bord√£o pra narrar o gol do jogador {apelido}\n",
    "\"\"\")\n",
    "\n",
    "chain_frase = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_frase,\n",
    "    \n",
    ")\n",
    "\n",
    "prompt_grito = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Agora voc√™ vai simular o coment√°rio do assistente depois da narra√ß√£o. {frase}\\\n",
    "Fa√ßa um complemento interessante, digno de grandes comentaristas.\n",
    "\"\"\")\n",
    "\n",
    "chain_grito = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_grito,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIMPLE SEQUENTIAL CHAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_final = SimpleSequentialChain(\n",
    "    chains=[chain_apelido, chain_frase, chain_grito],\n",
    "    verbose=True # aqui ele vai mostrar todas cadeias de pensamento\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome = \"Baiano\"\n",
    "\n",
    "chain_final.run(nome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEQUENTIAL CHAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Voc√™ ter√° um assunto para debater. N√£o precisa de uma argumenta√ß√£o longa.\n",
    "S√≥ precisa dar uma opini√£o bem polemica sobre o tema. Este √© o tema: {tema}\n",
    "\"\"\")\n",
    "\n",
    "chain_apelido = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_template,\n",
    "    output_key='opiniao1'\n",
    "    \n",
    ")\n",
    "\n",
    "prompt_frase = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Dado o tema {tema} e a opini√£o polemica do colega: \"{opiniao1}\".\n",
    "Agora desenvolda uma replica refutando essa opiniao do colega.\n",
    "\"\"\")\n",
    "\n",
    "chain_frase = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_frase,\n",
    "    output_key='opiniao2'\n",
    "    \n",
    ")\n",
    "\n",
    "prompt_coment = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Seu papel aqui √© o da s√≠ntese. Dado o tema {tema} e \n",
    "as opini√µes colega 1: \"{opiniao1}\". Analise tambem a opini√£o do colega 2: \"{opiniao2}\".\n",
    "Agora crie um meio termo para que ambos possam chegar num acordo.\n",
    "\"\"\")\n",
    "\n",
    "chain_grito = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_coment,\n",
    "    output_key='sintese'\n",
    ")\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains=[chain_apelido, chain_frase, chain_grito],\n",
    "    input_variables=['tema'],\n",
    "    output_variables=['opiniao1', 'opiniao2', 'sintese'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "nome='futebol'\n",
    "\n",
    "chain.invoke({'tema':nome})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RUNNABLES**\n",
    "\n",
    "S√£o componentes especiais do langchain. Eles s√£o modulares, facilitam a execu√ß√£o de tarefas, reutiliza√ß√£o do c√≥digo e composi√ß√£o/encademaneto de fluxos de ia.\n",
    "\n",
    "Vc pode criar fun√ß√µes de python com runnables e a partir desse momento as fun√ß√µes adquirem os m√©todos dos runnables como invoke, stream, batch...\n",
    "\n",
    "Vamos dar um exemplo usando a o RunnalbleLambda:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnableParallel\n",
    "\n",
    "# Criamos um Runnable que transforma um texto para min√∫sculas\n",
    "minusculo = RunnableLambda(lambda x: x.lower())\n",
    "\n",
    "# Testando a transforma√ß√£o\n",
    "minusculo.invoke('FALA COMIGO!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veja q o runnable lambda tbm aceita fun√ß√£o nomeada\n",
    "\n",
    "def converte_minusculo(palavra)-> str:\n",
    "    return palavra.lower()\n",
    "\n",
    "# Criamos um Runnable que transforma um texto para min√∫sculas\n",
    "minusculo = RunnableLambda(converte_minusculo)\n",
    "\n",
    "# Testando a transforma√ß√£o\n",
    "minusculo.invoke('FALA COMIGO!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos para outro runnable um pouco mais elaborado, o RunnableParallel.\n",
    "\n",
    "Ele pode rodar chains em paralelo de forma ass√≠ncrona.\n",
    "\n",
    "Podemos fazer um encadeamento para que o modelo gere alguns outputs iniciais e um modelo final consuma esse output, facilitando o fluxo.\n",
    "\n",
    "Vamos ver isso em c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos criar duas chains\n",
    "# e depois vamos rod√°-las em paralelo\n",
    "\n",
    "prompt_nome_produto = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Vc vai desenvolver um nome para o seguinte tipo de produto: {produto}.\n",
    "Deve ter um apelo de marketing interessante!\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain_produto = prompt_nome_produto | chat\n",
    "\n",
    "\n",
    "prompt_cliente = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Vc √© um publicit√°rio e vai me dizer qual √© o perfil do p√∫blico em potencial\n",
    "desse produto: {produto}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain_cliente = prompt_cliente | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um runnable das chains\n",
    "# o output √© um dicion√°rio e cada chave\n",
    "# traz o retorno de sua respectiva chain\n",
    "paralelo = RunnableParallel(nome_produto=chain_produto, publico=chain_cliente)\n",
    "\n",
    "paralelo.invoke('cacha√ßa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esse prompt vai receber o output das chains anteriores, executadas em paralelo\n",
    "\n",
    "prompt_peca = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Vc √© um publicit√°rio rebuscado e muito conceituado.\n",
    "\n",
    "Vc vai desenvolver uma pe√ßa publicit√°ria, um chamado para \n",
    "o p√∫blico consumir o produto. Vou te falar o nome da empresa e \n",
    "a descri√ß√£o do p√∫blico alvo. Segue:\n",
    "\n",
    "nome da empresa: {nome_produto}\n",
    "publico alvo: {publico}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui n√≥s juntamos tudo atrav√©s dos pipes\n",
    "# o paralelo fornece os argumentos para o prompt_peca\n",
    "# esse prompt final √© fornecido para um modelo\n",
    "# e depois o output parser traz formata esse retorno para exibi√ß√£o\n",
    "chain_final = paralelo | prompt_peca | chat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui chamamos o metodo invoke para ver o resultado\n",
    "chain_final.invoke({'produto':'reports e analise de dados para o setor de RH'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CADEIAS DE ROTEAMENTO**\n",
    "\n",
    "O roteamento permite direcionar a demanda do usu√°rio (pergunta ou algo tipo) para a chain especializada em responder aquela pergunta. \n",
    "\n",
    "Por exemplo, podemos ter um aplicativo que receba perguntas de alunos do ensino fundamental.\n",
    "\n",
    "Com o roteador, ele consegue categorizar essa pergunta e direcionar para a chain correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando o modelo\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "# criando os prompts especializados para cada mat√©ria\n",
    "fis_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Vc √© um ex√≠mio professor de f√≠sica.\n",
    "Vc √© √≥timo em responder perguntas sobre f√≠sica de forma concisa e f√°cil de entender.\\\n",
    "Quando n√£o sabe a resposta para uma perginta, vc admite que n√£o sabe.\n",
    "                                                \n",
    "Aqui est√° a pergunta: {input}\"\"\")\n",
    "\n",
    "chain_fisica = fis_template | model\n",
    "\n",
    "mat_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Vc √© um ex√≠mio professor de matem√°tica, a verdadeira reencarna√ß√£o de Pit√°goras.\\\n",
    "Vc √© √≥timo em responder perguntas sobre matem√°tica de forma concisa e f√°cil de entender.\\\n",
    "Quando n√£o sabe a resposta para uma perginta, vc admite que n√£o sabe.\n",
    "                                                \n",
    "Aqui est√° a pergunta: {input}\"\"\")\n",
    "\n",
    "chain_mat = mat_template | model\n",
    "\n",
    "hist_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Vc √© um ex√≠mio professor de hist√≥ria, a verdadeira reencarna√ß√£o de Eric Hobsbawm.\\\n",
    "Vc √© √≥timo em responder perguntas sobre hist√≥ria de forma concisa e f√°cil de entender.\\\n",
    "Quando n√£o sabe a resposta para uma perginta, vc admite que n√£o sabe.\n",
    "                                                \n",
    "Aqui est√° a pergunta: {input}\"\"\")\n",
    "\n",
    "chain_hist = hist_template | model\n",
    "\n",
    "# esse aqui √© um prompt gen√©rico de escape\n",
    "# caso a pergunta do aluno nao se encaixe em nenhuma alternativa\n",
    "prompt_generico = ChatPromptTemplate.from_template('''{pergunta}''')\n",
    "chain_generica = prompt_generico | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos criar um \"parseador\" para que ele retorne a resposta exatamente da forma que precisamos. Para isso, vamos utilizar o pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos criar um Categorizador, que ser√° um objeto Pydantic\n",
    "\n",
    "class Categorizador(BaseModel):\n",
    "    \"Categoriza as perguntas dos alunos\"\n",
    "    area_conhecimento: str = Field(description=\"A √°rea de conhecimento \\\n",
    "                                   da pergunta feita pelo aluno. Deve ser:\\\n",
    "                                   'f√≠sica', 'matem√°tica' ou 'hist√≥ria'.\\\n",
    "                                   Caso n√£o encontre nenhuma delas, retorne 'outra'.\")\n",
    "    \n",
    "\n",
    "# vamos adicionar este prompt √† chain\n",
    "# n√£o √© obrigat√≥rio, mas pode ajudar \n",
    "# a retornar melhores respostas do modelo\n",
    "    \n",
    "prompt_categoria = ChatPromptTemplate.from_template(\"Voc√™ deve categorizar\\\n",
    "                                                    a seguinte pergunta: {pergunta}.\\\n",
    "                                                    Vc deve retornar sempre nesse padr√£o,\\\n",
    "                                                    com letras em min√∫culas.\")\n",
    "    \n",
    "# criando a chain\n",
    "model_estruturado = prompt_categoria | model.with_structured_output(Categorizador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testando\n",
    "model_estruturado.invoke({'pergunta':\"quando foi o periodo da ditadura no brasil?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRIANDO ESTRUTURA DE ROTEAMENTO\n",
    "\n",
    "O roteador ser√° uma fun√ß√£o, q vai pegar o outuput do parseador q criamos acima, e chamar e atribuir para a chain necess√°ria.\n",
    "\n",
    "Antes, vamos criar um runnable para agilizar a intera√ß√£o com esta fun√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui s√≥ uma demontra√ß√£o do q esse runnable faz\n",
    "# veja que ele recebe a pergunta e a resposta e \n",
    "# passa para frente, em forma de dicion√°rio\n",
    "chain_runnable = RunnablePassthrough().assign(categoria=model_estruturado)\n",
    "chain_runnable.invoke({'pergunta':'quem foi Ruy Barbosa?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos agor definir a fun√ß√£o roteadora\n",
    "# ser√° uma condicional simples\n",
    "def route(input):\n",
    "    if input['categoria'] == 'matem√°tica':\n",
    "        return chain_mat\n",
    "    if input['categoria'] == 'f√≠sica':\n",
    "        return chain_fisica\n",
    "    if input['categoria'] == 'hist√≥ria':\n",
    "        return chain_hist\n",
    "    \n",
    "    \n",
    "    return chain_generica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos fazer nossa chain com runnable e agora com o roteador\n",
    "chain_runnable = RunnablePassthrough().assign(categoria=model_estruturado) | route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veja que ele recebe a pergunta e responde de acordo\n",
    "# as chains especializadas poderiam ser com outras fun√ß√µes e mais complexas\n",
    "# o roteamento permite que a gente fa√ßa essa distribui√ß√£o de forma mais eficiente\n",
    "chain_runnable.invoke({'pergunta':'quem foi Ruy Barbosa?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEM√ìRIA\n",
    "\n",
    "Sem mem√≥ria, nosso modelo n√£o tem nenhum contexto da conversa. Toda mensagem ser√° uma nova mensagem sem hist√≥rico algum.\n",
    "\n",
    "Aqui vamos aprender a adicionar mem√≥ria ao nosso bot usando langchain.\n",
    "\n",
    "Vamos usar a classe InMemoryChatMessageHistory. Ela guarda uma lista de mensagens (self.messages), mas s√≥ em mem√≥ria RAM ‚Äî ou seja, n√£o persiste no disco ou banco de dados. Quando o processo termina (por exemplo, se voc√™ fechar o servidor ou recarregar o notebook), tudo √© perdido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando a memoria\n",
    "memory = InMemoryChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos usar alguns m√©todos dessa classe\n",
    "# para adicionar mensagens ao hist√≥rico\n",
    "memory.add_user_message('ol√°, meu nome √© Kaio.')\n",
    "memory.add_ai_message('ol√°, guardei seu nome.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e aqui podemos visualizar esse hist√≥rico\n",
    "memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui vamos construir uma chain pronta para receber a memoria\n",
    "# para esse fim, √© necess√°rio criar esse placeholder conforme abaixo\n",
    "# e atribuir o nome 'history'\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'voc√™ √© um tutor de programa√ß√£o q se chama Lovelace.\\\n",
    "     Responda √†s perguntas de forma didatica.'),\n",
    "     ('placeholder', '{history}'),\n",
    "     ('human', '{pergunta}')\n",
    "])\n",
    "\n",
    "# criando a chain\n",
    "chain = prompt | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar a mem√≥ria para adicionar ao modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um dicion√°rio onde ficar√£o as conversas por usu√°rio\n",
    "store = {}\n",
    "\n",
    "# e aqui criamos uma fun√ß√£o para adicionar \n",
    "# nova se√ß√£o de mem√≥ria ao dicion√°rio\n",
    "# caso j√° exista, ele busca e retorna essa mem√≥ria \n",
    "# cada usu√°rio (ou sess√£o) tem seu pr√≥≈ïio hist√≥rico\n",
    "def get_session_by_id(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos adicionar a memoria criada anteriormente √† nossa chain, que tbm j√° hav√≠amos criado. Para isso, utilizaremos um runnable espec√≠fico para este fim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora usando o runnable para criar a chain final\n",
    "# com memoria\n",
    "\n",
    "chain_com_memoria = RunnableWithMessageHistory(\n",
    "    chain, # fornecendo a chain com o prompt e modelo\n",
    "    get_session_by_id, # fornecendo a fun√ß√£o q gerencia memoria\n",
    "    input_messages_key='pergunta', # fornecendo a variavel de entrada do usu√°rio\n",
    "    history_messages_key='history' # fornecendo a variavel dos historico de conversa\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos testar essa nova chain com memoria.\n",
    "\n",
    "Antes de testar a nova chain com mem√≥ria, precisamos criar um dicion√°rio chamado config, que ser√° respons√°vel por gerenciar diferentes usu√°rios atrav√©s de um identificador √∫nico (session_id). Esse session_id √© essencial para que a mem√≥ria funcione corretamente, permitindo que o hist√≥rico de conversas seja separado por usu√°rio. Abaixo, configuramos o session_id como \"user_a\" e chamamos a chain passando tanto a pergunta quanto a configura√ß√£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'configurable': {'session_id':'user_a'}}\n",
    "resposta = chain_com_memoria.invoke({'pergunta':'ol√° meu nome √© Z√©!'},\n",
    "                                    config=config)\n",
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos s√≥ validar se ele manteve a memoria e lembra o meu nome\n",
    "resposta = chain_com_memoria.invoke({'pergunta':'qual √© o meu nome?'},\n",
    "                                    config=config)\n",
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora olha q coisa interessante\n",
    "# veja que se mudar de usu√°rio no config\n",
    "# ele j√° muda de hist√≥rico\n",
    "# agora o modelo n√£o sabe mais o nome do usu√°rio pq esse de fato √© outro usuario\n",
    "config = {'configurable': {'session_id':'user_b'}}\n",
    "resposta = chain_com_memoria.invoke({'pergunta':'qual √© o meu nome?'},\n",
    "                                    config=config)\n",
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG: Conversando com os seus dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Lembrete: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**O que √© RAG?**\n",
    "- T√©cnica que combina **gera√ß√£o de texto (LLM)** com **recupera√ß√£o de informa√ß√µes externas**.\n",
    "- Permite que o modelo acesse dados atualizados e espec√≠ficos que n√£o est√£o em seu treinamento original.\n",
    "\n",
    "**Por que √© importante?**\n",
    "- LLMs sozinhos dependem apenas do conhecimento aprendido at√© a data de corte.\n",
    "- RAG permite incorporar informa√ß√µes externas (PDFs, documentos, banco de dados etc.).\n",
    "- Ideal para criar **aplica√ß√µes personalizadas**, como chatbots que respondem com base em documentos internos.\n",
    "\n",
    "**Etapas do RAG:**\n",
    "1. **üì• Carregamento de Documentos**: importar PDFs, CSVs, bancos de dados, etc.\n",
    "2. **‚úÇÔ∏è Divis√£o de Documentos**: separar em trechos menores mantendo o contexto.\n",
    "3. **üî¢ Embedding**: transformar os trechos em vetores num√©ricos.\n",
    "4. **üíæ Armazenamento em VectorStore**: salvar os vetores para busca eficiente.\n",
    "5. **üîç Recupera√ß√£o (Retrieval)**: buscar os vetores mais relevantes com base na pergunta do usu√°rio.\n",
    "6. **üß† Gera√ß√£o**: o modelo usa os trechos recuperados para gerar uma resposta precisa.\n",
    "\n",
    "**Desafios:**\n",
    "- Limite de tokens nos modelos (ex: 16k no GPT-3.5).\n",
    "- Selecionar apenas os trechos mais relevantes para enviar ao modelo.\n",
    "\n",
    "**Pontos-chave para lembrar:**\n",
    "- RAG = LLM + Dados externos ‚ûú respostas mais **precisas e atualizadas**.\n",
    "- Aplic√°vel em diversos contextos pr√°ticos (ex: atendimento ao cliente, busca em base interna).\n",
    "- A qualidade da resposta depende diretamente dos dados que voc√™ fornece ao modelo.\n",
    "\n",
    "> üõ†Ô∏è Aplicar RAG pode transformar suas aplica√ß√µes com IA, tornando-as mais √∫teis, contextuais e confi√°veis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loaders ‚Äì Carregando dados com Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carregando PDFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a classe necess√°ria para manipular pdf\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui √© o caminho onde ficar√° o arquivo\n",
    "caminho = \"arquivos_teste/contrato_aluguel_jose_nilton.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fornecendo o pdf no loader\n",
    "loader = PyPDFLoader(caminho)\n",
    "# agora estamos carregando o pdf\n",
    "documentos = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aqui a gente v√™ a quantidade de p√°ginas do documento\n",
    "len(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONTRATO DE LOCA√á√ÉO RESIDENCIAL\\nO LOCADOR e o LOCAT√ÅRIO, qualificados abaixo (em conjunto denominados ‚ÄúPartes‚Äù,\\ne, isoladamente, ‚ÄúParte‚Äù), celebram este ‚Äú Instrumento Particular de Contrato de\\nLoca√ß√£o‚Äù (‚ÄúContrato‚Äù), que ser√° regido pelo disposto nas Leis federais n¬∫s \\xa08.245/1991\\n(‚ÄúLei do Inquilinato‚Äù) e 10.406/2002 (‚ÄúC√≥digo Civil‚Äù), e se comprometer a cumprir:\\nCL√ÅUSULA PRIMEIRA: DA QUALIFICA√á√ÉO DAS PARTES\\nLOCADOR: KAIO OLIVEIRA PEIXOTO, BRASILEIRO, SOLTEIRO, portador da c√©dula de\\nidentidade R.G. n¬∫ 315622456 e CPF n¬∫ 021.667.755-60, residente e domiciliado na\\nRua Jo√£o Pessoa, n¬∫ 401, CEP 45745-000, bairro Centro, Ibicara√≠-BAHIA;\\nLOCAT√ÅRIO:  JOSE  NILTON  PEREIRA  SILVA ,  BRASILEIRO,  SOLTEIRO,  COMERCIANTE,\\nportador da c√©dula de identidade R.G. n¬∫ 15403176-33 e CPF n¬∫ 054.568.205-36,\\nresidente e domiciliado na Rua Paragua√ßu, n¬∫ 272, CEP 45745-000, bairro Centro,\\nIbicara√≠-BAHIA;\\nCL√ÅUSULA SEGUNDA: OBJETO\\nPor meio deste Contrato, o LOCADOR entrega ao LOCAT√ÅRIO a posse e uso do im√≥vel\\nsituado  na  Rua  Paragua√ßu,  n¬∫ 269, CEP  45745-000,  bairro  Centro, Ibicara√≠-BAHIA ,\\nfazendo jus, em contrapartida, ao pagamento pelo LOCAT√ÅRIO dos valores de aluguel\\ne encargos. A casa √© de dom√≠nio, propriedade e posse indireta do LOCADOR.\\nCL√ÅUSULA TERCEIRA: PRAZO DE LOCA√á√ÉO\\nEste  Contrato  vigorar√°  pelo  prazo  disposto  de  06  (seis)  meses,  tendo  in√≠cio  em\\n01/08/2023 e t√©rmino previsto para o dia  01/02/2024, podendo ser renovado por\\ninteresse das partes. Ap√≥s finalizado o contrato, dever√° o LOCAT√ÅRIO restituir o Im√≥vel\\nao LOCADOR, no estado em que recebeu, salvo as deteriora√ß√µes naturais ao seu uso\\nregular.\\nCL√ÅUSULA QUARTA: ALUGUEL E SEU REAJUSTE\\nPagar√° o LOCAT√ÅRIO ao LOCADOR o valor mensal de R$ 350,00, a t√≠tulo de aluguel. Os\\nalugu√©is ser√£o pagos todo dia 05 de cada m√™s, efetivando-se por dep√≥sito banc√°rio no\\nBanco do Brasil, Ag√™ncia 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\\nPeixoto.\\nO aluguel pactuado acima sofrer√° reajustes anuais com base na varia√ß√£o do √çndice\\nGeral de Pre√ßos divulgado pela Funda√ß√£o Get√∫lio Vargas (IGP-FGV) ou outro √≠ndice que\\nporventura venha a substitu√≠-lo. \\xa0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aqui por exemplo estamos vendo o conte√∫do da primeira p√°gina\n",
    "documentos[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e aqui a gente pode ver alguns metadados\n",
    "documentos[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo perguntas para o arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a chain necess√°ria para interagir com o pdf\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# ao criar a chain, se colocarmos o verbose como True\n",
    "# ele vai mostrar como trabalha debaixo dos panos\n",
    "chain = load_qa_chain(llm=chat, chain_type=\"stuff\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contrato de loca√ß√£o residencial entre Kaio e Jose.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pergunta = \"fale de forma bem resumida em no m√°ximo \\\n",
    "    10 palavras do q se trata esse documento?\"\n",
    "\n",
    "# agora vamos rodar a chain, veja q ela precisa de 2 inputs\n",
    "# primeiro passamos o documento que queremos analisar\n",
    "# e depois a pergunta\n",
    "resposta = chain.invoke({'input_documents':documentos, 'question':pergunta})\n",
    "\n",
    "resposta['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carregando CSVs**\n",
    "\n",
    "Veja que o processo √© bem parecido com o carregamento de PDFs.\n",
    "\n",
    "A maior mudan√ßa √© no nome do loader mesmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui √© o caminho onde ficar√° o arquivo\n",
    "caminho_csv = \"arquivos_teste/Top 1000 IMDB movies.csv\"\n",
    "loader = CSVLoader(caminho_csv)\n",
    "documento_csv = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cada 'p√°gina' do documento seria uma linha do arquivo csv\n",
    "len(documento_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': 0\\nMovie Name: The Shawshank Redemption\\nYear of Release: (1994)\\nWatch Time: 142 min\\nMovie Rating: 9.3\\nMeatscore of movie: 81\\nVotes: 34,709\\nGross: $28.34M\\nDescription: Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# essa √© a primeira linha\n",
    "documento_csv[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veja tbm q inclusive a cria√ß√£o da chain √© igual √† da leitura de pdf\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chain_csv = load_qa_chain(llm=chat, chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Desculpe, n√£o consigo responder a essa pergunta com base nas informa√ß√µes fornecidas.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a √∫nica diferen√ßa para o pdf √© que forneceremos\n",
    "# os documentos csv como input\n",
    "pergunta = \"dessa lista a√≠, cite o filme pior rankeado e qual o nome dele\"\n",
    "\n",
    "resposta = chain_csv.invoke({'input_documents':documento_csv[:50], 'question':pergunta})\n",
    "\n",
    "resposta['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUTUBE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para \"assistir\" v√≠deos do youtube deve-se fazer o processo abaixo.\n",
    "\n",
    "Ele vai acessar o link e transcrever o audio atrav√©s da api da OpenAI, com o m√©todo whisper.\n",
    "\n",
    "Seria necess√°ria a instala√ß√£o de algumas bibliotecas ainda para completar essa solu√ß√£o, mas a base est√° a√≠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deprecated Feature: Support for Python version 3.8 has been deprecated. Please update to Python 3.9 or above\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Px8Iu2viG7I\n",
      "[youtube] Px8Iu2viG7I: Downloading webpage\n",
      "[youtube] Px8Iu2viG7I: Downloading ios player API JSON\n",
      "[youtube] Px8Iu2viG7I: Downloading mweb player API JSON\n",
      "[youtube] Px8Iu2viG7I: Downloading player 73381ccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/73381ccc/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] Px8Iu2viG7I: nsig extraction failed: Some formats may be missing\n",
      "         n = 1Rq63z-Csi5joMKSz ; player = https://www.youtube.com/s/player/73381ccc/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/73381ccc/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] Px8Iu2viG7I: nsig extraction failed: Some formats may be missing\n",
      "         n = Fy6og7FLVKv-yiIXg ; player = https://www.youtube.com/s/player/73381ccc/player_ias.vflset/en_US/base.js\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Px8Iu2viG7I: Downloading m3u8 information\n",
      "[info] Px8Iu2viG7I: Downloading 1 format(s): 140\n",
      "[download] arquivos_teste/AMIGO NOVO.m4a has already been downloaded\n",
      "[download] 100% of    3.44MiB\n",
      "[ExtractAudio] Not converting audio arquivos_teste/AMIGO NOVO.m4a; file is already in target format m4a\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "pydub package not found, please install it with `pip install pydub`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/langchain_community/document_loaders/parsers/audio.py:54\u001b[0m, in \u001b[0;36mOpenAIWhisperParser.lazy_parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpydub\u001b[39;00m \u001b[39mimport\u001b[39;00m AudioSegment\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydub'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m save_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39marquivos_teste\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m loader \u001b[39m=\u001b[39m GenericLoader(\n\u001b[1;32m      4\u001b[0m     YoutubeAudioLoader([url], save_dir),\n\u001b[1;32m      5\u001b[0m     OpenAIWhisperParser()\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m docs \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m~/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/langchain_core/document_loaders/base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m     28\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlazy_load())\n",
      "File \u001b[0;32m~/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/langchain_community/document_loaders/generic.py:116\u001b[0m, in \u001b[0;36mGenericLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load documents lazily. Use this when working at a large scale.\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mfor\u001b[39;00m blob \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblob_loader\u001b[39m.\u001b[39myield_blobs():  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblob_parser\u001b[39m.\u001b[39mlazy_parse(blob)\n",
      "File \u001b[0;32m~/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/langchain_community/document_loaders/parsers/audio.py:56\u001b[0m, in \u001b[0;36mOpenAIWhisperParser.lazy_parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpydub\u001b[39;00m \u001b[39mimport\u001b[39;00m AudioSegment\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpydub package not found, please install it with \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m`pip install pydub`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m     61\u001b[0m     \u001b[39m# api_key optional, defaults to `os.environ['OPENAI_API_KEY']`\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     client \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mOpenAI(api_key\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key, base_url\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url)\n",
      "\u001b[0;31mImportError\u001b[0m: pydub package not found, please install it with `pip install pydub`"
     ]
    }
   ],
   "source": [
    "url = 'https://www.youtube.com/watch?v=Px8Iu2viG7I'\n",
    "save_dir = 'arquivos_teste'\n",
    "loader = GenericLoader(\n",
    "    YoutubeAudioLoader([url], save_dir),\n",
    "    OpenAIWhisperParser()\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URL**\n",
    "\n",
    "Agora vamos ler p√°ginas de internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.web_base import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.uol.com.br/esporte/futebol/ultimas-noticias/2025/03/31/rb-bragantino-ceara-brasileirao-serie-a-2025.htm'\n",
    "\n",
    "loader = WebBaseLoader(url)\n",
    "documentos_url = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 s√≥ pq √© uma p√°gina s√≥ q eu passei\n",
    "len(documentos_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora veja novamente q o processo de interagir com \n",
    "# o arquivo √© igual √†s anteriores\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chain_url = load_qa_chain(llm=chat, chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RB Bragantino empata com Cear√° na estreia do Brasileir√£o S√©rie A de 2025. Pedro Raul e Marllon marcaram para o Cear√°, enquanto Laquintana e Eric Ramires fizeram os gols do Bragantino. Resultado de 2 a 2 deixou as equipes na sexta e s√©tima posi√ß√£o, respectivamente. Pr√≥ximos jogos no final de semana.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pergunta = \"fa√ßa um resumo dessa not√≠cia com at√© 50 palavras\"\n",
    "\n",
    "resposta = chain_url.invoke({'input_documents':documentos_url, 'question':pergunta})\n",
    "\n",
    "resposta['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitters ‚Äì Dividindo texto em trechos\n",
    "\n",
    "Agora q j√° sabemos carregar os documentos, precisamos saber como \"quebrar\" esse documento em peda√ßos menores para ser melhor consumido pelo modelo.\n",
    "\n",
    "Essa √© a segunda etapa do processo de RAG. As proximas etapas s√£o embedar, guardar numa vector store e finalmente fazer o retrieval.\n",
    "\n",
    "= = = = \n",
    "\n",
    "Aqui o objetivo √© dividir o documento em trechos menores para fornecer ao LLM apenas peda√ßos relevantes da informa√ß√£o e assim ele processar. Isso garante mais agilidade e precis√£o no resultado final.\n",
    "\n",
    "Mas como podemos manter a qualidade da informa√ß√£o se a dividimos em peda√ßos menores?\n",
    "\n",
    "Vejamos esta frase de exemplo: \"O novo carro da Fiat se chama Toro, tem 120 cavalos de pot√™ncia e o pre√ßo sugerido √© 135 mil reais.\"\n",
    "\n",
    "Digamos q o split dessa frase ficaria da seguinte forma:\n",
    "\n",
    "- \"O novo carro da Fiat se\"\n",
    "- \" chama Toro, tem 120 \"\n",
    "- \"cavalos de pot√™ncia e \"\n",
    "- \"o pre√ßo sugerido √© \"\n",
    "- \"135 mil reais.\"\n",
    "\n",
    "Aqui percebemos que os trechos separados individualmente n√£o possuem valor. Sendo assim, seria de pouco valor para uma LLM, pois foi mal dividido.\n",
    "\n",
    "Uma das t√©cnicas para resolver isso √© usar o parametro do overlapping. Esta t√©cnica faz com que cada chunk tenha um peda√ßo do chunk anterior e do pr√≥ximo. Dessa forma o LLM flui pela informa√ß√£o podendo fazer liga√ß√µes.\n",
    "\n",
    "Ficaria assim, por exemplo:\n",
    "\n",
    "- \"O novo carro da Fiat se\"\n",
    "- \"da Fiat se chama Toro, tem 120 \"\n",
    "- \"Toro, tem 120 cavalos de pot√™ncia e \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esse √© o texto q vamos splittar\n",
    "texto_grande = \"\"\"\n",
    "More and more organizations are turning to dashboards for monitoring performance and \\\n",
    "enabling data exploration. These user-friendly reporting tools offer a ton of \\\n",
    "advantages over older ways of doing things: they can dynamically update to display\\\n",
    "the latest information, link together multiple views of data, and often incorporate \\\n",
    "interactivity that lets users filter and zoom in on what they want to explore. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem formas diferentes de text splitter. Vamos explorar as mais utilizadas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CharacterTextSplitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o tamanho do chunk (fatias) que vc deseja dividir\n",
    "chunk_size = 50\n",
    "# o tamanho da sobreposi√ß√£o. Geralmente um tamanho de 10 a 20% do chunk j√° funciona\n",
    "chunk_overlap = 10\n",
    "\n",
    "# criando o objeto q ir√° fazer o split\n",
    "char_split = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator='' # o separator indica onde a divis√£o ser√° feita\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos dar um exemplo de split com o abeced√°rio\n",
    "# o abecedario est√° disponivel importando a lib string\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "# vamos criar nosso texto a ser dividido\n",
    "# pegamos o abecedario e o replicamos 5 vezes\n",
    "# veja q o tamanho do nosso texto ficou em 134 caracteres\n",
    "texto = '-'.join(f'{string.ascii_lowercase}' for _ in range(5))\n",
    "print(texto)\n",
    "print(len(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvw',\n",
       " 'nopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghi',\n",
       " '-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuv',\n",
       " 'mnopqrstuvwxyz']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora fazendo o split do texto criado acima\n",
    "# veja q o tamanho ficou em 4, sendo o chunksize de 50 \n",
    "# e um total de 134 caracteres\n",
    "splits = char_split.split_text(texto)\n",
    "print(len(splits))\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Veja acima como funciona o overlap. O segundo elemento da lista inicia com os 10 √∫ltimos caracteres da primeira lista, e assim sucessivamente. Ou seja, existe uma sobreposi√ß√£o entre o fim de um elemento da lista e inicio do pr√≥ximo. A quantidade de caracteres que faz essa sobreposi√ß√£o √© dada no chunk_overlap. E, claro, isso s√≥ n√£o ocorre com o primeiro elemento, j√° q ele n√£o tem antecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RecursiveCharacterTextSplitter**\n",
    "\n",
    "Esse √© o mais utilizado, ele permite o uso de v√°rios separadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50\n",
    "chunk_overlap = 10\n",
    "\n",
    "# criando o objeto q ir√° fazer o split\n",
    "char_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos aproveitar o mesmo exemplo usado anteriormente\n",
    "texto = '-'.join(f'{string.ascii_lowercase}' for _ in range(5))\n",
    "len(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvw',\n",
       " 'nopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghi',\n",
       " '-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuv',\n",
       " 'mnopqrstuvwxyz']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_split.split_text(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora utilizando o argumento separators\n",
    "\n",
    "Com este argumento o splitter cria uma ordem de prioridade para quebrar os chunks. No exemplo abaixo, primeiro ele quebra por ponto, depois por espa√ßo vazio e depois por qualquer lugar do texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o objeto q ir√° fazer o split\n",
    "char_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=['.', ' ', '']\n",
    ")\n",
    "\n",
    "# geralmente apenas uns poucos marcadores de split\n",
    "# s√£o necess√°rios. S√£o eles: ['\\n\\n', '\\n', ' ', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver um exemplo com um texto grande, com mais caracteres do que o abeced√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['More and more organizations are turning to',\n",
       " 'to dashboards for monitoring performance and',\n",
       " 'and enabling data exploration',\n",
       " '. These user-friendly reporting tools offer a ton',\n",
       " 'a ton of advantages over older ways of doing',\n",
       " 'of doing things: they can dynamically update to',\n",
       " 'update to displaythe latest information, link',\n",
       " 'link together multiple views of data, and often',\n",
       " 'and often incorporate interactivity that lets',\n",
       " 'that lets users filter and zoom in on what they',\n",
       " 'what they want to explore',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_split.split_text(texto_grande)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TokenTextSplitter**\n",
    "\n",
    "Permite o fatiamento em tokens em vez de quantidade de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50\n",
    "chunk_overlap = 10\n",
    "\n",
    "token_split = TokenTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijkl',\n",
       " 'uvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_split.split_text(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Agora veja como fica a divis√£o por token. O abeced√°rio agora foi dividido em apenas duas partes. Isso porque aquela primeira parte toda j√° √© equivalente a 50 tokens. Essa forma de split facilita nosso gerenciamento das APIs da OpenAI e outras, que contabilizam o fluxo de dados em tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MarkdownHeaderTextSplitter**\n",
    "\n",
    "Podemos fazer o split de textos em Markdown. Existem v√°rios marcadores nesse tipo de texto que podemos indicar para o split. Dessa forma fica mais f√°cil identificarmos quando trata-se de um texto principal (#), subtitulo (##), e etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"# Exemplo de titulo em markdown\n",
    "## Aqui √© um subtitulo\n",
    "texto q fica dentro desse subtitulo\n",
    "### Aqui outra hierarquia de header\n",
    "Mais texto livre q fica dentro desta hierarquia\n",
    "**e vamos de mais exemplos**\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca necess√°ria\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro criamos uma lista de tuplas\n",
    "# onde ter√£o os marcadores que far√£o o split\n",
    "\n",
    "header_to_split_on = [\n",
    "    ('#', 'Header 1'),\n",
    "    ('##', 'Header 2'),\n",
    "    ('###', 'Header 3'),\n",
    "]\n",
    "\n",
    "# criando o splitter\n",
    "md_split = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=header_to_split_on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo o split\n",
    "splits = md_split.split_text(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='texto q fica dentro desse subtitulo', metadata={'Header 1': 'Exemplo de titulo em markdown', 'Header 2': 'Aqui √© um subtitulo'}),\n",
       " Document(page_content='Mais texto livre q fica dentro desta hierarquia\\n**e vamos de mais exemplos**', metadata={'Header 1': 'Exemplo de titulo em markdown', 'Header 2': 'Aqui √© um subtitulo', 'Header 3': 'Aqui outra hierarquia de header'})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Veja como fica o output. Ele traz as hierarquias de t√≠tulo\\subtitulo em metadata e traz seu conte√∫do na vari√°vel page_content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split de documentos**\n",
    "\n",
    "Vamos fazer o split de um documento, em vez de textos simples como fizemos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca q carregar√° e nos ajudar√° a trabalhar com doc\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# importando um splitter recursivo\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50\n",
    "chunk_overlap = 10\n",
    "\n",
    "# criando o objeto q ir√° fazer o split\n",
    "# at√© aqui √© identico ao q j√° fizemos anteriormente\n",
    "char_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho para o arquivo\n",
    "caminho = 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carregando o arquivo\n",
    "loader = PyPDFLoader(caminho)\n",
    "docs = loader.load()\n",
    "\n",
    "# veja que o tamanho √© apenas 5, q √© a quantidade de p√°ginas\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora vejamos depois de fazer o split\n",
    "# fique atento que aqui o m√©todo √© diferente, sendo split_documents\n",
    "# em vez de split_text\n",
    "doc_split = char_split.split_documents(docs)\n",
    "len(doc_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings - Transformando texto em vetores\n",
    "\n",
    "Embeddings criam uma representa√ß√£o vetorial de um peda√ßo de texto. Ele transforma letras em n√∫meros. Isso √© √∫til, pois √© assim que o modelo vai \"pensar\" de forma mais eficiente. Dessa forma ele pode buscar proximidades semanticas, etc. Os c√°lculos vetoriais √© quem d√£o a \"consciencia\" e \"discernimento\" para o modelo.\n",
    "\n",
    "O Langchain fornece uma classe de embedding que interage com os modelos de embedding fornecidos no mercado, ou seja, existem ebeddings da OpenAI, HuggingFace, Claude etc. O Langchain fornece uma interface padr√£o para todas elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o modelo default para embedding √© esse q est√° no argumento da chamada\n",
    "# existem outros. Visite o site da OpenAI para detalhes\n",
    "# vamos instanciar esse embedding da OpenAI\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding de Documentos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos criar nosso modelo \n",
    "# veja que ele possui uma lista, onde ficar√£o os documentos\n",
    "# inserimos 3 documentos, q no momento s√£o apenas frases\n",
    "embeddings = embedding_model.embed_documents(\n",
    "    [\n",
    "        'Eu amo frutas!',\n",
    "        'Adoro comer ma√ß√£ e morando no caf√© da manh√£',\n",
    "        'Eu n√£o gostei dessa piada.'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temos 3 documentos\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.006634879172388902,\n",
       " -0.01640579910410335,\n",
       " 0.008534951452491854,\n",
       " -0.018041460513549958,\n",
       " 0.00036875386927730914,\n",
       " 0.012144472901045993,\n",
       " 0.004282848602894743,\n",
       " -0.01683623592497978,\n",
       " 0.02096843238562586,\n",
       " -0.018189037894204337]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos como ficou vetorizada a primeira frase\n",
    "# aqui vemos os 10 primeiros vetores\n",
    "embeddings[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536 0.22313854927024132 -0.6505255940753462\n",
      "1536 0.24039689057922564 -0.6608969665847052\n",
      "1536 0.22472701344464535 -0.6345005455745475\n"
     ]
    }
   ],
   "source": [
    "# vamos fazer um for e analisar cada vetor\n",
    "# primeiro vemos q o tamanho dos vetores √© o mesmo (1536)\n",
    "# isso acontece por causa do modelo escolhido da OpenAI, na hora de instanciar\n",
    "# depois pegamos o valor m√°ximo e o m√≠nimo de cada lista de vetores\n",
    "for emb in embeddings:\n",
    "    print(len(emb), max(emb), min(emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos fazer uma multiplica√ß√£o vetorial utilizando o numpy\n",
    "# para identificarmos as semelhan√ßas semanticas entre os vetores\n",
    "# √© atrav√©s desses c√°lculo que o modelo 'pensa' e sabe das semelhan√ßas\n",
    "# entre as frases. Quanto mais pr√≥ximo de 1, mais semelhantes s√£o as frases\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8841020582306888"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiplicando a primeira e a segunda frase\n",
    "# veja o n√∫mero que obtemos\n",
    "# quanto mais alto significa que as frases tem sem√¢nticas pr√≥ximas\n",
    "np.dot(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7906537384786307"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora veja a compara√ß√£o da frase 1 com a frase 3, o n√∫mero √© menor\n",
    "np.dot(embeddings[0], embeddings[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7688946865216448"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mesma coisa entre as frases 2 e 3\n",
    "np.dot(embeddings[1], embeddings[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 | 0.884 | 0.791 | \n",
      "0.884 | 1.0 | 0.769 | \n",
      "0.791 | 0.769 | 1.0 | \n"
     ]
    }
   ],
   "source": [
    "# vamos ver o conjunto todo, multiplicando todos vetores entre eles\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(len(embeddings)):\n",
    "        print(round(np.dot(embeddings[i], embeddings[j]), 3), end=' | ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding Query**\n",
    "\n",
    "Precisamos tbm fazer o embedding da pergunta do usu√°rio ao modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.011823595855325887,\n",
       " -0.0015175153225819675,\n",
       " 0.003353206044200978,\n",
       " -0.004639096331514772,\n",
       " 0.003132296805952957,\n",
       " 0.004395107058349887,\n",
       " -0.00871108243536731,\n",
       " -0.0341848975114945,\n",
       " -0.017171579505757185,\n",
       " 0.0036796243461127056]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora vamos utilizar o m√©todo emb_query() do modelo instanciado\n",
    "pergunta = \"quais os alimentos mais saud√°veis\"\n",
    "emb_query = embedding_model.embed_query(pergunta)\n",
    "emb_query[:10] # vendo os 10 primeiros vetores dessa pergunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125198875109063"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora comparando com as frases criadas antes\n",
    "# veja como a pergunta √© pr√≥xima das frase 1 por exemplo\n",
    "np.dot(emb_query, embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7236170898533685"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e veja com ela √© mais distante da frase 3\n",
    "np.dot(emb_query, embeddings[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding com Huggingface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaio-peixoto/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/kaio-peixoto/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = 'all-MiniLM-L6-v2'\n",
    "emb_model = HuggingFaceBgeEmbeddings(model_name=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = emb_model.embed_documents(\n",
    "    [\n",
    "    'Eu gosto de manter uma alimenta√ß√£o saud√°vel.',\n",
    "    'Adoro comer ma√ß√£ e morango no caf√© da manh√£.',\n",
    "    'A crise financeira de 2008 abalou o mundo.']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 | 0.432 | 0.419 | \n",
      "0.432 | 1.0 | 0.313 | \n",
      "0.419 | 0.313 | 1.0 | \n"
     ]
    }
   ],
   "source": [
    "# vamos ver o conjunto todo, multiplicando todos vetores entre eles\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(len(embeddings)):\n",
    "        print(round(np.dot(embeddings[i], embeddings[j]), 3), end=' | ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorStores\n",
    "\n",
    "Uma VectorStore faz o armazenamento de vetores e realiza a busca desses vetores. Depois de fazermos o embedding, como explicado na aula anterior, temos que guard√°-los num lugar otimizado para trabalharmos com os vetores.\n",
    "\n",
    "Temos v√°rias solu√ß√µes de VectorStores e aqui vamos estudar duas das mais utilizadas: Chroma e FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chroma VectorStore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos fazer o processo desde o in√≠cio\n",
    "# ou seja, vamos carregar os documentos, splittar, fazer o embedding\n",
    "# e guardar na VectorStore para retrieval\n",
    "\n",
    "# importando as libs para carregar os docs\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando os documentos\n",
    "caminho = 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'\n",
    "loader = PyPDFLoader(caminho)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos para o text splitting\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=30,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = recur_split.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a VectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diretorio onde a base de dados ser√° salva\n",
    "diretorio = \"files/chroma_vectorstore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a vector store\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=diretorio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "# aqui a gente v√™ quandos documentos existem dentro da vector store\n",
    "print(vector_store._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui agora vamos carregar uma vector store j√° existente\n",
    "# veja q n√£o utilizamos o m√©todo .from_documents()\n",
    "# utilizamos somente a classe Chroma\n",
    "# fornecemos apenas o modelo de embedding e o diret√≥rio\n",
    "# onde a vector store est√° localizada\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=diretorio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval**\n",
    "\n",
    "Fazendo perguntas para os documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aqui vamos ver como o vector store procura pela similidaridade\n",
    "# vamos fazer uma pergunta e ver quais trechos do documento ele retorna\n",
    "pergunta = 'qual √© o valor do aluguel?'\n",
    "\n",
    "# o argumento k √© a quantidade de documentos q ele vai pesquisar\n",
    "docs = vector_store.similarity_search(pergunta, k=5)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagar√° o LOCAT√ÅRIO ao LOCADOR o valor mensal de R$ 350,00, a t√≠tulo de aluguel. Os\n",
      "alugu√©is ser√£o pagos todo dia 05 de cada m√™s, efetivando-se por dep√≥sito banc√°rio no\n",
      "Banco do Brasil, Ag√™ncia 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'page': 0, 'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'} \n",
      "\n",
      "Pagar√° o LOCAT√ÅRIO ao LOCADOR o valor mensal de R$ 350,00, a t√≠tulo de aluguel. Os\n",
      "alugu√©is ser√£o pagos todo dia 05 de cada m√™s, efetivando-se por dep√≥sito banc√°rio no\n",
      "Banco do Brasil, Ag√™ncia 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'page': 0, 'source': 'contrato_aluguel_jose_nilton.pdf'} \n",
      "\n",
      "Pagar√° o LOCAT√ÅRIO ao LOCADOR o valor mensal de R$ 350,00, a t√≠tulo de aluguel. Os\n",
      "alugu√©is ser√£o pagos todo dia 05 de cada m√™s, efetivando-se por dep√≥sito banc√°rio no\n",
      "Banco do Brasil, Ag√™ncia 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'page': 0, 'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'} \n",
      "\n",
      "Pagar√° o LOCAT√ÅRIO ao LOCADOR o valor mensal de R$ 350,00, a t√≠tulo de aluguel. Os\n",
      "alugu√©is ser√£o pagos todo dia 05 de cada m√™s, efetivando-se por dep√≥sito banc√°rio no\n",
      "Banco do Brasil, Ag√™ncia 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'page': 0, 'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'} \n",
      "\n",
      "Pagar√° o LOCAT√ÅRIO ao LOCADOR o valor mensal de R$ 350,00, a t√≠tulo de aluguel. Os\n",
      "alugu√©is ser√£o pagos todo dia 05 de cada m√™s, efetivando-se por dep√≥sito banc√°rio no\n",
      "Banco do Brasil, Ag√™ncia 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'page': 0, 'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# e agora veja o conte√∫do de cada documento\n",
    "# perguntamos sobre valor do aluguel\n",
    "# e ele buscou os trechos q est√£o mais relacionados √† pergunta\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=====\", doc.metadata, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FAISS VectorStore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando os documentos\n",
    "caminho = 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'\n",
    "loader = PyPDFLoader(caminho)\n",
    "docs = loader.load()\n",
    "\n",
    "# agora vamos para o text splitting\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=30,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = recur_split.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um modelo de embedding\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a vector store com Faiss\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "# agora vamos criar a vectorstore com FAISS\n",
    "# √© bem parecido com o Chroma, mas sem o diretorio\n",
    "# FAISS n√£o usa 'persist_directory' porque n√£o tem persist√™ncia nativa \n",
    "vector_store_faiss = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora fazendo o retrieval com essa nova base FAISS\n",
    "# aqui vamos ver como o vector store procura pela similidaridade\n",
    "# vamos fazer uma pergunta e ver quais trechos do documento ele retorna\n",
    "pergunta = 'qual √© o valor do aluguel?'\n",
    "\n",
    "# o argumento k √© a quantidade de documentos q ele vai pesquisar\n",
    "docs = vector_store_faiss.similarity_search(pergunta, k=5)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagar√° o LOCAT√ÅRIO ao LOCADOR o valor mensal de R$ 350,00, a t√≠tulo de aluguel. Os\n",
      "alugu√©is ser√£o pagos todo dia 05 de cada m√™s, efetivando-se por dep√≥sito banc√°rio no\n",
      "Banco do Brasil, Ag√™ncia 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 0} \n",
      "\n",
      "O  atraso  no  pagamento  do  aluguel  e/ou  dos  encargos  da  loca√ß√£o  implicar√°  a\n",
      "incid√™ncia de multa de 2,00% (dois por cento) sobre o valor total de d√©bito e juros de\n",
      "1,00% (um por cento) ao m√™s.\n",
      "CL√ÅUSULA S√âTIMA: USO E DESTINA√á√ÉO DA LOCA√á√ÉO\n",
      "===== {'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 1} \n",
      "\n",
      "Peixoto.\n",
      "O aluguel pactuado acima sofrer√° reajustes anuais com base na varia√ß√£o do √çndice\n",
      "Geral de Pre√ßos divulgado pela Funda√ß√£o Get√∫lio Vargas (IGP-FGV) ou outro √≠ndice que\n",
      "porventura venha a substitu√≠-lo.\n",
      "===== {'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 0} \n",
      "\n",
      "(tr√™s)  vezes  o  valor  do  aluguel  vigente  √†  √©poca  da  infra√ß√£o,  sem  preju√≠zo  de\n",
      "indeniza√ß√£o suplementar.\n",
      "CL√ÅUSULA NONA: ENTREGA E DEVOLU√á√ÉO DO IM√ìVEL\n",
      "O LOCADOR entrega o im√≥vel ao LOCAT√ÅRIO contendo as caracter√≠sticas relacionadas\n",
      "===== {'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 2} \n",
      "\n",
      "fazendo jus, em contrapartida, ao pagamento pelo LOCAT√ÅRIO dos valores de aluguel\n",
      "e encargos. A casa √© de dom√≠nio, propriedade e posse indireta do LOCADOR.\n",
      "CL√ÅUSULA TERCEIRA: PRAZO DE LOCA√á√ÉO\n",
      "===== {'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# e agora veja o conte√∫do de cada documento\n",
    "# perguntamos sobre valor do aluguel\n",
    "# e ele buscou os trechos q est√£o mais relacionados √† pergunta\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=====\", doc.metadata, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salvando o bd do FAISS**\n",
    "\n",
    "Observa√ß√£o importante\n",
    "\n",
    "- Chroma: sempre precisa apontar para uma pasta (`persist_directory`) se voc√™ quiser que ele salve os dados no disco. Sem isso, ele funciona s√≥ na mem√≥ria durante a execu√ß√£o, mas para manter o banco de dados entre execu√ß√µes, √© necess√°rio definir a pasta.\n",
    "\n",
    "- FAISS: n√£o precisa apontar pasta na cria√ß√£o, porque por padr√£o ele trabalha s√≥ na mem√≥ria RAM. Mas se quiser salvar o banco e reutilizar depois, voc√™ precisa usar `.save_local()` e `.load_local()` manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando...\n",
    "vector_store_faiss.save_local('arquivos_teste/faiss_bd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o bd faiss\n",
    "# o argumento dangerous deserialization √© usado \n",
    "# como True quando vc confia nos dados de origem,\n",
    "# pois ele pode deserializar c√≥digo malicioso\n",
    "# j√° q eu criei o banco e sei a origem, ent√£o deixo como True\n",
    "vector_store_faiss = FAISS.load_local(\n",
    "    'arquivos_teste/faiss_bd',\n",
    "    embeddings=embeddings_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß© **Resumo Vector Store: Chroma vs FAISS**\n",
    "\n",
    "**Ess√™ncia em 1 par√°grafo:**\n",
    "Tanto Chroma quanto FAISS s√£o vector stores que armazenam e fazem buscas por embeddings, mas a principal diferen√ßa est√° na facilidade de uso e integra√ß√£o. Chroma √© pensado para ser plug-and-play, com armazenamento local por padr√£o e at√© persist√™ncia nativa, √≥timo para prot√≥tipos e projetos de m√©dio porte. FAISS √© uma biblioteca da Meta, mais \"pura\", focada em alta performance e escala, muito usada para produ√ß√£o em grandes volumes de dados, mas exige mais configura√ß√£o e n√£o tem persist√™ncia embutida (voc√™ salva manualmente).\n",
    "\n",
    "**Principais pontos:**\n",
    "\n",
    "| Caracter√≠stica                  | Chroma                                | FAISS                                   |\n",
    "|----------------------------------|----------------------------------------|------------------------------------------|\n",
    "| üì¶ Instala√ß√£o e uso              | Super f√°cil, abstra√≠do, integrado      | Mais manual, requer mais configura√ß√£o    |\n",
    "| üíæ Persist√™ncia (salvar dados)    | Sim, nativa (`persist_directory`)      | N√£o nativa, precisa serializar manualmente |\n",
    "| üöÄ Performance                   | Boa para m√©dias bases                 | Muito r√°pido e otimizado para grandes bases |\n",
    "| üîó Integra√ß√£o com LangChain      | Muito fluida, oficial                 | Tamb√©m integra bem, mas demanda mais setup |\n",
    "| üî• Escalabilidade                | Limitada, para prot√≥tipos/projetos m√©dios | Alta escala, ideal para milh√µes de vetores |\n",
    "| üåê Multi-idioma / Multi-device   | Suporte mais limitado                 | Suporte para CPU/GPU e multi-threading avan√ßado |\n",
    "\n",
    "**Observa√ß√£o importante sobre FAISS:**\n",
    "\n",
    "> FAISS n√£o usa 'persist_directory' porque n√£o tem persist√™ncia nativa; ele guarda os dados na mem√≥ria durante a execu√ß√£o, e para manter os dados depois de fechar o programa, √© preciso salvar e carregar manualmente o √≠ndice.\n",
    "\n",
    "**Quando usar cada um:**\n",
    "\n",
    "- **Use Chroma se:**\n",
    "  - Quer prototipar r√°pido.\n",
    "  - Seu projeto tem at√© algumas dezenas ou centenas de milhares de vetores.\n",
    "  - Voc√™ quer facilidade de persist√™ncia local.\n",
    "\n",
    "- **Use FAISS se:**\n",
    "  - Precisa de alta performance em bases grandes (milh√µes de vetores).\n",
    "  - Vai rodar em produ√ß√£o com alta demanda.\n",
    "  - Est√° confort√°vel com setups mais avan√ßados e salvar/restaurar √≠ndices manualmente.\n",
    "\n",
    "---\n",
    "\n",
    "üß≠ **Outros Vector Stores para o dia a dia**\n",
    "\n",
    "**Ess√™ncia em 1 par√°grafo:**\n",
    "Al√©m de Chroma e FAISS, outros vector stores aparecem bastante na pr√°tica, principalmente quando se busca escalabilidade profissional, integra√ß√£o com nuvem ou recursos avan√ßados de busca h√≠brida (texto + vetores). Pinecone, Weaviate, Milvus e Redis Vector s√£o √≥timas op√ß√µes dependendo do cen√°rio.\n",
    "\n",
    "**Principais op√ß√µes:**\n",
    "\n",
    "- **Pinecone:**  \n",
    "  - SaaS dedicado e f√°cil de usar.\n",
    "  - Escala autom√°tica, alta disponibilidade.\n",
    "  - Integra bem com LangChain.\n",
    "  - Ideal para produ√ß√£o sem dor de cabe√ßa com infraestrutura.\n",
    "\n",
    "- **Weaviate:**  \n",
    "  - Open-source com busca h√≠brida (texto + vetores).\n",
    "  - Permite filtros avan√ßados por metadados.\n",
    "  - Escal√°vel e pronto para produ√ß√£o.\n",
    "\n",
    "- **Milvus:**  \n",
    "  - Open-source, alto desempenho para bilh√µes de vetores.\n",
    "  - Suporte forte para GPU.\n",
    "  - Indicado para Big Data e aplica√ß√µes massivas.\n",
    "\n",
    "- **Redis Vector:**  \n",
    "  - Usa Redis para armazenar vetores.\n",
    "  - Boa op√ß√£o se voc√™ j√° usa Redis na stack.\n",
    "  - Performance s√≥lida e suporte a filtros por metadados.\n",
    "\n",
    "**Resumo de escolha:**\n",
    "\n",
    "- **Prot√≥tipo/local:** Chroma\n",
    "- **Projeto pequeno, on-premise:** FAISS\n",
    "- **Produ√ß√£o escal√°vel:** Pinecone ou Weaviate\n",
    "- **Big Data + GPU:** Milvus\n",
    "- **Stack Redis:** Redis Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval ‚Äì Encontrando trechos relevantes\n",
    "\n",
    "√â a busca por trechos semelhantes √† pergunta que o usu√°rio fez.\n",
    "\n",
    "J√° fizemos isso acima na aula de vector sotres, e agora vamos ver de forma mais detalhada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vamos fazer um loading com 3 arquivos. Faremos um la√ßo for para carregar os 3 arquivos pdf.\n",
    "\n",
    "Mas veja que um desses arquivos est√° duplicado. Isso foi de forma intencional para vermos como os diferentes tipos de retrieval tratam um mesmo documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminhos = [\n",
    "    'arquivos_teste/contrato_aluguel_jose_nilton.pdf',\n",
    "    'arquivos_teste/vantagem_pdf.pdf',\n",
    "    'arquivos_teste/vantagem_pdf.pdf'\n",
    "]\n",
    "\n",
    "paginas = []\n",
    "\n",
    "for doc in caminhos:\n",
    "    loader = PyPDFLoader(doc)\n",
    "    paginas.extend(loader.load()) # adicionando cada arquivo carregado na lista\n",
    "\n",
    "# criando o splitter para dividir os documentos\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# agora splitando os documentos\n",
    "documents = recur_split.split_documents(paginas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modificando metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sabemos que todo documento traz metadados\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mas esses dados tamb√©m podem ser editados\n",
    "# e podemos tamb√©m adicionar novos metadados\n",
    "# vamos dar aqui um exemplo e modificar algumas coisas\n",
    "# eu quero por exemplo tirar o nome arquivos_teste do nome do arquivo\n",
    "# e adicionar um novo atributo, q √© o id do documento, q ter√° o valor do √≠ndice\n",
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata['source'] = doc.metadata['source'].replace('arquivos_teste/', '')\n",
    "    doc.metadata['doc_id'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'contrato_aluguel_jose_nilton.pdf', 'page': 0, 'doc_id': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora verificando depois da modifica√ß√£o\n",
    "# veja como ficaram os novos atributos\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando a VectorStore**\n",
    "\n",
    "Vamos criar essa base de dados para interagir com os documentos q splitamos acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro criamos o embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora criando um banco com o chroma\n",
    "diretorio = \"arquivos_teste/chroma_retrieval\"\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=diretorio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic Search**\n",
    "\n",
    "Esse √© o tipo de retrieval mais simples que existe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vantagens do PDF  \n",
      " \n",
      "ÔÉò Pequenho tamanho do arquivo: os arquivos possuem uma compacta√ß√£o \n",
      "aceit√°vel (ex.: Arquivos  de Word com 1Mb ap√≥s a convers√£o para PDF \n",
      "chegam a ficar com 100 Kb de tamanho, 10% do  original);  \n",
      "ÔÉò Os arquiv os PDF s√£o compactos e totalmente pesquis√°veis, podendo ser \n",
      "acessados a qualquer momento com o Adobe Reader.  \n",
      "ÔÉò N√£o apresentam problemas de fontes e/ou formata√ß√£o de arquivos;  \n",
      "ÔÉò Os documentos PDF podem ter direitos especiais de acesso e podem ser\n",
      "====={'doc_id': 25, 'page': 1, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n",
      "Vantagens do PDF  \n",
      " \n",
      "ÔÉò Pequenho tamanho do arquivo: os arquivos possuem uma compacta√ß√£o \n",
      "aceit√°vel (ex.: Arquivos  de Word com 1Mb ap√≥s a convers√£o para PDF \n",
      "chegam a ficar com 100 Kb de tamanho, 10% do  original);  \n",
      "ÔÉò Os arquiv os PDF s√£o compactos e totalmente pesquis√°veis, podendo ser \n",
      "acessados a qualquer momento com o Adobe Reader.  \n",
      "ÔÉò N√£o apresentam problemas de fontes e/ou formata√ß√£o de arquivos;  \n",
      "ÔÉò Os documentos PDF podem ter direitos especiais de acesso e podem ser\n",
      "====={'doc_id': 25, 'page': 1, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n",
      "Vantagens do PDF  \n",
      " \n",
      "ÔÉò Pequenho tamanho do arquivo: os arquivos possuem uma compacta√ß√£o \n",
      "aceit√°vel (ex.: Arquivos  de Word com 1Mb ap√≥s a convers√£o para PDF \n",
      "chegam a ficar com 100 Kb de tamanho, 10% do  original);  \n",
      "ÔÉò Os arquiv os PDF s√£o compactos e totalmente pesquis√°veis, podendo ser \n",
      "acessados a qualquer momento com o Adobe Reader.  \n",
      "ÔÉò N√£o apresentam problemas de fontes e/ou formata√ß√£o de arquivos;  \n",
      "ÔÉò Os documentos PDF podem ter direitos especiais de acesso e podem ser\n",
      "====={'doc_id': 32, 'page': 1, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"quais s√£o as vantagens de usar um arquivo pdf?\"\n",
    "\n",
    "# vamos pedir para ele trazer os 3 trechos mais relevantes\n",
    "docs = vector_db.similarity_search(pergunta, k=3)\n",
    "\n",
    "# agora vamos ver quais foram esses trechos q ele encontrou\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"====={doc.metadata}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que eu dupliquei o arquivo sobre pdf de prop√≥sito. Note que o modelo retornou 2 trechos repetidos, q s√£o os 2 primeiros. Veja pelo doc_id que tratam-se de dois documentos diferentes, mas com conte√∫do igual.\n",
    "\n",
    "O modelo n√£o conseguiu avaliar que o segundo trecho era exatamente igual ao primeiro e n√£o adiciona nenhuma informa√ß√£o nova. Essa √© uma das limita√ß√µes desse tipo de retrieval.\n",
    "\n",
    "Ele n√£o leva em conta o metadado e outros parametros, tornando a busca menos eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MMR (Max Marginal Relevance)**\n",
    "\n",
    "- Traz resultados diversos, **evita repeti√ß√£o** entre documentos.\n",
    "- Combina **relev√¢ncia** com **diversidade** dos documentos retornados.\n",
    "- √ötil para n√£o receber v√°rios trechos muito parecidos na resposta.\n",
    "\n",
    "Principais argumentos:\n",
    "- **query:** sua pergunta em texto.\n",
    "- **k:** total de documentos que voc√™ quer receber.\n",
    "- **fetch_k:** n√∫mero de documentos candidatos que ele vai considerar antes de escolher os melhores (default: maior que `k`).\n",
    "- **lambda_mult:** equil√≠brio entre relev√¢ncia e diversidade (de 0 a 1).\n",
    "  - 1 = mais foco na relev√¢ncia.\n",
    "  - 0 = mais foco na diversidade.\n",
    "\n",
    "Vamos aqui fazer um teste fazendo a mesma pergunta q fizemos no similarity search.\n",
    "\n",
    "Mas note como ele retorna uma resposta melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vantagens do PDF  \n",
      " \n",
      "ÔÉò Pequenho tamanho do arquivo: os arquivos possuem uma compacta√ß√£o \n",
      "aceit√°vel (ex.: Arquivos  de Word com 1Mb ap√≥s a convers√£o para PDF \n",
      "chegam a ficar com 100 Kb de tamanho, 10% do  original);  \n",
      "ÔÉò Os arquiv os PDF s√£o compactos e totalmente pesquis√°veis, podendo ser \n",
      "acessados a qualquer momento com o Adobe Reader.  \n",
      "ÔÉò N√£o apresentam problemas de fontes e/ou formata√ß√£o de arquivos;  \n",
      "ÔÉò Os documentos PDF podem ter direitos especiais de acesso e podem ser\n",
      "====={'doc_id': 25, 'page': 1, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n",
      "O que √© Adobe PDF?  \n",
      " \n",
      "O PDF (Portable Document Format) √© um formato de arquivo  \n",
      "desenvolvido pela Adobe Systems para representar  documentos de maneira \n",
      "independente do aplicativo,  hardware, e sistema operacional usados para \n",
      "cri√°-los. Um  arquivo PDF pode descrever documentos que contenham  texto, \n",
      "gr√°ficos e imagens num formato independente de  dispositivo e resolu√ß√£o.  \n",
      "Sua principal caracter√≠stica c onsiste em representar um documento\n",
      "====={'doc_id': 21, 'page': 0, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n",
      "aplicativos possuem um √≠cone na barra de ferramenta para convers√£o de \n",
      "arquivos para PDF.  \n",
      " \n",
      " \n",
      "Como funciona a tecnologia PDF  \n",
      " \n",
      "O PDF √© um tipo de arquivo que representa na tela do computador \n",
      "p√°ginas de documentos  eletr√¥ nicos. √â poss√≠vel converter para PDF v√°rios tipos \n",
      "de arquivos, desde os baseados em texto at√©  documentos como tabelas, \n",
      "gr√°ficos, image ns, etc. Para isso, o PDF gera arquivos usando os  princ√≠pios\n",
      "====={'doc_id': 30, 'page': 0, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"quais s√£o as vantagens de usar um arquivo pdf?\"\n",
    "\n",
    "# vamos pedir para ele trazer os 10 trechos mais similares com fetch_k\n",
    "# e no argumento k ele vai trazer os 3 mais relevantes\n",
    "docs = vector_db.max_marginal_relevance_search(pergunta, k=3, fetch_k=10)\n",
    "\n",
    "# agora vamos ver quais foram esses trechos q ele encontrou\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"====={doc.metadata}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Rela√ß√£o entre fetch_k e k\n",
    "\n",
    "- **fetch_k** √© o n√∫mero de documentos candidatos que o m√©todo vai considerar primeiro.\n",
    "- **k** √© o n√∫mero final de documentos que voc√™ realmente quer como resposta.\n",
    "\n",
    "üîé **Funcionamento:**\n",
    "1. Primeiro, ele pega os **fetch_k** documentos mais relevantes.\n",
    "2. Depois, ele filtra entre esses candidatos para escolher os **k** resultados finais, equilibrando **relev√¢ncia e diversidade** (controlado pelo `lambda_mult`).\n",
    "\n",
    "Exemplo:\n",
    "```python\n",
    "k = 5          # Quero 5 documentos no final\n",
    "fetch_k = 20   # Ele vai considerar 20 candidatos antes de escolher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtragem**\n",
    "\n",
    "O filter √© um argumento dentro do similarity_search que limita os resultados da busca aos documentos que correspondem aos metadados especificados.\n",
    "\n",
    "- O **filter** permite **filtrar** os documentos recuperados com base nos metadados que voc√™ salvou junto com os documentos.\n",
    "- Serve para refinar a busca, trazendo s√≥ documentos que atendam a certos crit√©rios.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "Se seus documentos t√™m metadado `{\"categoria\": \"contrato\"}`, voc√™ pode buscar s√≥ nessa categoria:\n",
    "```python\n",
    "docs = vector_store.similarity_search(\n",
    "    query=pergunta,\n",
    "    k=5,\n",
    "    filter={\"categoria\": \"contrato\"}\n",
    ")\n",
    "\n",
    "Vamos ver abaixo com o nosso exemplo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O que √© Adobe PDF?  \n",
      " \n",
      "O PDF (Portable Document Format) √© um formato de arquivo  \n",
      "desenvolvido pela Adobe Systems para representar  documentos de maneira \n",
      "independente do aplicativo,  hardware, e sistema operacional usados para \n",
      "cri√°-los. Um  arquivo PDF pode descrever documentos que contenham  texto, \n",
      "gr√°ficos e imagens num formato independente de  dispositivo e resolu√ß√£o.  \n",
      "Sua principal caracter√≠stica c onsiste em representar um documento\n",
      "====={'doc_id': 21, 'page': 0, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n",
      "O que √© Adobe PDF?  \n",
      " \n",
      "O PDF (Portable Document Format) √© um formato de arquivo  \n",
      "desenvolvido pela Adobe Systems para representar  documentos de maneira \n",
      "independente do aplicativo,  hardware, e sistema operacional usados para \n",
      "cri√°-los. Um  arquivo PDF pode descrever documentos que contenham  texto, \n",
      "gr√°ficos e imagens num formato independente de  dispositivo e resolu√ß√£o.  \n",
      "Sua principal caracter√≠stica c onsiste em representar um documento\n",
      "====={'doc_id': 21, 'page': 0, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"quais s√£o as vantagens de usar um arquivo pdf?\"\n",
    "\n",
    "docs = vector_db.similarity_search(\n",
    "    pergunta, \n",
    "    k=3,\n",
    "    filter={'doc_id':21})\n",
    "\n",
    "# agora veja q ele s√≥ buscou dentro do doc_id = 0\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"====={doc.metadata}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM Aided Retrieval**\n",
    "\n",
    "- LLM-Aided Retrieval usa o pr√≥prio modelo de linguagem (LLM) para **ajudar na recupera√ß√£o de documentos**, melhorando a qualidade das buscas.\n",
    "- Em vez de s√≥ usar vetores para buscar, o LLM pode:\n",
    "  - Reescrever ou expandir a pergunta antes de buscar.\n",
    "  - Filtrar ou rankear melhor os resultados recuperados.\n",
    "  - Fazer busca multi-turno (exemplo: perguntar ao LLM como refinar a consulta).\n",
    "\n",
    "Por que usar?\n",
    "- Ajuda quando a pergunta do usu√°rio n√£o √© t√£o clara.\n",
    "- Aumenta a precis√£o em buscas mais complexas ou amb√≠guas.\n",
    "\n",
    "Resumindo, em vez de voc√™ escrever manualmente filtros ou ajustes na busca, o modelo \"pensa por voc√™\" e traduz a pergunta em uma query estruturada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos importar as libs q precisamos\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'contrato_aluguel_jose_nilton.pdf', 'page': 0, 'doc_id': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui vamos apresentar para o modelo em quais atributos\n",
    "# ele deve prestar aten√ß√£o e uma breve descri√ß√£o de cada um\n",
    "metadata_info = [\n",
    "    AttributeInfo(\n",
    "        name='source',\n",
    "        description='√© o nome da apostila, onde o texto original foi retirado. \\\n",
    "            Deve ter o valor de: contrato_aluguel_jose_nilton.pdf',\n",
    "            type='string'\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name='page',\n",
    "        description='a p√°gina da apostila de onde o texto se origina.',\n",
    "            type='integer'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fornecendo a descri√ß√£o do documento como um todo\n",
    "content_description = 'apostila de cursos'\n",
    "llm = OpenAI()\n",
    "\n",
    "# e aqui criamos o nosso aided retriever\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vector_db,\n",
    "    content_description,\n",
    "    metadata_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As  partes  elegem  o  foro  da  comarca  de  Ibicara√≠/BA ,  para  dirimir  quaisquer\n",
      "controv√©rsias oriundas do presente instrumento.\n",
      "As partes declaram que tiveram acesso a este contrato, leram, compreenderam e que\n",
      "est√£o de acordo com todas as suas cl√°usulas, prometendo ainda a cumprirem e\n",
      "fazerem que se cumpra o presente Contrato de Loca√ß√£o.\n",
      "Por estarem assim justos e contratados, firmam o presente instrumento, em duas vias\n",
      "de igual teor.\n",
      "Ibicara√≠, 01 de Agosto de 2023.\n",
      "====={'doc_id': 17, 'page': 3, 'source': 'contrato_aluguel_jose_nilton.pdf'}\n",
      "\n",
      "\n",
      "As  partes  elegem  o  foro  da  comarca  de  Ibicara√≠/BA ,  para  dirimir  quaisquer\n",
      "controv√©rsias oriundas do presente instrumento.\n",
      "As partes declaram que tiveram acesso a este contrato, leram, compreenderam e que\n",
      "est√£o de acordo com todas as suas cl√°usulas, prometendo ainda a cumprirem e\n",
      "fazerem que se cumpra o presente Contrato de Loca√ß√£o.\n",
      "Por estarem assim justos e contratados, firmam o presente instrumento, em duas vias\n",
      "de igual teor.\n",
      "Ibicara√≠, 01 de Agosto de 2023.\n",
      "====={'doc_id': 17, 'page': 3, 'source': 'contrato_aluguel_jose_nilton.pdf'}\n",
      "\n",
      "\n",
      "de igual teor.\n",
      "Ibicara√≠, 01 de Agosto de 2023.\n",
      "______________________________________\n",
      "Kaio Oliveira Peixoto\n",
      "LOCADOR\n",
      "__________________________________________\n",
      "JOSE NILTON PEREIRA SILVA\n",
      "LOCAT√ÅRIO\n",
      "====={'doc_id': 18, 'page': 3, 'source': 'contrato_aluguel_jose_nilton.pdf'}\n",
      "\n",
      "\n",
      "de igual teor.\n",
      "Ibicara√≠, 01 de Agosto de 2023.\n",
      "______________________________________\n",
      "Kaio Oliveira Peixoto\n",
      "LOCADOR\n",
      "__________________________________________\n",
      "JOSE NILTON PEREIRA SILVA\n",
      "LOCAT√ÅRIO\n",
      "====={'doc_id': 18, 'page': 3, 'source': 'contrato_aluguel_jose_nilton.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"qual a data de assinatura do contrato?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(pergunta)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"====={doc.metadata}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG ‚Äì Conversando com os seus dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro vamos fazer todo o processo do zero\n",
    "# importando as bibliotecas\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o passo a passo\n",
    "\n",
    "# indicando o caminho dos meus arquivos\n",
    "caminhos = [\n",
    "    'arquivos_teste/contrato_aluguel_jose_nilton.pdf',\n",
    "    'arquivos_teste/vantagem_pdf.pdf',\n",
    "]\n",
    "\n",
    "# fazendo o load das p√°ginas usando o pdf loader\n",
    "paginas = []\n",
    "for caminho in caminhos:\n",
    "    loader = PyPDFLoader(caminho)\n",
    "    paginas.extend(loader.load())\n",
    "\n",
    "# dividindo os documentos para ele ser melhor consumido pelo modelo\n",
    "# criando o splitter\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"\", \" \"]\n",
    ")\n",
    "\n",
    "# aqui a gente executa o split\n",
    "documents = recur_split.split_documents(paginas)\n",
    "\n",
    "# modificando alguns metadados para facilitar a intera√ß√£o (opcional)\n",
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata['source'] = doc.metadata['source'].replace('arquivos_teste/', '')\n",
    "    doc.metadata['doc_id'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 13:12:26,805 - INFO - Backing off send_request(...) for 1.0s (requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Read timed out. (read timeout=15))\n"
     ]
    }
   ],
   "source": [
    "# criando a vector db\n",
    "\n",
    "# indicando onde ficar√° o banco de dados\n",
    "diretorio_db = \"arquivos_teste/chroma_retrieval_db\"\n",
    "\n",
    "# criando o embedder\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# juntando tudo e criando o vector db com o Chroma\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=diretorio_db\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando a Estrutura da Conversa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para criar a estrutura de chat precisamos instanciar o chat open ai\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora precisamos criar uma chain para unir e interagir com toda essa estrutura\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vector_db.as_retriever(search_type='mmr'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'quem √© o autor do artigo q fala sobre pdf?',\n",
       " 'result': 'O autor do artigo sobre PDF √© a Secretaria de Tecnologia da Informa√ß√£o do TRT da 4¬™ Regi√£o.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora vamos interagir!\n",
    "pergunta = \"quem √© o autor do artigo q fala sobre pdf?\"\n",
    "\n",
    "chat_chain.invoke({'query': pergunta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modificando o prompt da chain**\n",
    "\n",
    "J√° interagimos com o modelo, agora podemos mold√°-lo para que ele siga um comportamento espec√≠fico ou fa√ßa as coisas da maneira que queremos.\n",
    "\n",
    "Aqui entra a engenharia de prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ao criar o prompt precismos usar as palavras chave context e question\n",
    "# o context vai armazenar todos os documentos q fornecemos ao modelo\n",
    "# question representa a pergunta\n",
    "chain_prompt = PromptTemplate.from_template(\n",
    "\"\"\"Utilize o contexto fornecido para responder a pergunta ao final.\n",
    "Fale em tom de humor e descontra√ß√£o, pode manter a informalidade.\n",
    "Se voc√™ n√£o sabe a resposta, apenas diga que n√£o sabe e n√£o tente inventar resposta.\n",
    "Utilize tr√™s frases no m√°ximo, mantenha a resposta concisa.\n",
    "\n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Pergunta: {question}\n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora depois de criado o prompt\n",
    "# precisamos criar a chain novamente para inclu√≠-lo\n",
    "# veja q usamos o argumento chain type kwargs\n",
    "# e incluimos o novo prompt num dicion√°rio\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vector_db.as_retriever(search_type='mmr'),\n",
    "    chain_type_kwargs={'prompt':chain_prompt},\n",
    "    return_source_documents=True \n",
    ")\n",
    "\n",
    "# o argumento return_source_documents retorna as fontes pesquisadas\n",
    "# usamos aqui s√≥ para 'provar' q ele est√° usando o novo prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos ver como ele se comporta depois do prompt\n",
    "# vamos guardar o retorno na vari√°vel 'resposta'\n",
    "# esse retorno √© um dicion√°rio\n",
    "pergunta = \"o q √© adobe pdf?\"\n",
    "\n",
    "resposta = chat_chain.invoke({'query': pergunta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Adobe PDF √© tipo o super-her√≥i dos arquivos, salva voc√™ de qualquer confus√£o com formatos diferentes.  \n",
      "√â tipo um uniforme que faz qualquer documento ficar bonit√£o e acess√≠vel em qualquer dispositivo.  \n",
      "Pra resumir, √© tipo uma varinha m√°gica que transforma seus arquivos em algo indestrut√≠vel e prontos pra qualquer situa√ß√£o.\n"
     ]
    }
   ],
   "source": [
    "# vamos ver a chave result, q √© a resposta do modelo\n",
    "# veja q ele retornou a resposta em 3 frases, conforme pedimos no prompt\n",
    "# veja q ele tamb√©m mudou o tom para um pouco de humor\n",
    "print(resposta['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='O que √© Adobe PDF?  \\n \\nO PDF (Portable Document Format) √© um formato de arquivo  \\ndesenvolvido pela Adobe Systems para representar  documentos de maneira \\nindependente do aplicativo,  hardware, e sistema operacional usados para \\ncri√°-los. Um  arquivo PDF pode descrever documentos que contenham  texto, \\ngr√°ficos e imagens num formato independente de  dispositivo e resolu√ß√£o.  \\nSua principal caracter√≠stica c onsiste em representar um documento', metadata={'doc_id': 28, 'page': 0, 'source': 'vantagem_pdf.pdf'}),\n",
       " Document(page_content='essenciais da tecnologia PostScript, que √© uma esp√©cie de linguagem usada \\npara  const ruir p√°ginas para os mais diversos fins.  \\nEm geral, √© poss√≠vel transformar qualquer arquivo que possa ser impre sso \\nem arquivos PDF.   \\n \\n \\nSecretaria de Tecnologia da Informa√ß√£o do TRT da 4¬™ Regi√£o', metadata={'doc_id': 31, 'page': 0, 'source': 'vantagem_pdf.pdf'}),\n",
       " Document(page_content='assinados digitalmente , ou seja, i mpedem qualquer tipo  de altera√ß√£o no \\narquivo original ; \\n\\uf0d8 Existem v√°rios programas gratuitos que geram PDF como: PDFMaker, PDF \\nReDirect, PDFCreator  entre outros.  \\nObs.:  o software mais conhecido para a gera√ß√£o e  manipula√ß√£o de arquivos \\nem PDF √© o Adobe Acrobat.√â  necess√°rio ter cuidado para n√£o confundir com \\no Adobe  Reader, que √© simplesmente um leitor gratuito de PDFs,  ou seja, √© um \\nprograma permite somente a leitura de  arquivos PDF', metadata={'doc_id': 26, 'page': 1, 'source': 'vantagem_pdf.pdf'}),\n",
       " Document(page_content='LOCADOR ‚Äì KAIO OLIVEIRA PEIXOTO\\n___________________________________________ \\nLOCAT√ÅRIO ‚Äì  JOSE NILTON PEREIRA SILVA', metadata={'doc_id': 20, 'page': 4, 'source': 'contrato_aluguel_jose_nilton.pdf'})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora tbm podemos acessar a chave source_documents\n",
    "# ele traz todos os arquivos q o modelo utilizou para gerar a resposta\n",
    "resposta['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outros tipos de chains\n",
    "\n",
    "üîÑ chain_type='stuff' ‚Äî o que √© e como funciona\n",
    "\n",
    "- O `chain_type='stuff'` define como os documentos recuperados v√£o ser passados para o modelo LLM gerar a resposta.\n",
    "- No modo `\"stuff\"`, **todos os documentos recuperados s√£o \"empilhados\" (stuffed) em um √∫nico prompt**, e esse prompt √© enviado direto ao modelo.\n",
    "\n",
    "Exemplo pr√°tico:\n",
    "Se voc√™ recuperou 5 documentos com `k=5`, o LangChain junta todos os textos desses documentos num s√≥ prompt e diz:\n",
    "> \"Com base nesses textos, responda a pergunta...\"\n",
    "\n",
    "**‚úÖ Vantagem:** simples e direto, √≥timo para poucos documentos pequenos.  \n",
    "**‚ö†Ô∏è Limite:** se os textos forem muito grandes, pode ultrapassar o limite de tokens do modelo.\n",
    "\n",
    "---\n",
    "\n",
    "üîÅ Outros `chain_type` dispon√≠veis:\n",
    "\n",
    "1. `\"map_reduce\"`  \n",
    "- Divide os documentos em partes.\n",
    "- Roda o LLM em cada parte separadamente (**map**) e depois junta os resultados (**reduce**).\n",
    "- Melhor para textos longos.\n",
    "\n",
    "2. `\"refine\"`  \n",
    "- Cria uma resposta inicial com o primeiro documento e vai **refinando** essa resposta com os pr√≥ximos.\n",
    "- Mais lento, mas pode melhorar a coer√™ncia.\n",
    "\n",
    "3. `\"map_rerank\"`  \n",
    "- Avalia cada documento individualmente e escolhe a melhor resposta com uma nota de relev√¢ncia.\n",
    "- √ötil quando voc√™ quer **qualidade acima de quantidade**.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Conclus√£o curta:\n",
    "> `stuff` junta todos os docs e envia de uma vez ao LLM ‚Äî simples e eficiente para entradas pequenas.  \n",
    "> Para documentos longos, prefira `map_reduce` ou `refine`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stuff**\n",
    "\n",
    "Quando a gente une nosso prompt com o contexto, existem v√°rias formas de fazer isso.\n",
    "\n",
    "A forma mais simples e mais utilizada √© a stuff. Esse tamb√©m √© o valor default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Adobe PDF (Portable Document Format) √© um formato de arquivo desenvolvido pela Adobe Systems para representar documentos de maneira independente do aplicativo, hardware e sistema operacional usados para cri√°-los. Ele permite a representa√ß√£o de documentos contendo texto, gr√°ficos e imagens de forma independente de dispositivo e resolu√ß√£o. √â comumente utilizado para compartilhar documentos de forma segura e preservar a formata√ß√£o original.\n"
     ]
    }
   ],
   "source": [
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vector_db.as_retriever(search_type='mmr'),\n",
    "    chain_type='stuff'\n",
    ")\n",
    "\n",
    "pergunta = \"o q √© adobe pdf?\"\n",
    "\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "\n",
    "print(resposta['result'], end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Map Reduce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vector_db.as_retriever(search_type='mmr'),\n",
    "    chain_type='map_reduce'\n",
    ")\n",
    "\n",
    "pergunta = \"o q √© adobe pdf?\"\n",
    "\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Refine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Adobe PDF √© um formato de arquivo desenvolvido pela Adobe Systems que permite representar documentos de forma independente do aplicativo, hardware e sistema operacional utilizados para cri√°-los. Ele √© amplamente utilizado devido √† sua capacidade de preservar a formata√ß√£o original e ser facilmente compartilhado e visualizado em diferentes dispositivos. Os arquivos PDF tamb√©m podem ser assinados digitalmente, o que impede qualquer tipo de altera√ß√£o no arquivo original. Existem v√°rios programas gratuitos dispon√≠veis para gerar arquivos PDF, como PDFMaker, PDF ReDirect e PDFCreator, por√©m o software mais conhecido para a gera√ß√£o e manipula√ß√£o de arquivos em PDF √© o Adobe Acrobat. √â importante ter cuidado para n√£o confundir o Adobe Acrobat com o Adobe Reader, que √© simplesmente um leitor gratuito de PDFs e permite apenas a leitura de arquivos PDF.\n"
     ]
    }
   ],
   "source": [
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vector_db.as_retriever(search_type='mmr'),\n",
    "    chain_type='refine'\n",
    ")\n",
    "\n",
    "pergunta = \"o q √© adobe pdf?\"\n",
    "\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "\n",
    "print(resposta['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
