{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Você é um grande leitor de livros e conselheiro tbm.\\\n",
    "Tem um tom ironico e sacadas que lembra Abujamra.\\\n",
    "Não precisa ficar falando tanto, mas você é uma pessoa interessante.\\\n",
    "Seu nome é Bonifácio e sabe tudo sobre literatura, mas sabe conversar sobre qualquer assunto.\\\n",
    "você trabalhou em várias bibliotecas, depois começou a dar palestras e ficou rico.\\\n",
    "Hoje você está morando na Finlandia.\\\n",
    "\n",
    "Conversa atual:\n",
    "\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "AI:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(input='onde vc mora e qual autor q eu gosto?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLMCHAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Você vai receber um nome comum e vai transformá-lo em apelido do futebol brasileiro\\\n",
    "use nomes e apelidos comumente usados para jogadores no Brasil. Pode ter ligação com \\\n",
    "o nome, mas não necessariamente. Este é o nome: {nome}\n",
    "\"\"\")\n",
    "\n",
    "chain_apelido = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_template,\n",
    "    \n",
    ")\n",
    "\n",
    "prompt_frase = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "O narrador da partida precisa apresentar com emoção!\\\n",
    "Crie uma frase épica e um bordão pra narrar o gol do jogador {apelido}\n",
    "\"\"\")\n",
    "\n",
    "chain_frase = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_frase,\n",
    "    \n",
    ")\n",
    "\n",
    "prompt_grito = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Agora você vai simular o comentário do assistente depois da narração. {frase}\\\n",
    "Faça um complemento interessante, digno de grandes comentaristas.\n",
    "\"\"\")\n",
    "\n",
    "chain_grito = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_grito,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIMPLE SEQUENTIAL CHAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_final = SimpleSequentialChain(\n",
    "    chains=[chain_apelido, chain_frase, chain_grito],\n",
    "    verbose=True # aqui ele vai mostrar todas cadeias de pensamento\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome = \"Baiano\"\n",
    "\n",
    "chain_final.run(nome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEQUENTIAL CHAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Você terá um assunto para debater. Não precisa de uma argumentação longa.\n",
    "Só precisa dar uma opinião bem polemica sobre o tema. Este é o tema: {tema}\n",
    "\"\"\")\n",
    "\n",
    "chain_apelido = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_template,\n",
    "    output_key='opiniao1'\n",
    "    \n",
    ")\n",
    "\n",
    "prompt_frase = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Dado o tema {tema} e a opinião polemica do colega: \"{opiniao1}\".\n",
    "Agora desenvolda uma replica refutando essa opiniao do colega.\n",
    "\"\"\")\n",
    "\n",
    "chain_frase = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_frase,\n",
    "    output_key='opiniao2'\n",
    "    \n",
    ")\n",
    "\n",
    "prompt_coment = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Seu papel aqui é o da síntese. Dado o tema {tema} e \n",
    "as opiniões colega 1: \"{opiniao1}\". Analise tambem a opinião do colega 2: \"{opiniao2}\".\n",
    "Agora crie um meio termo para que ambos possam chegar num acordo.\n",
    "\"\"\")\n",
    "\n",
    "chain_grito = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_coment,\n",
    "    output_key='sintese'\n",
    ")\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains=[chain_apelido, chain_frase, chain_grito],\n",
    "    input_variables=['tema'],\n",
    "    output_variables=['opiniao1', 'opiniao2', 'sintese'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "nome='futebol'\n",
    "\n",
    "chain.invoke({'tema':nome})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RUNNABLES**\n",
    "\n",
    "São componentes especiais do langchain. Eles são modulares, facilitam a execução de tarefas, reutilização do código e composição/encademaneto de fluxos de ia.\n",
    "\n",
    "Vc pode criar funções de python com runnables e a partir desse momento as funções adquirem os métodos dos runnables como invoke, stream, batch...\n",
    "\n",
    "Vamos dar um exemplo usando a o RunnalbleLambda:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnableParallel\n",
    "\n",
    "# Criamos um Runnable que transforma um texto para minúsculas\n",
    "minusculo = RunnableLambda(lambda x: x.lower())\n",
    "\n",
    "# Testando a transformação\n",
    "minusculo.invoke('FALA COMIGO!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veja q o runnable lambda tbm aceita função nomeada\n",
    "\n",
    "def converte_minusculo(palavra)-> str:\n",
    "    return palavra.lower()\n",
    "\n",
    "# Criamos um Runnable que transforma um texto para minúsculas\n",
    "minusculo = RunnableLambda(converte_minusculo)\n",
    "\n",
    "# Testando a transformação\n",
    "minusculo.invoke('FALA COMIGO!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos para outro runnable um pouco mais elaborado, o RunnableParallel.\n",
    "\n",
    "Ele pode rodar chains em paralelo de forma assíncrona.\n",
    "\n",
    "Podemos fazer um encadeamento para que o modelo gere alguns outputs iniciais e um modelo final consuma esse output, facilitando o fluxo.\n",
    "\n",
    "Vamos ver isso em código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos criar duas chains\n",
    "# e depois vamos rodá-las em paralelo\n",
    "\n",
    "prompt_nome_produto = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Vc vai desenvolver um nome para o seguinte tipo de produto: {produto}.\n",
    "Deve ter um apelo de marketing interessante!\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain_produto = prompt_nome_produto | chat\n",
    "\n",
    "\n",
    "prompt_cliente = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Vc é um publicitário e vai me dizer qual é o perfil do público em potencial\n",
    "desse produto: {produto}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain_cliente = prompt_cliente | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um runnable das chains\n",
    "# o output é um dicionário e cada chave\n",
    "# traz o retorno de sua respectiva chain\n",
    "paralelo = RunnableParallel(nome_produto=chain_produto, publico=chain_cliente)\n",
    "\n",
    "paralelo.invoke('cachaça')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esse prompt vai receber o output das chains anteriores, executadas em paralelo\n",
    "\n",
    "prompt_peca = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Vc é um publicitário rebuscado e muito conceituado.\n",
    "\n",
    "Vc vai desenvolver uma peça publicitária, um chamado para \n",
    "o público consumir o produto. Vou te falar o nome da empresa e \n",
    "a descrição do público alvo. Segue:\n",
    "\n",
    "nome da empresa: {nome_produto}\n",
    "publico alvo: {publico}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui nós juntamos tudo através dos pipes\n",
    "# o paralelo fornece os argumentos para o prompt_peca\n",
    "# esse prompt final é fornecido para um modelo\n",
    "# e depois o output parser traz formata esse retorno para exibição\n",
    "chain_final = paralelo | prompt_peca | chat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui chamamos o metodo invoke para ver o resultado\n",
    "chain_final.invoke({'produto':'reports e analise de dados para o setor de RH'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CADEIAS DE ROTEAMENTO**\n",
    "\n",
    "O roteamento permite direcionar a demanda do usuário (pergunta ou algo tipo) para a chain especializada em responder aquela pergunta. \n",
    "\n",
    "Por exemplo, podemos ter um aplicativo que receba perguntas de alunos do ensino fundamental.\n",
    "\n",
    "Com o roteador, ele consegue categorizar essa pergunta e direcionar para a chain correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando o modelo\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "# criando os prompts especializados para cada matéria\n",
    "fis_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Vc é um exímio professor de física.\n",
    "Vc é ótimo em responder perguntas sobre física de forma concisa e fácil de entender.\\\n",
    "Quando não sabe a resposta para uma perginta, vc admite que não sabe.\n",
    "                                                \n",
    "Aqui está a pergunta: {input}\"\"\")\n",
    "\n",
    "chain_fisica = fis_template | model\n",
    "\n",
    "mat_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Vc é um exímio professor de matemática, a verdadeira reencarnação de Pitágoras.\\\n",
    "Vc é ótimo em responder perguntas sobre matemática de forma concisa e fácil de entender.\\\n",
    "Quando não sabe a resposta para uma perginta, vc admite que não sabe.\n",
    "                                                \n",
    "Aqui está a pergunta: {input}\"\"\")\n",
    "\n",
    "chain_mat = mat_template | model\n",
    "\n",
    "hist_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Vc é um exímio professor de história, a verdadeira reencarnação de Eric Hobsbawm.\\\n",
    "Vc é ótimo em responder perguntas sobre história de forma concisa e fácil de entender.\\\n",
    "Quando não sabe a resposta para uma perginta, vc admite que não sabe.\n",
    "                                                \n",
    "Aqui está a pergunta: {input}\"\"\")\n",
    "\n",
    "chain_hist = hist_template | model\n",
    "\n",
    "# esse aqui é um prompt genérico de escape\n",
    "# caso a pergunta do aluno nao se encaixe em nenhuma alternativa\n",
    "prompt_generico = ChatPromptTemplate.from_template('''{pergunta}''')\n",
    "chain_generica = prompt_generico | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos criar um \"parseador\" para que ele retorne a resposta exatamente da forma que precisamos. Para isso, vamos utilizar o pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos criar um Categorizador, que será um objeto Pydantic\n",
    "\n",
    "class Categorizador(BaseModel):\n",
    "    \"Categoriza as perguntas dos alunos\"\n",
    "    area_conhecimento: str = Field(description=\"A área de conhecimento \\\n",
    "                                   da pergunta feita pelo aluno. Deve ser:\\\n",
    "                                   'física', 'matemática' ou 'história'.\\\n",
    "                                   Caso não encontre nenhuma delas, retorne 'outra'.\")\n",
    "    \n",
    "\n",
    "# vamos adicionar este prompt à chain\n",
    "# não é obrigatório, mas pode ajudar \n",
    "# a retornar melhores respostas do modelo\n",
    "    \n",
    "prompt_categoria = ChatPromptTemplate.from_template(\"Você deve categorizar\\\n",
    "                                                    a seguinte pergunta: {pergunta}.\\\n",
    "                                                    Vc deve retornar sempre nesse padrão,\\\n",
    "                                                    com letras em minúculas.\")\n",
    "    \n",
    "# criando a chain\n",
    "model_estruturado = prompt_categoria | model.with_structured_output(Categorizador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testando\n",
    "model_estruturado.invoke({'pergunta':\"quando foi o periodo da ditadura no brasil?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRIANDO ESTRUTURA DE ROTEAMENTO\n",
    "\n",
    "O roteador será uma função, q vai pegar o outuput do parseador q criamos acima, e chamar e atribuir para a chain necessária.\n",
    "\n",
    "Antes, vamos criar um runnable para agilizar a interação com esta função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui só uma demontração do q esse runnable faz\n",
    "# veja que ele recebe a pergunta e a resposta e \n",
    "# passa para frente, em forma de dicionário\n",
    "chain_runnable = RunnablePassthrough().assign(categoria=model_estruturado)\n",
    "chain_runnable.invoke({'pergunta':'quem foi Ruy Barbosa?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos agor definir a função roteadora\n",
    "# será uma condicional simples\n",
    "def route(input):\n",
    "    if input['categoria'] == 'matemática':\n",
    "        return chain_mat\n",
    "    if input['categoria'] == 'física':\n",
    "        return chain_fisica\n",
    "    if input['categoria'] == 'história':\n",
    "        return chain_hist\n",
    "    \n",
    "    \n",
    "    return chain_generica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos fazer nossa chain com runnable e agora com o roteador\n",
    "chain_runnable = RunnablePassthrough().assign(categoria=model_estruturado) | route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veja que ele recebe a pergunta e responde de acordo\n",
    "# as chains especializadas poderiam ser com outras funções e mais complexas\n",
    "# o roteamento permite que a gente faça essa distribuição de forma mais eficiente\n",
    "chain_runnable.invoke({'pergunta':'quem foi Ruy Barbosa?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMÓRIA\n",
    "\n",
    "Sem memória, nosso modelo não tem nenhum contexto da conversa. Toda mensagem será uma nova mensagem sem histórico algum.\n",
    "\n",
    "Aqui vamos aprender a adicionar memória ao nosso bot usando langchain.\n",
    "\n",
    "Vamos usar a classe InMemoryChatMessageHistory. Ela guarda uma lista de mensagens (self.messages), mas só em memória RAM — ou seja, não persiste no disco ou banco de dados. Quando o processo termina (por exemplo, se você fechar o servidor ou recarregar o notebook), tudo é perdido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando a memoria\n",
    "memory = InMemoryChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos usar alguns métodos dessa classe\n",
    "# para adicionar mensagens ao histórico\n",
    "memory.add_user_message('olá, meu nome é Kaio.')\n",
    "memory.add_ai_message('olá, guardei seu nome.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e aqui podemos visualizar esse histórico\n",
    "memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui vamos construir uma chain pronta para receber a memoria\n",
    "# para esse fim, é necessário criar esse placeholder conforme abaixo\n",
    "# e atribuir o nome 'history'\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'você é um tutor de programação q se chama Lovelace.\\\n",
    "     Responda às perguntas de forma didatica.'),\n",
    "     ('placeholder', '{history}'),\n",
    "     ('human', '{pergunta}')\n",
    "])\n",
    "\n",
    "# criando a chain\n",
    "chain = prompt | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar a memória para adicionar ao modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um dicionário onde ficarão as conversas por usuário\n",
    "store = {}\n",
    "\n",
    "# e aqui criamos uma função para adicionar \n",
    "# nova seção de memória ao dicionário\n",
    "# caso já exista, ele busca e retorna essa memória \n",
    "# cada usuário (ou sessão) tem seu próŕio histórico\n",
    "def get_session_by_id(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos adicionar a memoria criada anteriormente à nossa chain, que tbm já havíamos criado. Para isso, utilizaremos um runnable específico para este fim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora usando o runnable para criar a chain final\n",
    "# com memoria\n",
    "\n",
    "chain_com_memoria = RunnableWithMessageHistory(\n",
    "    chain, # fornecendo a chain com o prompt e modelo\n",
    "    get_session_by_id, # fornecendo a função q gerencia memoria\n",
    "    input_messages_key='pergunta', # fornecendo a variavel de entrada do usuário\n",
    "    history_messages_key='history' # fornecendo a variavel dos historico de conversa\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos testar essa nova chain com memoria.\n",
    "\n",
    "Antes de testar a nova chain com memória, precisamos criar um dicionário chamado config, que será responsável por gerenciar diferentes usuários através de um identificador único (session_id). Esse session_id é essencial para que a memória funcione corretamente, permitindo que o histórico de conversas seja separado por usuário. Abaixo, configuramos o session_id como \"user_a\" e chamamos a chain passando tanto a pergunta quanto a configuração:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'configurable': {'session_id':'user_a'}}\n",
    "resposta = chain_com_memoria.invoke({'pergunta':'olá meu nome é Zé!'},\n",
    "                                    config=config)\n",
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos só validar se ele manteve a memoria e lembra o meu nome\n",
    "resposta = chain_com_memoria.invoke({'pergunta':'qual é o meu nome?'},\n",
    "                                    config=config)\n",
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora olha q coisa interessante\n",
    "# veja que se mudar de usuário no config\n",
    "# ele já muda de histórico\n",
    "# agora o modelo não sabe mais o nome do usuário pq esse de fato é outro usuario\n",
    "config = {'configurable': {'session_id':'user_b'}}\n",
    "resposta = chain_com_memoria.invoke({'pergunta':'qual é o meu nome?'},\n",
    "                                    config=config)\n",
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG: Conversando com os seus dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Lembrete: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**O que é RAG?**\n",
    "- Técnica que combina **geração de texto (LLM)** com **recuperação de informações externas**.\n",
    "- Permite que o modelo acesse dados atualizados e específicos que não estão em seu treinamento original.\n",
    "\n",
    "**Por que é importante?**\n",
    "- LLMs sozinhos dependem apenas do conhecimento aprendido até a data de corte.\n",
    "- RAG permite incorporar informações externas (PDFs, documentos, banco de dados etc.).\n",
    "- Ideal para criar **aplicações personalizadas**, como chatbots que respondem com base em documentos internos.\n",
    "\n",
    "**Etapas do RAG:**\n",
    "1. **📥 Carregamento de Documentos**: importar PDFs, CSVs, bancos de dados, etc.\n",
    "2. **✂️ Divisão de Documentos**: separar em trechos menores mantendo o contexto.\n",
    "3. **🔢 Embedding**: transformar os trechos em vetores numéricos.\n",
    "4. **💾 Armazenamento em VectorStore**: salvar os vetores para busca eficiente.\n",
    "5. **🔍 Recuperação (Retrieval)**: buscar os vetores mais relevantes com base na pergunta do usuário.\n",
    "6. **🧠 Geração**: o modelo usa os trechos recuperados para gerar uma resposta precisa.\n",
    "\n",
    "**Desafios:**\n",
    "- Limite de tokens nos modelos (ex: 16k no GPT-3.5).\n",
    "- Selecionar apenas os trechos mais relevantes para enviar ao modelo.\n",
    "\n",
    "**Pontos-chave para lembrar:**\n",
    "- RAG = LLM + Dados externos ➜ respostas mais **precisas e atualizadas**.\n",
    "- Aplicável em diversos contextos práticos (ex: atendimento ao cliente, busca em base interna).\n",
    "- A qualidade da resposta depende diretamente dos dados que você fornece ao modelo.\n",
    "\n",
    "> 🛠️ Aplicar RAG pode transformar suas aplicações com IA, tornando-as mais úteis, contextuais e confiáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loaders – Carregando dados com Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carregando PDFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a classe necessária para manipular pdf\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui é o caminho onde ficará o arquivo\n",
    "caminho = \"arquivos_teste/contrato_aluguel_jose_nilton.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fornecendo o pdf no loader\n",
    "loader = PyPDFLoader(caminho)\n",
    "# agora estamos carregando o pdf\n",
    "documentos = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aqui a gente vê a quantidade de páginas do documento\n",
    "len(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONTRATO DE LOCAÇÃO RESIDENCIAL\\nO LOCADOR e o LOCATÁRIO, qualificados abaixo (em conjunto denominados “Partes”,\\ne, isoladamente, “Parte”), celebram este “ Instrumento Particular de Contrato de\\nLocação” (“Contrato”), que será regido pelo disposto nas Leis federais nºs \\xa08.245/1991\\n(“Lei do Inquilinato”) e 10.406/2002 (“Código Civil”), e se comprometer a cumprir:\\nCLÁUSULA PRIMEIRA: DA QUALIFICAÇÃO DAS PARTES\\nLOCADOR: KAIO OLIVEIRA PEIXOTO, BRASILEIRO, SOLTEIRO, portador da cédula de\\nidentidade R.G. nº 315622456 e CPF nº 021.667.755-60, residente e domiciliado na\\nRua João Pessoa, nº 401, CEP 45745-000, bairro Centro, Ibicaraí-BAHIA;\\nLOCATÁRIO:  JOSE  NILTON  PEREIRA  SILVA ,  BRASILEIRO,  SOLTEIRO,  COMERCIANTE,\\nportador da cédula de identidade R.G. nº 15403176-33 e CPF nº 054.568.205-36,\\nresidente e domiciliado na Rua Paraguaçu, nº 272, CEP 45745-000, bairro Centro,\\nIbicaraí-BAHIA;\\nCLÁUSULA SEGUNDA: OBJETO\\nPor meio deste Contrato, o LOCADOR entrega ao LOCATÁRIO a posse e uso do imóvel\\nsituado  na  Rua  Paraguaçu,  nº 269, CEP  45745-000,  bairro  Centro, Ibicaraí-BAHIA ,\\nfazendo jus, em contrapartida, ao pagamento pelo LOCATÁRIO dos valores de aluguel\\ne encargos. A casa é de domínio, propriedade e posse indireta do LOCADOR.\\nCLÁUSULA TERCEIRA: PRAZO DE LOCAÇÃO\\nEste  Contrato  vigorará  pelo  prazo  disposto  de  06  (seis)  meses,  tendo  início  em\\n01/08/2023 e término previsto para o dia  01/02/2024, podendo ser renovado por\\ninteresse das partes. Após finalizado o contrato, deverá o LOCATÁRIO restituir o Imóvel\\nao LOCADOR, no estado em que recebeu, salvo as deteriorações naturais ao seu uso\\nregular.\\nCLÁUSULA QUARTA: ALUGUEL E SEU REAJUSTE\\nPagará o LOCATÁRIO ao LOCADOR o valor mensal de R$ 350,00, a título de aluguel. Os\\naluguéis serão pagos todo dia 05 de cada mês, efetivando-se por depósito bancário no\\nBanco do Brasil, Agência 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\\nPeixoto.\\nO aluguel pactuado acima sofrerá reajustes anuais com base na variação do Índice\\nGeral de Preços divulgado pela Fundação Getúlio Vargas (IGP-FGV) ou outro índice que\\nporventura venha a substituí-lo. \\xa0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aqui por exemplo estamos vendo o conteúdo da primeira página\n",
    "documentos[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e aqui a gente pode ver alguns metadados\n",
    "documentos[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo perguntas para o arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a chain necessária para interagir com o pdf\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# ao criar a chain, se colocarmos o verbose como True\n",
    "# ele vai mostrar como trabalha debaixo dos panos\n",
    "chain = load_qa_chain(llm=chat, chain_type=\"stuff\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contrato de locação residencial entre Kaio e Jose.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pergunta = \"fale de forma bem resumida em no máximo \\\n",
    "    10 palavras do q se trata esse documento?\"\n",
    "\n",
    "# agora vamos rodar a chain, veja q ela precisa de 2 inputs\n",
    "# primeiro passamos o documento que queremos analisar\n",
    "# e depois a pergunta\n",
    "resposta = chain.invoke({'input_documents':documentos, 'question':pergunta})\n",
    "\n",
    "resposta['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carregando CSVs**\n",
    "\n",
    "Veja que o processo é bem parecido com o carregamento de PDFs.\n",
    "\n",
    "A maior mudança é no nome do loader mesmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui é o caminho onde ficará o arquivo\n",
    "caminho_csv = \"arquivos_teste/Top 1000 IMDB movies.csv\"\n",
    "loader = CSVLoader(caminho_csv)\n",
    "documento_csv = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cada 'página' do documento seria uma linha do arquivo csv\n",
    "len(documento_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': 0\\nMovie Name: The Shawshank Redemption\\nYear of Release: (1994)\\nWatch Time: 142 min\\nMovie Rating: 9.3\\nMeatscore of movie: 81\\nVotes: 34,709\\nGross: $28.34M\\nDescription: Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# essa é a primeira linha\n",
    "documento_csv[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veja tbm q inclusive a criação da chain é igual à da leitura de pdf\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chain_csv = load_qa_chain(llm=chat, chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Desculpe, não consigo responder a essa pergunta com base nas informações fornecidas.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a única diferença para o pdf é que forneceremos\n",
    "# os documentos csv como input\n",
    "pergunta = \"dessa lista aí, cite o filme pior rankeado e qual o nome dele\"\n",
    "\n",
    "resposta = chain_csv.invoke({'input_documents':documento_csv[:50], 'question':pergunta})\n",
    "\n",
    "resposta['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUTUBE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para \"assistir\" vídeos do youtube deve-se fazer o processo abaixo.\n",
    "\n",
    "Ele vai acessar o link e transcrever o audio através da api da OpenAI, com o método whisper.\n",
    "\n",
    "Seria necessária a instalação de algumas bibliotecas ainda para completar essa solução, mas a base está aí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deprecated Feature: Support for Python version 3.8 has been deprecated. Please update to Python 3.9 or above\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Px8Iu2viG7I\n",
      "[youtube] Px8Iu2viG7I: Downloading webpage\n",
      "[youtube] Px8Iu2viG7I: Downloading ios player API JSON\n",
      "[youtube] Px8Iu2viG7I: Downloading mweb player API JSON\n",
      "[youtube] Px8Iu2viG7I: Downloading player 73381ccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/73381ccc/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] Px8Iu2viG7I: nsig extraction failed: Some formats may be missing\n",
      "         n = 1Rq63z-Csi5joMKSz ; player = https://www.youtube.com/s/player/73381ccc/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/73381ccc/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] Px8Iu2viG7I: nsig extraction failed: Some formats may be missing\n",
      "         n = Fy6og7FLVKv-yiIXg ; player = https://www.youtube.com/s/player/73381ccc/player_ias.vflset/en_US/base.js\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Px8Iu2viG7I: Downloading m3u8 information\n",
      "[info] Px8Iu2viG7I: Downloading 1 format(s): 140\n",
      "[download] arquivos_teste/AMIGO NOVO.m4a has already been downloaded\n",
      "[download] 100% of    3.44MiB\n",
      "[ExtractAudio] Not converting audio arquivos_teste/AMIGO NOVO.m4a; file is already in target format m4a\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "pydub package not found, please install it with `pip install pydub`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/langchain_community/document_loaders/parsers/audio.py:54\u001b[0m, in \u001b[0;36mOpenAIWhisperParser.lazy_parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpydub\u001b[39;00m \u001b[39mimport\u001b[39;00m AudioSegment\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydub'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m save_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39marquivos_teste\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m loader \u001b[39m=\u001b[39m GenericLoader(\n\u001b[1;32m      4\u001b[0m     YoutubeAudioLoader([url], save_dir),\n\u001b[1;32m      5\u001b[0m     OpenAIWhisperParser()\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m docs \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m~/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/langchain_core/document_loaders/base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m     28\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlazy_load())\n",
      "File \u001b[0;32m~/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/langchain_community/document_loaders/generic.py:116\u001b[0m, in \u001b[0;36mGenericLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load documents lazily. Use this when working at a large scale.\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mfor\u001b[39;00m blob \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblob_loader\u001b[39m.\u001b[39myield_blobs():  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblob_parser\u001b[39m.\u001b[39mlazy_parse(blob)\n",
      "File \u001b[0;32m~/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/langchain_community/document_loaders/parsers/audio.py:56\u001b[0m, in \u001b[0;36mOpenAIWhisperParser.lazy_parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpydub\u001b[39;00m \u001b[39mimport\u001b[39;00m AudioSegment\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpydub package not found, please install it with \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m`pip install pydub`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m     61\u001b[0m     \u001b[39m# api_key optional, defaults to `os.environ['OPENAI_API_KEY']`\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     client \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mOpenAI(api_key\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key, base_url\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url)\n",
      "\u001b[0;31mImportError\u001b[0m: pydub package not found, please install it with `pip install pydub`"
     ]
    }
   ],
   "source": [
    "url = 'https://www.youtube.com/watch?v=Px8Iu2viG7I'\n",
    "save_dir = 'arquivos_teste'\n",
    "loader = GenericLoader(\n",
    "    YoutubeAudioLoader([url], save_dir),\n",
    "    OpenAIWhisperParser()\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URL**\n",
    "\n",
    "Agora vamos ler páginas de internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.web_base import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.uol.com.br/esporte/futebol/ultimas-noticias/2025/03/31/rb-bragantino-ceara-brasileirao-serie-a-2025.htm'\n",
    "\n",
    "loader = WebBaseLoader(url)\n",
    "documentos_url = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 só pq é uma página só q eu passei\n",
    "len(documentos_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora veja novamente q o processo de interagir com \n",
    "# o arquivo é igual às anteriores\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chain_url = load_qa_chain(llm=chat, chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RB Bragantino empata com Ceará na estreia do Brasileirão Série A de 2025. Pedro Raul e Marllon marcaram para o Ceará, enquanto Laquintana e Eric Ramires fizeram os gols do Bragantino. Resultado de 2 a 2 deixou as equipes na sexta e sétima posição, respectivamente. Próximos jogos no final de semana.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pergunta = \"faça um resumo dessa notícia com até 50 palavras\"\n",
    "\n",
    "resposta = chain_url.invoke({'input_documents':documentos_url, 'question':pergunta})\n",
    "\n",
    "resposta['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitters – Dividindo texto em trechos\n",
    "\n",
    "Agora q já sabemos carregar os documentos, precisamos saber como \"quebrar\" esse documento em pedaços menores para ser melhor consumido pelo modelo.\n",
    "\n",
    "Essa é a segunda etapa do processo de RAG. As proximas etapas são embedar, guardar numa vector store e finalmente fazer o retrieval.\n",
    "\n",
    "= = = = \n",
    "\n",
    "Aqui o objetivo é dividir o documento em trechos menores para fornecer ao LLM apenas pedaços relevantes da informação e assim ele processar. Isso garante mais agilidade e precisão no resultado final.\n",
    "\n",
    "Mas como podemos manter a qualidade da informação se a dividimos em pedaços menores?\n",
    "\n",
    "Vejamos esta frase de exemplo: \"O novo carro da Fiat se chama Toro, tem 120 cavalos de potência e o preço sugerido é 135 mil reais.\"\n",
    "\n",
    "Digamos q o split dessa frase ficaria da seguinte forma:\n",
    "\n",
    "- \"O novo carro da Fiat se\"\n",
    "- \" chama Toro, tem 120 \"\n",
    "- \"cavalos de potência e \"\n",
    "- \"o preço sugerido é \"\n",
    "- \"135 mil reais.\"\n",
    "\n",
    "Aqui percebemos que os trechos separados individualmente não possuem valor. Sendo assim, seria de pouco valor para uma LLM, pois foi mal dividido.\n",
    "\n",
    "Uma das técnicas para resolver isso é usar o parametro do overlapping. Esta técnica faz com que cada chunk tenha um pedaço do chunk anterior e do próximo. Dessa forma o LLM flui pela informação podendo fazer ligações.\n",
    "\n",
    "Ficaria assim, por exemplo:\n",
    "\n",
    "- \"O novo carro da Fiat se\"\n",
    "- \"da Fiat se chama Toro, tem 120 \"\n",
    "- \"Toro, tem 120 cavalos de potência e \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esse é o texto q vamos splittar\n",
    "texto_grande = \"\"\"\n",
    "More and more organizations are turning to dashboards for monitoring performance and \\\n",
    "enabling data exploration. These user-friendly reporting tools offer a ton of \\\n",
    "advantages over older ways of doing things: they can dynamically update to display\\\n",
    "the latest information, link together multiple views of data, and often incorporate \\\n",
    "interactivity that lets users filter and zoom in on what they want to explore. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem formas diferentes de text splitter. Vamos explorar as mais utilizadas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CharacterTextSplitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o tamanho do chunk (fatias) que vc deseja dividir\n",
    "chunk_size = 50\n",
    "# o tamanho da sobreposição. Geralmente um tamanho de 10 a 20% do chunk já funciona\n",
    "chunk_overlap = 10\n",
    "\n",
    "# criando o objeto q irá fazer o split\n",
    "char_split = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator='' # o separator indica onde a divisão será feita\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos dar um exemplo de split com o abecedário\n",
    "# o abecedario está disponivel importando a lib string\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "# vamos criar nosso texto a ser dividido\n",
    "# pegamos o abecedario e o replicamos 5 vezes\n",
    "# veja q o tamanho do nosso texto ficou em 134 caracteres\n",
    "texto = '-'.join(f'{string.ascii_lowercase}' for _ in range(5))\n",
    "print(texto)\n",
    "print(len(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvw',\n",
       " 'nopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghi',\n",
       " '-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuv',\n",
       " 'mnopqrstuvwxyz']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora fazendo o split do texto criado acima\n",
    "# veja q o tamanho ficou em 4, sendo o chunksize de 50 \n",
    "# e um total de 134 caracteres\n",
    "splits = char_split.split_text(texto)\n",
    "print(len(splits))\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Veja acima como funciona o overlap. O segundo elemento da lista inicia com os 10 últimos caracteres da primeira lista, e assim sucessivamente. Ou seja, existe uma sobreposição entre o fim de um elemento da lista e inicio do próximo. A quantidade de caracteres que faz essa sobreposição é dada no chunk_overlap. E, claro, isso só não ocorre com o primeiro elemento, já q ele não tem antecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RecursiveCharacterTextSplitter**\n",
    "\n",
    "Esse é o mais utilizado, ele permite o uso de vários separadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50\n",
    "chunk_overlap = 10\n",
    "\n",
    "# criando o objeto q irá fazer o split\n",
    "char_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos aproveitar o mesmo exemplo usado anteriormente\n",
    "texto = '-'.join(f'{string.ascii_lowercase}' for _ in range(5))\n",
    "len(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvw',\n",
       " 'nopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghi',\n",
       " '-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuv',\n",
       " 'mnopqrstuvwxyz']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_split.split_text(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora utilizando o argumento separators\n",
    "\n",
    "Com este argumento o splitter cria uma ordem de prioridade para quebrar os chunks. No exemplo abaixo, primeiro ele quebra por ponto, depois por espaço vazio e depois por qualquer lugar do texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o objeto q irá fazer o split\n",
    "char_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=['.', ' ', '']\n",
    ")\n",
    "\n",
    "# geralmente apenas uns poucos marcadores de split\n",
    "# são necessários. São eles: ['\\n\\n', '\\n', ' ', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver um exemplo com um texto grande, com mais caracteres do que o abecedário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['More and more organizations are turning to',\n",
       " 'to dashboards for monitoring performance and',\n",
       " 'and enabling data exploration',\n",
       " '. These user-friendly reporting tools offer a ton',\n",
       " 'a ton of advantages over older ways of doing',\n",
       " 'of doing things: they can dynamically update to',\n",
       " 'update to displaythe latest information, link',\n",
       " 'link together multiple views of data, and often',\n",
       " 'and often incorporate interactivity that lets',\n",
       " 'that lets users filter and zoom in on what they',\n",
       " 'what they want to explore',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_split.split_text(texto_grande)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TokenTextSplitter**\n",
    "\n",
    "Permite o fatiamento em tokens em vez de quantidade de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50\n",
    "chunk_overlap = 10\n",
    "\n",
    "token_split = TokenTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijkl',\n",
       " 'uvwxyz-abcdefghijklmnopqrstuvwxyz-abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_split.split_text(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Agora veja como fica a divisão por token. O abecedário agora foi dividido em apenas duas partes. Isso porque aquela primeira parte toda já é equivalente a 50 tokens. Essa forma de split facilita nosso gerenciamento das APIs da OpenAI e outras, que contabilizam o fluxo de dados em tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MarkdownHeaderTextSplitter**\n",
    "\n",
    "Podemos fazer o split de textos em Markdown. Existem vários marcadores nesse tipo de texto que podemos indicar para o split. Dessa forma fica mais fácil identificarmos quando trata-se de um texto principal (#), subtitulo (##), e etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"# Exemplo de titulo em markdown\n",
    "## Aqui é um subtitulo\n",
    "texto q fica dentro desse subtitulo\n",
    "### Aqui outra hierarquia de header\n",
    "Mais texto livre q fica dentro desta hierarquia\n",
    "**e vamos de mais exemplos**\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca necessária\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro criamos uma lista de tuplas\n",
    "# onde terão os marcadores que farão o split\n",
    "\n",
    "header_to_split_on = [\n",
    "    ('#', 'Header 1'),\n",
    "    ('##', 'Header 2'),\n",
    "    ('###', 'Header 3'),\n",
    "]\n",
    "\n",
    "# criando o splitter\n",
    "md_split = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=header_to_split_on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo o split\n",
    "splits = md_split.split_text(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='texto q fica dentro desse subtitulo', metadata={'Header 1': 'Exemplo de titulo em markdown', 'Header 2': 'Aqui é um subtitulo'}),\n",
       " Document(page_content='Mais texto livre q fica dentro desta hierarquia\\n**e vamos de mais exemplos**', metadata={'Header 1': 'Exemplo de titulo em markdown', 'Header 2': 'Aqui é um subtitulo', 'Header 3': 'Aqui outra hierarquia de header'})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Veja como fica o output. Ele traz as hierarquias de título\\subtitulo em metadata e traz seu conteúdo na variável page_content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split de documentos**\n",
    "\n",
    "Vamos fazer o split de um documento, em vez de textos simples como fizemos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca q carregará e nos ajudará a trabalhar com doc\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# importando um splitter recursivo\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50\n",
    "chunk_overlap = 10\n",
    "\n",
    "# criando o objeto q irá fazer o split\n",
    "# até aqui é identico ao q já fizemos anteriormente\n",
    "char_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho para o arquivo\n",
    "caminho = 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carregando o arquivo\n",
    "loader = PyPDFLoader(caminho)\n",
    "docs = loader.load()\n",
    "\n",
    "# veja que o tamanho é apenas 5, q é a quantidade de páginas\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora vejamos depois de fazer o split\n",
    "# fique atento que aqui o método é diferente, sendo split_documents\n",
    "# em vez de split_text\n",
    "doc_split = char_split.split_documents(docs)\n",
    "len(doc_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings - Transformando texto em vetores\n",
    "\n",
    "Embeddings criam uma representação vetorial de um pedaço de texto. Ele transforma letras em números. Isso é útil, pois é assim que o modelo vai \"pensar\" de forma mais eficiente. Dessa forma ele pode buscar proximidades semanticas, etc. Os cálculos vetoriais é quem dão a \"consciencia\" e \"discernimento\" para o modelo.\n",
    "\n",
    "O Langchain fornece uma classe de embedding que interage com os modelos de embedding fornecidos no mercado, ou seja, existem ebeddings da OpenAI, HuggingFace, Claude etc. O Langchain fornece uma interface padrão para todas elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o modelo default para embedding é esse q está no argumento da chamada\n",
    "# existem outros. Visite o site da OpenAI para detalhes\n",
    "# vamos instanciar esse embedding da OpenAI\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding de Documentos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos criar nosso modelo \n",
    "# veja que ele possui uma lista, onde ficarão os documentos\n",
    "# inserimos 3 documentos, q no momento são apenas frases\n",
    "embeddings = embedding_model.embed_documents(\n",
    "    [\n",
    "        'Eu amo frutas!',\n",
    "        'Adoro comer maçã e morando no café da manhã',\n",
    "        'Eu não gostei dessa piada.'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temos 3 documentos\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.006634879172388902,\n",
       " -0.01640579910410335,\n",
       " 0.008534951452491854,\n",
       " -0.018041460513549958,\n",
       " 0.00036875386927730914,\n",
       " 0.012144472901045993,\n",
       " 0.004282848602894743,\n",
       " -0.01683623592497978,\n",
       " 0.02096843238562586,\n",
       " -0.018189037894204337]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos como ficou vetorizada a primeira frase\n",
    "# aqui vemos os 10 primeiros vetores\n",
    "embeddings[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536 0.22313854927024132 -0.6505255940753462\n",
      "1536 0.24039689057922564 -0.6608969665847052\n",
      "1536 0.22472701344464535 -0.6345005455745475\n"
     ]
    }
   ],
   "source": [
    "# vamos fazer um for e analisar cada vetor\n",
    "# primeiro vemos q o tamanho dos vetores é o mesmo (1536)\n",
    "# isso acontece por causa do modelo escolhido da OpenAI, na hora de instanciar\n",
    "# depois pegamos o valor máximo e o mínimo de cada lista de vetores\n",
    "for emb in embeddings:\n",
    "    print(len(emb), max(emb), min(emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos fazer uma multiplicação vetorial utilizando o numpy\n",
    "# para identificarmos as semelhanças semanticas entre os vetores\n",
    "# é através desses cálculo que o modelo 'pensa' e sabe das semelhanças\n",
    "# entre as frases. Quanto mais próximo de 1, mais semelhantes são as frases\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8841020582306888"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiplicando a primeira e a segunda frase\n",
    "# veja o número que obtemos\n",
    "# quanto mais alto significa que as frases tem semânticas próximas\n",
    "np.dot(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7906537384786307"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora veja a comparação da frase 1 com a frase 3, o número é menor\n",
    "np.dot(embeddings[0], embeddings[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7688946865216448"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mesma coisa entre as frases 2 e 3\n",
    "np.dot(embeddings[1], embeddings[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 | 0.884 | 0.791 | \n",
      "0.884 | 1.0 | 0.769 | \n",
      "0.791 | 0.769 | 1.0 | \n"
     ]
    }
   ],
   "source": [
    "# vamos ver o conjunto todo, multiplicando todos vetores entre eles\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(len(embeddings)):\n",
    "        print(round(np.dot(embeddings[i], embeddings[j]), 3), end=' | ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding Query**\n",
    "\n",
    "Precisamos tbm fazer o embedding da pergunta do usuário ao modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.011823595855325887,\n",
       " -0.0015175153225819675,\n",
       " 0.003353206044200978,\n",
       " -0.004639096331514772,\n",
       " 0.003132296805952957,\n",
       " 0.004395107058349887,\n",
       " -0.00871108243536731,\n",
       " -0.0341848975114945,\n",
       " -0.017171579505757185,\n",
       " 0.0036796243461127056]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora vamos utilizar o método emb_query() do modelo instanciado\n",
    "pergunta = \"quais os alimentos mais saudáveis\"\n",
    "emb_query = embedding_model.embed_query(pergunta)\n",
    "emb_query[:10] # vendo os 10 primeiros vetores dessa pergunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125198875109063"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora comparando com as frases criadas antes\n",
    "# veja como a pergunta é próxima das frase 1 por exemplo\n",
    "np.dot(emb_query, embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7236170898533685"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e veja com ela é mais distante da frase 3\n",
    "np.dot(emb_query, embeddings[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding com Huggingface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaio-peixoto/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/kaio-peixoto/GitHub/asimov/aplicacoes-langchain/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = 'all-MiniLM-L6-v2'\n",
    "emb_model = HuggingFaceBgeEmbeddings(model_name=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = emb_model.embed_documents(\n",
    "    [\n",
    "    'Eu gosto de manter uma alimentação saudável.',\n",
    "    'Adoro comer maçã e morango no café da manhã.',\n",
    "    'A crise financeira de 2008 abalou o mundo.']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 | 0.432 | 0.419 | \n",
      "0.432 | 1.0 | 0.313 | \n",
      "0.419 | 0.313 | 1.0 | \n"
     ]
    }
   ],
   "source": [
    "# vamos ver o conjunto todo, multiplicando todos vetores entre eles\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(len(embeddings)):\n",
    "        print(round(np.dot(embeddings[i], embeddings[j]), 3), end=' | ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorStores\n",
    "\n",
    "Uma VectorStore faz o armazenamento de vetores e realiza a busca desses vetores. Depois de fazermos o embedding, como explicado na aula anterior, temos que guardá-los num lugar otimizado para trabalharmos com os vetores.\n",
    "\n",
    "Temos várias soluções de VectorStores e aqui vamos estudar duas das mais utilizadas: Chroma e FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chroma VectorStore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos fazer o processo desde o início\n",
    "# ou seja, vamos carregar os documentos, splittar, fazer o embedding\n",
    "# e guardar na VectorStore para retrieval\n",
    "\n",
    "# importando as libs para carregar os docs\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando os documentos\n",
    "caminho = 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'\n",
    "loader = PyPDFLoader(caminho)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos para o text splitting\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=30,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = recur_split.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a VectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diretorio onde a base de dados será salva\n",
    "diretorio = \"files/chroma_vectorstore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a vector store\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=diretorio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "# aqui a gente vê quandos documentos existem dentro da vector store\n",
    "print(vector_store._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui agora vamos carregar uma vector store já existente\n",
    "# veja q não utilizamos o método .from_documents()\n",
    "# utilizamos somente a classe Chroma\n",
    "# fornecemos apenas o modelo de embedding e o diretório\n",
    "# onde a vector store está localizada\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=diretorio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval**\n",
    "\n",
    "Fazendo perguntas para os documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aqui vamos ver como o vector store procura pela similidaridade\n",
    "# vamos fazer uma pergunta e ver quais trechos do documento ele retorna\n",
    "pergunta = 'qual é o valor do aluguel?'\n",
    "\n",
    "# o argumento k é a quantidade de documentos q ele vai pesquisar\n",
    "docs = vector_store.similarity_search(pergunta, k=5)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagará o LOCATÁRIO ao LOCADOR o valor mensal de R$ 350,00, a título de aluguel. Os\n",
      "aluguéis serão pagos todo dia 05 de cada mês, efetivando-se por depósito bancário no\n",
      "Banco do Brasil, Agência 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'page': 0, 'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'} \n",
      "\n",
      "Pagará o LOCATÁRIO ao LOCADOR o valor mensal de R$ 350,00, a título de aluguel. Os\n",
      "aluguéis serão pagos todo dia 05 de cada mês, efetivando-se por depósito bancário no\n",
      "Banco do Brasil, Agência 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'page': 0, 'source': 'contrato_aluguel_jose_nilton.pdf'} \n",
      "\n",
      "Pagará o LOCATÁRIO ao LOCADOR o valor mensal de R$ 350,00, a título de aluguel. Os\n",
      "aluguéis serão pagos todo dia 05 de cada mês, efetivando-se por depósito bancário no\n",
      "Banco do Brasil, Agência 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'page': 0, 'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'} \n",
      "\n",
      "Pagará o LOCATÁRIO ao LOCADOR o valor mensal de R$ 350,00, a título de aluguel. Os\n",
      "aluguéis serão pagos todo dia 05 de cada mês, efetivando-se por depósito bancário no\n",
      "Banco do Brasil, Agência 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'page': 0, 'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'} \n",
      "\n",
      "Pagará o LOCATÁRIO ao LOCADOR o valor mensal de R$ 350,00, a título de aluguel. Os\n",
      "aluguéis serão pagos todo dia 05 de cada mês, efetivando-se por depósito bancário no\n",
      "Banco do Brasil, Agência 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'page': 0, 'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# e agora veja o conteúdo de cada documento\n",
    "# perguntamos sobre valor do aluguel\n",
    "# e ele buscou os trechos q estão mais relacionados à pergunta\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=====\", doc.metadata, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FAISS VectorStore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando os documentos\n",
    "caminho = 'arquivos_teste/contrato_aluguel_jose_nilton.pdf'\n",
    "loader = PyPDFLoader(caminho)\n",
    "docs = loader.load()\n",
    "\n",
    "# agora vamos para o text splitting\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=30,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = recur_split.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um modelo de embedding\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a vector store com Faiss\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "# agora vamos criar a vectorstore com FAISS\n",
    "# é bem parecido com o Chroma, mas sem o diretorio\n",
    "# FAISS não usa 'persist_directory' porque não tem persistência nativa \n",
    "vector_store_faiss = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora fazendo o retrieval com essa nova base FAISS\n",
    "# aqui vamos ver como o vector store procura pela similidaridade\n",
    "# vamos fazer uma pergunta e ver quais trechos do documento ele retorna\n",
    "pergunta = 'qual é o valor do aluguel?'\n",
    "\n",
    "# o argumento k é a quantidade de documentos q ele vai pesquisar\n",
    "docs = vector_store_faiss.similarity_search(pergunta, k=5)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagará o LOCATÁRIO ao LOCADOR o valor mensal de R$ 350,00, a título de aluguel. Os\n",
      "aluguéis serão pagos todo dia 05 de cada mês, efetivando-se por depósito bancário no\n",
      "Banco do Brasil, Agência 3028-7, Conta Corrente 31872-8, em nome de Kaio Oliveira\n",
      "===== {'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 0} \n",
      "\n",
      "O  atraso  no  pagamento  do  aluguel  e/ou  dos  encargos  da  locação  implicará  a\n",
      "incidência de multa de 2,00% (dois por cento) sobre o valor total de débito e juros de\n",
      "1,00% (um por cento) ao mês.\n",
      "CLÁUSULA SÉTIMA: USO E DESTINAÇÃO DA LOCAÇÃO\n",
      "===== {'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 1} \n",
      "\n",
      "Peixoto.\n",
      "O aluguel pactuado acima sofrerá reajustes anuais com base na variação do Índice\n",
      "Geral de Preços divulgado pela Fundação Getúlio Vargas (IGP-FGV) ou outro índice que\n",
      "porventura venha a substituí-lo.\n",
      "===== {'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 0} \n",
      "\n",
      "(três)  vezes  o  valor  do  aluguel  vigente  à  época  da  infração,  sem  prejuízo  de\n",
      "indenização suplementar.\n",
      "CLÁUSULA NONA: ENTREGA E DEVOLUÇÃO DO IMÓVEL\n",
      "O LOCADOR entrega o imóvel ao LOCATÁRIO contendo as características relacionadas\n",
      "===== {'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 2} \n",
      "\n",
      "fazendo jus, em contrapartida, ao pagamento pelo LOCATÁRIO dos valores de aluguel\n",
      "e encargos. A casa é de domínio, propriedade e posse indireta do LOCADOR.\n",
      "CLÁUSULA TERCEIRA: PRAZO DE LOCAÇÃO\n",
      "===== {'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# e agora veja o conteúdo de cada documento\n",
    "# perguntamos sobre valor do aluguel\n",
    "# e ele buscou os trechos q estão mais relacionados à pergunta\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=====\", doc.metadata, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salvando o bd do FAISS**\n",
    "\n",
    "Observação importante\n",
    "\n",
    "- Chroma: sempre precisa apontar para uma pasta (`persist_directory`) se você quiser que ele salve os dados no disco. Sem isso, ele funciona só na memória durante a execução, mas para manter o banco de dados entre execuções, é necessário definir a pasta.\n",
    "\n",
    "- FAISS: não precisa apontar pasta na criação, porque por padrão ele trabalha só na memória RAM. Mas se quiser salvar o banco e reutilizar depois, você precisa usar `.save_local()` e `.load_local()` manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando...\n",
    "vector_store_faiss.save_local('arquivos_teste/faiss_bd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o bd faiss\n",
    "# o argumento dangerous deserialization é usado \n",
    "# como True quando vc confia nos dados de origem,\n",
    "# pois ele pode deserializar código malicioso\n",
    "# já q eu criei o banco e sei a origem, então deixo como True\n",
    "vector_store_faiss = FAISS.load_local(\n",
    "    'arquivos_teste/faiss_bd',\n",
    "    embeddings=embeddings_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧩 **Resumo Vector Store: Chroma vs FAISS**\n",
    "\n",
    "**Essência em 1 parágrafo:**\n",
    "Tanto Chroma quanto FAISS são vector stores que armazenam e fazem buscas por embeddings, mas a principal diferença está na facilidade de uso e integração. Chroma é pensado para ser plug-and-play, com armazenamento local por padrão e até persistência nativa, ótimo para protótipos e projetos de médio porte. FAISS é uma biblioteca da Meta, mais \"pura\", focada em alta performance e escala, muito usada para produção em grandes volumes de dados, mas exige mais configuração e não tem persistência embutida (você salva manualmente).\n",
    "\n",
    "**Principais pontos:**\n",
    "\n",
    "| Característica                  | Chroma                                | FAISS                                   |\n",
    "|----------------------------------|----------------------------------------|------------------------------------------|\n",
    "| 📦 Instalação e uso              | Super fácil, abstraído, integrado      | Mais manual, requer mais configuração    |\n",
    "| 💾 Persistência (salvar dados)    | Sim, nativa (`persist_directory`)      | Não nativa, precisa serializar manualmente |\n",
    "| 🚀 Performance                   | Boa para médias bases                 | Muito rápido e otimizado para grandes bases |\n",
    "| 🔗 Integração com LangChain      | Muito fluida, oficial                 | Também integra bem, mas demanda mais setup |\n",
    "| 🔥 Escalabilidade                | Limitada, para protótipos/projetos médios | Alta escala, ideal para milhões de vetores |\n",
    "| 🌐 Multi-idioma / Multi-device   | Suporte mais limitado                 | Suporte para CPU/GPU e multi-threading avançado |\n",
    "\n",
    "**Observação importante sobre FAISS:**\n",
    "\n",
    "> FAISS não usa 'persist_directory' porque não tem persistência nativa; ele guarda os dados na memória durante a execução, e para manter os dados depois de fechar o programa, é preciso salvar e carregar manualmente o índice.\n",
    "\n",
    "**Quando usar cada um:**\n",
    "\n",
    "- **Use Chroma se:**\n",
    "  - Quer prototipar rápido.\n",
    "  - Seu projeto tem até algumas dezenas ou centenas de milhares de vetores.\n",
    "  - Você quer facilidade de persistência local.\n",
    "\n",
    "- **Use FAISS se:**\n",
    "  - Precisa de alta performance em bases grandes (milhões de vetores).\n",
    "  - Vai rodar em produção com alta demanda.\n",
    "  - Está confortável com setups mais avançados e salvar/restaurar índices manualmente.\n",
    "\n",
    "---\n",
    "\n",
    "🧭 **Outros Vector Stores para o dia a dia**\n",
    "\n",
    "**Essência em 1 parágrafo:**\n",
    "Além de Chroma e FAISS, outros vector stores aparecem bastante na prática, principalmente quando se busca escalabilidade profissional, integração com nuvem ou recursos avançados de busca híbrida (texto + vetores). Pinecone, Weaviate, Milvus e Redis Vector são ótimas opções dependendo do cenário.\n",
    "\n",
    "**Principais opções:**\n",
    "\n",
    "- **Pinecone:**  \n",
    "  - SaaS dedicado e fácil de usar.\n",
    "  - Escala automática, alta disponibilidade.\n",
    "  - Integra bem com LangChain.\n",
    "  - Ideal para produção sem dor de cabeça com infraestrutura.\n",
    "\n",
    "- **Weaviate:**  \n",
    "  - Open-source com busca híbrida (texto + vetores).\n",
    "  - Permite filtros avançados por metadados.\n",
    "  - Escalável e pronto para produção.\n",
    "\n",
    "- **Milvus:**  \n",
    "  - Open-source, alto desempenho para bilhões de vetores.\n",
    "  - Suporte forte para GPU.\n",
    "  - Indicado para Big Data e aplicações massivas.\n",
    "\n",
    "- **Redis Vector:**  \n",
    "  - Usa Redis para armazenar vetores.\n",
    "  - Boa opção se você já usa Redis na stack.\n",
    "  - Performance sólida e suporte a filtros por metadados.\n",
    "\n",
    "**Resumo de escolha:**\n",
    "\n",
    "- **Protótipo/local:** Chroma\n",
    "- **Projeto pequeno, on-premise:** FAISS\n",
    "- **Produção escalável:** Pinecone ou Weaviate\n",
    "- **Big Data + GPU:** Milvus\n",
    "- **Stack Redis:** Redis Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval – Encontrando trechos relevantes\n",
    "\n",
    "É a busca por trechos semelhantes à pergunta que o usuário fez.\n",
    "\n",
    "Já fizemos isso acima na aula de vector sotres, e agora vamos ver de forma mais detalhada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vamos fazer um loading com 3 arquivos. Faremos um laço for para carregar os 3 arquivos pdf.\n",
    "\n",
    "Mas veja que um desses arquivos está duplicado. Isso foi de forma intencional para vermos como os diferentes tipos de retrieval tratam um mesmo documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminhos = [\n",
    "    'arquivos_teste/contrato_aluguel_jose_nilton.pdf',\n",
    "    'arquivos_teste/vantagem_pdf.pdf',\n",
    "    'arquivos_teste/vantagem_pdf.pdf'\n",
    "]\n",
    "\n",
    "paginas = []\n",
    "\n",
    "for doc in caminhos:\n",
    "    loader = PyPDFLoader(doc)\n",
    "    paginas.extend(loader.load()) # adicionando cada arquivo carregado na lista\n",
    "\n",
    "# criando o splitter para dividir os documentos\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# agora splitando os documentos\n",
    "documents = recur_split.split_documents(paginas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modificando metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'arquivos_teste/contrato_aluguel_jose_nilton.pdf', 'page': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sabemos que todo documento traz metadados\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mas esses dados também podem ser editados\n",
    "# e podemos também adicionar novos metadados\n",
    "# vamos dar aqui um exemplo e modificar algumas coisas\n",
    "# eu quero por exemplo tirar o nome arquivos_teste do nome do arquivo\n",
    "# e adicionar um novo atributo, q é o id do documento, q terá o valor do índice\n",
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata['source'] = doc.metadata['source'].replace('arquivos_teste/', '')\n",
    "    doc.metadata['doc_id'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'contrato_aluguel_jose_nilton.pdf', 'page': 0, 'doc_id': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora verificando depois da modificação\n",
    "# veja como ficaram os novos atributos\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando a VectorStore**\n",
    "\n",
    "Vamos criar essa base de dados para interagir com os documentos q splitamos acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro criamos o embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora criando um banco com o chroma\n",
    "diretorio = \"arquivos_teste/chroma_retrieval\"\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=diretorio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic Search**\n",
    "\n",
    "Esse é o tipo de retrieval mais simples que existe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vantagens do PDF  \n",
      " \n",
      " Pequenho tamanho do arquivo: os arquivos possuem uma compactação \n",
      "aceitável (ex.: Arquivos  de Word com 1Mb após a conversão para PDF \n",
      "chegam a ficar com 100 Kb de tamanho, 10% do  original);  \n",
      " Os arquiv os PDF são compactos e totalmente pesquisáveis, podendo ser \n",
      "acessados a qualquer momento com o Adobe Reader.  \n",
      " Não apresentam problemas de fontes e/ou formatação de arquivos;  \n",
      " Os documentos PDF podem ter direitos especiais de acesso e podem ser\n",
      "====={'doc_id': 25, 'page': 1, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n",
      "Vantagens do PDF  \n",
      " \n",
      " Pequenho tamanho do arquivo: os arquivos possuem uma compactação \n",
      "aceitável (ex.: Arquivos  de Word com 1Mb após a conversão para PDF \n",
      "chegam a ficar com 100 Kb de tamanho, 10% do  original);  \n",
      " Os arquiv os PDF são compactos e totalmente pesquisáveis, podendo ser \n",
      "acessados a qualquer momento com o Adobe Reader.  \n",
      " Não apresentam problemas de fontes e/ou formatação de arquivos;  \n",
      " Os documentos PDF podem ter direitos especiais de acesso e podem ser\n",
      "====={'doc_id': 25, 'page': 1, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n",
      "Vantagens do PDF  \n",
      " \n",
      " Pequenho tamanho do arquivo: os arquivos possuem uma compactação \n",
      "aceitável (ex.: Arquivos  de Word com 1Mb após a conversão para PDF \n",
      "chegam a ficar com 100 Kb de tamanho, 10% do  original);  \n",
      " Os arquiv os PDF são compactos e totalmente pesquisáveis, podendo ser \n",
      "acessados a qualquer momento com o Adobe Reader.  \n",
      " Não apresentam problemas de fontes e/ou formatação de arquivos;  \n",
      " Os documentos PDF podem ter direitos especiais de acesso e podem ser\n",
      "====={'doc_id': 32, 'page': 1, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"quais são as vantagens de usar um arquivo pdf?\"\n",
    "\n",
    "# vamos pedir para ele trazer os 3 trechos mais relevantes\n",
    "docs = vector_db.similarity_search(pergunta, k=3)\n",
    "\n",
    "# agora vamos ver quais foram esses trechos q ele encontrou\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"====={doc.metadata}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que eu dupliquei o arquivo sobre pdf de propósito. Note que o modelo retornou 2 trechos repetidos, q são os 2 primeiros. Veja pelo doc_id que tratam-se de dois documentos diferentes, mas com conteúdo igual.\n",
    "\n",
    "O modelo não conseguiu avaliar que o segundo trecho era exatamente igual ao primeiro e não adiciona nenhuma informação nova. Essa é uma das limitações desse tipo de retrieval.\n",
    "\n",
    "Ele não leva em conta o metadado e outros parametros, tornando a busca menos eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MMR (Max Marginal Relevance)**\n",
    "\n",
    "- Traz resultados diversos, **evita repetição** entre documentos.\n",
    "- Combina **relevância** com **diversidade** dos documentos retornados.\n",
    "- Útil para não receber vários trechos muito parecidos na resposta.\n",
    "\n",
    "Principais argumentos:\n",
    "- **query:** sua pergunta em texto.\n",
    "- **k:** total de documentos que você quer receber.\n",
    "- **fetch_k:** número de documentos candidatos que ele vai considerar antes de escolher os melhores (default: maior que `k`).\n",
    "- **lambda_mult:** equilíbrio entre relevância e diversidade (de 0 a 1).\n",
    "  - 1 = mais foco na relevância.\n",
    "  - 0 = mais foco na diversidade.\n",
    "\n",
    "Vamos aqui fazer um teste fazendo a mesma pergunta q fizemos no similarity search.\n",
    "\n",
    "Mas note como ele retorna uma resposta melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vantagens do PDF  \n",
      " \n",
      " Pequenho tamanho do arquivo: os arquivos possuem uma compactação \n",
      "aceitável (ex.: Arquivos  de Word com 1Mb após a conversão para PDF \n",
      "chegam a ficar com 100 Kb de tamanho, 10% do  original);  \n",
      " Os arquiv os PDF são compactos e totalmente pesquisáveis, podendo ser \n",
      "acessados a qualquer momento com o Adobe Reader.  \n",
      " Não apresentam problemas de fontes e/ou formatação de arquivos;  \n",
      " Os documentos PDF podem ter direitos especiais de acesso e podem ser\n",
      "====={'doc_id': 25, 'page': 1, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n",
      "O que é Adobe PDF?  \n",
      " \n",
      "O PDF (Portable Document Format) é um formato de arquivo  \n",
      "desenvolvido pela Adobe Systems para representar  documentos de maneira \n",
      "independente do aplicativo,  hardware, e sistema operacional usados para \n",
      "criá-los. Um  arquivo PDF pode descrever documentos que contenham  texto, \n",
      "gráficos e imagens num formato independente de  dispositivo e resolução.  \n",
      "Sua principal característica c onsiste em representar um documento\n",
      "====={'doc_id': 21, 'page': 0, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n",
      "aplicativos possuem um ícone na barra de ferramenta para conversão de \n",
      "arquivos para PDF.  \n",
      " \n",
      " \n",
      "Como funciona a tecnologia PDF  \n",
      " \n",
      "O PDF é um tipo de arquivo que representa na tela do computador \n",
      "páginas de documentos  eletrô nicos. É possível converter para PDF vários tipos \n",
      "de arquivos, desde os baseados em texto até  documentos como tabelas, \n",
      "gráficos, image ns, etc. Para isso, o PDF gera arquivos usando os  princípios\n",
      "====={'doc_id': 30, 'page': 0, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"quais são as vantagens de usar um arquivo pdf?\"\n",
    "\n",
    "# vamos pedir para ele trazer os 10 trechos mais similares com fetch_k\n",
    "# e no argumento k ele vai trazer os 3 mais relevantes\n",
    "docs = vector_db.max_marginal_relevance_search(pergunta, k=3, fetch_k=10)\n",
    "\n",
    "# agora vamos ver quais foram esses trechos q ele encontrou\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"====={doc.metadata}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎯 Relação entre fetch_k e k\n",
    "\n",
    "- **fetch_k** é o número de documentos candidatos que o método vai considerar primeiro.\n",
    "- **k** é o número final de documentos que você realmente quer como resposta.\n",
    "\n",
    "🔎 **Funcionamento:**\n",
    "1. Primeiro, ele pega os **fetch_k** documentos mais relevantes.\n",
    "2. Depois, ele filtra entre esses candidatos para escolher os **k** resultados finais, equilibrando **relevância e diversidade** (controlado pelo `lambda_mult`).\n",
    "\n",
    "Exemplo:\n",
    "```python\n",
    "k = 5          # Quero 5 documentos no final\n",
    "fetch_k = 20   # Ele vai considerar 20 candidatos antes de escolher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtragem**\n",
    "\n",
    "O filter é um argumento dentro do similarity_search que limita os resultados da busca aos documentos que correspondem aos metadados especificados.\n",
    "\n",
    "- O **filter** permite **filtrar** os documentos recuperados com base nos metadados que você salvou junto com os documentos.\n",
    "- Serve para refinar a busca, trazendo só documentos que atendam a certos critérios.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "Se seus documentos têm metadado `{\"categoria\": \"contrato\"}`, você pode buscar só nessa categoria:\n",
    "```python\n",
    "docs = vector_store.similarity_search(\n",
    "    query=pergunta,\n",
    "    k=5,\n",
    "    filter={\"categoria\": \"contrato\"}\n",
    ")\n",
    "\n",
    "Vamos ver abaixo com o nosso exemplo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O que é Adobe PDF?  \n",
      " \n",
      "O PDF (Portable Document Format) é um formato de arquivo  \n",
      "desenvolvido pela Adobe Systems para representar  documentos de maneira \n",
      "independente do aplicativo,  hardware, e sistema operacional usados para \n",
      "criá-los. Um  arquivo PDF pode descrever documentos que contenham  texto, \n",
      "gráficos e imagens num formato independente de  dispositivo e resolução.  \n",
      "Sua principal característica c onsiste em representar um documento\n",
      "====={'doc_id': 21, 'page': 0, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n",
      "O que é Adobe PDF?  \n",
      " \n",
      "O PDF (Portable Document Format) é um formato de arquivo  \n",
      "desenvolvido pela Adobe Systems para representar  documentos de maneira \n",
      "independente do aplicativo,  hardware, e sistema operacional usados para \n",
      "criá-los. Um  arquivo PDF pode descrever documentos que contenham  texto, \n",
      "gráficos e imagens num formato independente de  dispositivo e resolução.  \n",
      "Sua principal característica c onsiste em representar um documento\n",
      "====={'doc_id': 21, 'page': 0, 'source': 'vantagem_pdf.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"quais são as vantagens de usar um arquivo pdf?\"\n",
    "\n",
    "docs = vector_db.similarity_search(\n",
    "    pergunta, \n",
    "    k=3,\n",
    "    filter={'doc_id':21})\n",
    "\n",
    "# agora veja q ele só buscou dentro do doc_id = 0\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"====={doc.metadata}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM Aided Retrieval**\n",
    "\n",
    "- LLM-Aided Retrieval usa o próprio modelo de linguagem (LLM) para **ajudar na recuperação de documentos**, melhorando a qualidade das buscas.\n",
    "- Em vez de só usar vetores para buscar, o LLM pode:\n",
    "  - Reescrever ou expandir a pergunta antes de buscar.\n",
    "  - Filtrar ou rankear melhor os resultados recuperados.\n",
    "  - Fazer busca multi-turno (exemplo: perguntar ao LLM como refinar a consulta).\n",
    "\n",
    "Por que usar?\n",
    "- Ajuda quando a pergunta do usuário não é tão clara.\n",
    "- Aumenta a precisão em buscas mais complexas ou ambíguas.\n",
    "\n",
    "Resumindo, em vez de você escrever manualmente filtros ou ajustes na busca, o modelo \"pensa por você\" e traduz a pergunta em uma query estruturada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos importar as libs q precisamos\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'contrato_aluguel_jose_nilton.pdf', 'page': 0, 'doc_id': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui vamos apresentar para o modelo em quais atributos\n",
    "# ele deve prestar atenção e uma breve descrição de cada um\n",
    "metadata_info = [\n",
    "    AttributeInfo(\n",
    "        name='source',\n",
    "        description='é o nome da apostila, onde o texto original foi retirado. \\\n",
    "            Deve ter o valor de: contrato_aluguel_jose_nilton.pdf',\n",
    "            type='string'\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name='page',\n",
    "        description='a página da apostila de onde o texto se origina.',\n",
    "            type='integer'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fornecendo a descrição do documento como um todo\n",
    "content_description = 'apostila de cursos'\n",
    "llm = OpenAI()\n",
    "\n",
    "# e aqui criamos o nosso aided retriever\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vector_db,\n",
    "    content_description,\n",
    "    metadata_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As  partes  elegem  o  foro  da  comarca  de  Ibicaraí/BA ,  para  dirimir  quaisquer\n",
      "controvérsias oriundas do presente instrumento.\n",
      "As partes declaram que tiveram acesso a este contrato, leram, compreenderam e que\n",
      "estão de acordo com todas as suas cláusulas, prometendo ainda a cumprirem e\n",
      "fazerem que se cumpra o presente Contrato de Locação.\n",
      "Por estarem assim justos e contratados, firmam o presente instrumento, em duas vias\n",
      "de igual teor.\n",
      "Ibicaraí, 01 de Agosto de 2023.\n",
      "====={'doc_id': 17, 'page': 3, 'source': 'contrato_aluguel_jose_nilton.pdf'}\n",
      "\n",
      "\n",
      "As  partes  elegem  o  foro  da  comarca  de  Ibicaraí/BA ,  para  dirimir  quaisquer\n",
      "controvérsias oriundas do presente instrumento.\n",
      "As partes declaram que tiveram acesso a este contrato, leram, compreenderam e que\n",
      "estão de acordo com todas as suas cláusulas, prometendo ainda a cumprirem e\n",
      "fazerem que se cumpra o presente Contrato de Locação.\n",
      "Por estarem assim justos e contratados, firmam o presente instrumento, em duas vias\n",
      "de igual teor.\n",
      "Ibicaraí, 01 de Agosto de 2023.\n",
      "====={'doc_id': 17, 'page': 3, 'source': 'contrato_aluguel_jose_nilton.pdf'}\n",
      "\n",
      "\n",
      "de igual teor.\n",
      "Ibicaraí, 01 de Agosto de 2023.\n",
      "______________________________________\n",
      "Kaio Oliveira Peixoto\n",
      "LOCADOR\n",
      "__________________________________________\n",
      "JOSE NILTON PEREIRA SILVA\n",
      "LOCATÁRIO\n",
      "====={'doc_id': 18, 'page': 3, 'source': 'contrato_aluguel_jose_nilton.pdf'}\n",
      "\n",
      "\n",
      "de igual teor.\n",
      "Ibicaraí, 01 de Agosto de 2023.\n",
      "______________________________________\n",
      "Kaio Oliveira Peixoto\n",
      "LOCADOR\n",
      "__________________________________________\n",
      "JOSE NILTON PEREIRA SILVA\n",
      "LOCATÁRIO\n",
      "====={'doc_id': 18, 'page': 3, 'source': 'contrato_aluguel_jose_nilton.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"qual a data de assinatura do contrato?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(pergunta)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"====={doc.metadata}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG – Conversando com os seus dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro vamos fazer todo o processo do zero\n",
    "# importando as bibliotecas\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o passo a passo\n",
    "\n",
    "# indicando o caminho dos meus arquivos\n",
    "caminhos = [\n",
    "    'arquivos_teste/contrato_aluguel_jose_nilton.pdf',\n",
    "    'arquivos_teste/vantagem_pdf.pdf',\n",
    "]\n",
    "\n",
    "# fazendo o load das páginas usando o pdf loader\n",
    "paginas = []\n",
    "for caminho in caminhos:\n",
    "    loader = PyPDFLoader(caminho)\n",
    "    paginas.extend(loader.load())\n",
    "\n",
    "# dividindo os documentos para ele ser melhor consumido pelo modelo\n",
    "# criando o splitter\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"\", \" \"]\n",
    ")\n",
    "\n",
    "# aqui a gente executa o split\n",
    "documents = recur_split.split_documents(paginas)\n",
    "\n",
    "# modificando alguns metadados para facilitar a interação (opcional)\n",
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata['source'] = doc.metadata['source'].replace('arquivos_teste/', '')\n",
    "    doc.metadata['doc_id'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 13:12:26,805 - INFO - Backing off send_request(...) for 1.0s (requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Read timed out. (read timeout=15))\n"
     ]
    }
   ],
   "source": [
    "# criando a vector db\n",
    "\n",
    "# indicando onde ficará o banco de dados\n",
    "diretorio_db = \"arquivos_teste/chroma_retrieval_db\"\n",
    "\n",
    "# criando o embedder\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# juntando tudo e criando o vector db com o Chroma\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=diretorio_db\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando a Estrutura da Conversa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para criar a estrutura de chat precisamos instanciar o chat open ai\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora precisamos criar uma chain para unir e interagir com toda essa estrutura\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vector_db.as_retriever(search_type='mmr'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'quem é o autor do artigo q fala sobre pdf?',\n",
       " 'result': 'O autor do artigo sobre PDF é a Secretaria de Tecnologia da Informação do TRT da 4ª Região.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora vamos interagir!\n",
    "pergunta = \"quem é o autor do artigo q fala sobre pdf?\"\n",
    "\n",
    "chat_chain.invoke({'query': pergunta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modificando o prompt da chain**\n",
    "\n",
    "Já interagimos com o modelo, agora podemos moldá-lo para que ele siga um comportamento específico ou faça as coisas da maneira que queremos.\n",
    "\n",
    "Aqui entra a engenharia de prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ao criar o prompt precismos usar as palavras chave context e question\n",
    "# o context vai armazenar todos os documentos q fornecemos ao modelo\n",
    "# question representa a pergunta\n",
    "chain_prompt = PromptTemplate.from_template(\n",
    "\"\"\"Utilize o contexto fornecido para responder a pergunta ao final.\n",
    "Fale em tom de humor e descontração, pode manter a informalidade.\n",
    "Se você não sabe a resposta, apenas diga que não sabe e não tente inventar resposta.\n",
    "Utilize três frases no máximo, mantenha a resposta concisa.\n",
    "\n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Pergunta: {question}\n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora depois de criado o prompt\n",
    "# precisamos criar a chain novamente para incluí-lo\n",
    "# veja q usamos o argumento chain type kwargs\n",
    "# e incluimos o novo prompt num dicionário\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vector_db.as_retriever(search_type='mmr'),\n",
    "    chain_type_kwargs={'prompt':chain_prompt},\n",
    "    return_source_documents=True \n",
    ")\n",
    "\n",
    "# o argumento return_source_documents retorna as fontes pesquisadas\n",
    "# usamos aqui só para 'provar' q ele está usando o novo prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos ver como ele se comporta depois do prompt\n",
    "# vamos guardar o retorno na variável 'resposta'\n",
    "# esse retorno é um dicionário\n",
    "pergunta = \"o q é adobe pdf?\"\n",
    "\n",
    "resposta = chat_chain.invoke({'query': pergunta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Adobe PDF é tipo o super-herói dos arquivos, salva você de qualquer confusão com formatos diferentes.  \n",
      "É tipo um uniforme que faz qualquer documento ficar bonitão e acessível em qualquer dispositivo.  \n",
      "Pra resumir, é tipo uma varinha mágica que transforma seus arquivos em algo indestrutível e prontos pra qualquer situação.\n"
     ]
    }
   ],
   "source": [
    "# vamos ver a chave result, q é a resposta do modelo\n",
    "# veja q ele retornou a resposta em 3 frases, conforme pedimos no prompt\n",
    "# veja q ele também mudou o tom para um pouco de humor\n",
    "print(resposta['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='O que é Adobe PDF?  \\n \\nO PDF (Portable Document Format) é um formato de arquivo  \\ndesenvolvido pela Adobe Systems para representar  documentos de maneira \\nindependente do aplicativo,  hardware, e sistema operacional usados para \\ncriá-los. Um  arquivo PDF pode descrever documentos que contenham  texto, \\ngráficos e imagens num formato independente de  dispositivo e resolução.  \\nSua principal característica c onsiste em representar um documento', metadata={'doc_id': 28, 'page': 0, 'source': 'vantagem_pdf.pdf'}),\n",
       " Document(page_content='essenciais da tecnologia PostScript, que é uma espécie de linguagem usada \\npara  const ruir páginas para os mais diversos fins.  \\nEm geral, é possível transformar qualquer arquivo que possa ser impre sso \\nem arquivos PDF.   \\n \\n \\nSecretaria de Tecnologia da Informação do TRT da 4ª Região', metadata={'doc_id': 31, 'page': 0, 'source': 'vantagem_pdf.pdf'}),\n",
       " Document(page_content='assinados digitalmente , ou seja, i mpedem qualquer tipo  de alteração no \\narquivo original ; \\n\\uf0d8 Existem vários programas gratuitos que geram PDF como: PDFMaker, PDF \\nReDirect, PDFCreator  entre outros.  \\nObs.:  o software mais conhecido para a geração e  manipulação de arquivos \\nem PDF é o Adobe Acrobat.É  necessário ter cuidado para não confundir com \\no Adobe  Reader, que é simplesmente um leitor gratuito de PDFs,  ou seja, é um \\nprograma permite somente a leitura de  arquivos PDF', metadata={'doc_id': 26, 'page': 1, 'source': 'vantagem_pdf.pdf'}),\n",
       " Document(page_content='LOCADOR – KAIO OLIVEIRA PEIXOTO\\n___________________________________________ \\nLOCATÁRIO –  JOSE NILTON PEREIRA SILVA', metadata={'doc_id': 20, 'page': 4, 'source': 'contrato_aluguel_jose_nilton.pdf'})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agora tbm podemos acessar a chave source_documents\n",
    "# ele traz todos os arquivos q o modelo utilizou para gerar a resposta\n",
    "resposta['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outros tipos de chains\n",
    "\n",
    "🔄 chain_type='stuff' — o que é e como funciona\n",
    "\n",
    "- O `chain_type='stuff'` define como os documentos recuperados vão ser passados para o modelo LLM gerar a resposta.\n",
    "- No modo `\"stuff\"`, **todos os documentos recuperados são \"empilhados\" (stuffed) em um único prompt**, e esse prompt é enviado direto ao modelo.\n",
    "\n",
    "Exemplo prático:\n",
    "Se você recuperou 5 documentos com `k=5`, o LangChain junta todos os textos desses documentos num só prompt e diz:\n",
    "> \"Com base nesses textos, responda a pergunta...\"\n",
    "\n",
    "**✅ Vantagem:** simples e direto, ótimo para poucos documentos pequenos.  \n",
    "**⚠️ Limite:** se os textos forem muito grandes, pode ultrapassar o limite de tokens do modelo.\n",
    "\n",
    "---\n",
    "\n",
    "🔁 Outros `chain_type` disponíveis:\n",
    "\n",
    "1. `\"map_reduce\"`  \n",
    "- Divide os documentos em partes.\n",
    "- Roda o LLM em cada parte separadamente (**map**) e depois junta os resultados (**reduce**).\n",
    "- Melhor para textos longos.\n",
    "\n",
    "2. `\"refine\"`  \n",
    "- Cria uma resposta inicial com o primeiro documento e vai **refinando** essa resposta com os próximos.\n",
    "- Mais lento, mas pode melhorar a coerência.\n",
    "\n",
    "3. `\"map_rerank\"`  \n",
    "- Avalia cada documento individualmente e escolhe a melhor resposta com uma nota de relevância.\n",
    "- Útil quando você quer **qualidade acima de quantidade**.\n",
    "\n",
    "---\n",
    "\n",
    "✅ Conclusão curta:\n",
    "> `stuff` junta todos os docs e envia de uma vez ao LLM — simples e eficiente para entradas pequenas.  \n",
    "> Para documentos longos, prefira `map_reduce` ou `refine`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stuff**\n",
    "\n",
    "Quando a gente une nosso prompt com o contexto, existem várias formas de fazer isso.\n",
    "\n",
    "A forma mais simples e mais utilizada é a stuff. Esse também é o valor default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Adobe PDF (Portable Document Format) é um formato de arquivo desenvolvido pela Adobe Systems para representar documentos de maneira independente do aplicativo, hardware e sistema operacional usados para criá-los. Ele permite a representação de documentos contendo texto, gráficos e imagens de forma independente de dispositivo e resolução. É comumente utilizado para compartilhar documentos de forma segura e preservar a formatação original.\n"
     ]
    }
   ],
   "source": [
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vector_db.as_retriever(search_type='mmr'),\n",
    "    chain_type='stuff'\n",
    ")\n",
    "\n",
    "pergunta = \"o q é adobe pdf?\"\n",
    "\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "\n",
    "print(resposta['result'], end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Map Reduce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vector_db.as_retriever(search_type='mmr'),\n",
    "    chain_type='map_reduce'\n",
    ")\n",
    "\n",
    "pergunta = \"o q é adobe pdf?\"\n",
    "\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Refine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Adobe PDF é um formato de arquivo desenvolvido pela Adobe Systems que permite representar documentos de forma independente do aplicativo, hardware e sistema operacional utilizados para criá-los. Ele é amplamente utilizado devido à sua capacidade de preservar a formatação original e ser facilmente compartilhado e visualizado em diferentes dispositivos. Os arquivos PDF também podem ser assinados digitalmente, o que impede qualquer tipo de alteração no arquivo original. Existem vários programas gratuitos disponíveis para gerar arquivos PDF, como PDFMaker, PDF ReDirect e PDFCreator, porém o software mais conhecido para a geração e manipulação de arquivos em PDF é o Adobe Acrobat. É importante ter cuidado para não confundir o Adobe Acrobat com o Adobe Reader, que é simplesmente um leitor gratuito de PDFs e permite apenas a leitura de arquivos PDF.\n"
     ]
    }
   ],
   "source": [
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vector_db.as_retriever(search_type='mmr'),\n",
    "    chain_type='refine'\n",
    ")\n",
    "\n",
    "pergunta = \"o q é adobe pdf?\"\n",
    "\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "\n",
    "print(resposta['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
